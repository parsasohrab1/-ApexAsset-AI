# **سند نیازمندی‌های نرم‌افزاری (SRS) جامع - سیستم تولید داده‌های سنتتیک برای تمام مراحل چرخه حیات دارایی**

## **1. مقدمه**

### **1.1 هدف**
این سند نیازمندی‌های سیستم جامع تولید داده‌های سنتتیک برای شبیه‌سازی تمام مراحل چرخه حیات دارایی در صنعت نفت و گاز را تعریف می‌کند. سیستم داده‌های سنتتیک را برای 6 ماه عملکرد با فرکانس 1 هرتز برای کلیه دستگاه‌ها و سنسورها در تمامی مراحل از اکتشاف تا برچیدن تولید می‌کند.

### **1.2 دامنه**
سیستم تمامی مراحل زیر را پوشش می‌دهد:
1. اکتشاف و ارزیابی
2. توسعه و برنامه‌ریزی
3. تولید و عملیات
4. نگهداشت و بازسازی
5. برچیدن و بازسازی سایت

### **1.3 مخاطبان**
- توسعه‌دهندگان نرم‌افزار
- دانشمندان داده
- مهندسین نفت و گاز
- مدیران پروژه
- تسترهای سیستم

## **2. توصیف کلی سیستم**

### **2.1 معماری کلی سیستم**
```
┌─────────────────────────────────────────────────────────────────┐
│                 Synthetic Data Generation System                │
├─────────────────────────────────────────────────────────────────┤
│  Phase 1: Exploration & Appraisal Data Generator              │
│  Phase 2: Development & Planning Data Generator               │
│  Phase 3: Production & Operations Data Generator              │
│  Phase 4: Maintenance & Revamp Data Generator                 │
│  Phase 5: Decommissioning Data Generator                      │
│                                                                │
│  Common Modules:                                              │
│  - Configuration Manager                                      │
│  - Physics-Based Model Engine                                 │
│  - Failure/Anomaly Generator                                  │
│  - Environmental Simulator                                    │
│  - Economic Data Generator                                    │
│  - Data Validation & Quality Control                          │
│  - Export & Formatting Engine                                 │
└─────────────────────────────────────────────────────────────────┘
```

### **2.2 مشخصات فنی**
- **دوره زمانی:** 6 ماه (182 روز)
- **فرکانس نمونه‌برداری:** 1 هرتز (1 نمونه در ثانیه)
- **تعداد کل نقاط داده:** 15,724,800 نقطه بر سنسور
- **فرمت‌های خروجی:** CSV, Parquet, JSON, SQLite, HDF5
- **زبان برنامه‌نویسی:** Python 3.9+
- **تکرارپذیری:** با seedهای تصادفی کنترل‌شده

## **3. نیازمندی‌های ویژه**

### **3.1 نیازمندی‌های عملکردی**

#### **مرحله 1: اکتشاف و ارزیابی**
| ID | نیازمندی | توضیح | اولویت |
|----|----------|--------|---------|
| EXP-01 | تولید داده‌های لرزه‌ای 3D سنتتیک | داده‌های لرزه‌ای با ابعاد 1000×1000×500 پیکسل با ویژگی‌های زمین‌شناسی واقع‌گرا | بالا |
| EXP-02 | مدل‌سازی سازندهای زمین‌شناسی | ایجاد لایه‌های سنگی، گسل‌ها، دایک‌ها و تله‌های هیدروکربنی | بالا |
| EXP-03 | داده‌های مغناطیس‌سنجی | نقشه‌های مغناطیسی با وضوح 100×100 متر | متوسط |
| EXP-04 | داده‌های گرانش‌سنجی | نقشه‌های گرانشی با آنومالی‌های واقع‌گرا | متوسط |
| EXP-05 | داده‌های چاه‌های اکتشافی | لاگ‌های چاهی (GR, Resistivity, Sonic, etc.) برای 10 چاه اکتشافی | بالا |
| EXP-06 | داده‌های هسته‌ای | آنالیز مغزه برای 5 چاه | پایین |
| EXP-07 | نشانه‌های هیدروکربنی | شناسایی BSR، نقاط روشن (bright spots) در داده‌های لرزه‌ای | متوسط |

#### **مرحله 2: توسعه و برنامه‌ریزی**
| ID | نیازمندی | توضیح | اولویت |
|----|----------|--------|---------|
| DEV-01 | مدل مخزن سنتتیک | مدل سه‌بعدی مخزن با خواص پتروفیزیکی (تراوایی، تخلخل، اشباع) | بالا |
| DEV-02 | شبیه‌سازی جریان مخزن | داده‌های شبیه‌سازی برای 1000 سناریو | بالا |
| DEV-03 | داده‌های حفاری | پارامترهای حفاری (WOB, RPM, Flow Rate) برای 20 چاه توسعه | بالا |
| DEV-04 | داده‌های سکوی حفاری | وضعیت تجهیزات، مصرف انرژی، تولید پسماند | متوسط |
| DEV-05 | داده‌های طراحی شبکه جمع‌آوری | پارامترهای خطوط لوله، پمپ‌ها، کمپرسورها | متوسط |
| DEV-06 | داده‌های اقتصادی | قیمت نفت، هزینه‌های سرمایه‌ای و عملیاتی، نرخ تنزیل | بالا |
| DEV-07 | داده‌های ریسک | احتمالات رویدادهای ریسکی، تأثیرات مالی | متوسط |

#### **مرحله 3: تولید و عملیات** (قبلاً پیاده‌سازی شده - نیاز به توسعه)
| ID | نیازمندی | توضیح | اولویت |
|----|----------|--------|---------|
| PROD-01 | داده‌های فرآیندی پالایشگاه | توسعه بیشتر پالایشگاه موجود | بالا |
| PROD-02 | داده‌های تولید چاه | نرخ تولید، فشار چاه، تست تولید | بالا |
| PROD-03 | داده‌های تاسیسات فرآورش | جداکننده‌ها، سیستم‌های تزریق، واحدهای تصفیه | بالا |
| PROD-04 | داده‌های بهره‌وری انرژی | مصرف برق، گرمایش، سرمایش | متوسط |
| PROD-05 | داده‌های انتشارات | آلاینده‌های هوا، پساب، پسماند | متوسط |
| PROD-06 | داده‌های لجستیک | حمل‌ونقل، ذخیره‌سازی، بارگیری | پایین |
| PROD-07 | داده‌های ایمنی | حوادث، بازرسی‌ها، تمرینات | متوسط |

#### **مرحله 4: نگهداشت و بازسازی**
| ID | نیازمندی | توضیح | اولویت |
|----|----------|--------|---------|
| MNT-01 | داده‌های سلامت تجهیزات | ارتعاش، دما، آنالیز روغن | بالا |
| MNT-02 | داده‌های بازرسی | نتایج NDT، ضخامت‌سنجی، بازدید چشمی | بالا |
| MNT-03 | داده‌های تعمیرات | کارنه‌های تعمیر، زمان توقف، هزینه‌ها | بالا |
| MNT-04 | داده‌های موجودی قطعات | سطح موجودی، زمان تحویل، هزینه‌ها | متوسط |
| MNT-05 | داده‌های برنامه‌ریزی تعمیرات | تقویم تولید، در دسترس بودن نیرو | متوسط |
| MNT-06 | داده‌های پروژه‌های بازسازی | ROI، زمان اجرا، ریسک‌ها | متوسط |
| MNT-07 | داده‌های کالیبراسیون | تاریخچه کالیبراسیون تجهیزات | پایین |

#### **مرحله 5: برچیدن و بازسازی سایت**
| ID | نیازمندی | توضیح | اولویت |
|----|----------|--------|---------|
| DEC-01 | داده‌های وضعیت تجهیزات | شرایط فیزیکی، ارزش اسقاط | بالا |
| DEC-02 | داده‌های هزینه برچیدن | برآورد هزینه‌های مختلف سناریوها | بالا |
| DEC-03 | داده‌های محیط‌زیستی | آلودگی خاک، آب، شرایط پایه | بالا |
| DEC-04 | داده‌های مقررات | الزامات قانونی، مجوزها | متوسط |
| DEC-05 | داده‌های لجستیک برچیدن | حمل تجهیزات، مدیریت پسماند | متوسط |
| DEC-06 | داده‌های بازسازی سایت | برنامه‌های احیاء، پایش بلندمدت | متوسط |
| DEC-07 | داده‌های ریسک برچیدن | احتمالات حوادث، تأثیرات | پایین |

#### **ماژول‌های مشترک**
| ID | نیازمندی | توضیح | اولویت |
|----|----------|--------|---------|
| COM-01 | داده‌های محیطی | آب و هوا، شرایط جوی، زمین‌لرزه | بالا |
| COM-02 | داده‌های اقتصادی | قیمت‌های جهانی، نرخ ارز، تورم | بالا |
| COM-03 | داده‌های متنی | گزارش‌ها، صورتجلسات، دستورالعمل‌ها | متوسط |
| COM-04 | داده‌های مکانی | مختصات، نقشه‌ها، GIS | متوسط |
| COM-05 | داده‌های زمانی | برچسب‌های زمانی هماهنگ شده | بالا |

### **3.2 نیازمندی‌های غیرعملکردی**
| ID | نیازمندی | توضیح | معیار سنجش |
|----|----------|--------|-------------|
| NFR-01 | کارایی | تولید 15M نقطه داده در کمتر از 2 ساعت | زمان پردازش |
| NFR-02 | مقیاس‌پذیری | پشتیبانی از 10,000 سنسور همزمان | تعداد سنسورها |
| NFR-03 | قابلیت اطمینان | نرخ خطای کمتر از 0.001% | صحت داده‌ها |
| NFR-04 | تکرارپذیری | تولید داده یکسان با seed ثابت | همبستگی داده‌ها |
| NFR-05 | واقع‌گرایی | داده‌ها از نظر فیزیکی معتبر | اعتبارسنجی دامنه |
| NFR-06 | قابلیت نگهداری | کد ماژولار با مستندات کامل | چرخه عمر توسعه |
| NFR-07 | سازگاری | پشتیبانی از سیستم‌های عامل مختلف | پلتفرم‌های تست شده |
| NFR-08 | امنیت | رمزنگاری داده‌های حساس | استانداردهای امنیتی |

## **4. معماری سیستم**

### **4.1 نمودار اجزای سیستم**
```
┌─────────────────────────────────────────────────────────────────┐
│                    Data Generation Controller                   │
├─────────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐           │
│  │ Exploration │  │ Development │  │ Production  │           │
│  │   Module    │  │   Module    │  │   Module    │           │
│  └─────────────┘  └─────────────┘  └─────────────┘           │
│                                                               │
│  ┌─────────────┐  ┌─────────────┐  ┌────────────────────┐    │
│  │ Maintenance │  │Decommission │  │ Common Modules     │    │
│  │   Module    │  │   Module    │  │ - Environmental    │    │
│  └─────────────┘  └─────────────┘  │ - Economic        │    │
│                                     │ - Validation      │    │
│                                     │ - Export          │    │
│                                     └────────────────────┘    │
└─────────────────────────────────────────────────────────────────┘
```

### **4.2 جریان داده‌ها**
```
Configuration → Physics Models → Anomaly Injection → 
Environmental Effects → Economic Context → Validation → Export
```

## **5. جزئیات طراحی**

### **5.1 مدل‌های فیزیکی استفاده‌شده**

#### **مدل‌های اکتشافی:**
1. **مدل لرزه‌ای:** معادله موج برای انتشار امواج لرزه‌ای
2. **مدل زمین‌شناسی:** الگوریتم‌های ژئواستاتیستیک برای ایجاد سازندها
3. **مدل مغناطیسی:** معادلات ماکسول برای میدان‌های مغناطیسی
4. **مدل گرانشی:** قانون گرانش نیوتن با توزیع چگالی

#### **مدل‌های تولیدی:**
1. **مدل جریان مخزن:** معادلات دارسی برای جریان سیال
2. **مدل چاه:** معادلات افت فشار در چاه
3. **مدل فرآیندی:** معادلات بقای جرم، انرژی و مومنتوم

#### **مدل‌های تجهیزاتی:**
1. **مدل فرسایش:** قانون Arrhenius برای فرسایش حرارتی
2. **مدل خستگی:** قانون Miner برای خستگی مواد
3. **مدل خوردگی:** قانون Faraday برای خوردگی الکتروشیمیایی

### **5.2 الگوهای خرابی و ناهنجاری**

#### **الگوهای اکتشافی:**
- تفسیر اشتباه داده‌های لرزه‌ای
- خطا در تخمین حجم مخزن
- عدم کشف منابع کوچک

#### **الگوهای تولیدی:**
- رسوب‌گیری در چاه
- آب‌ breakthrough
- خرابی پمپ‌ها و کمپرسورها

#### **الگوهای ایمنی:**
- نشتی‌ها
- آتش‌سوزی
- انتشار آلاینده‌ها

### **5.3 ساختار داده‌ها**

#### **فرمت‌های خروجی:**
1. **CSV:** برای داده‌های جدولی
2. **Parquet:** برای داده‌های حجیم
3. **JSON:** برای داده‌های ساختاریافته
4. **SQLite:** برای داده‌های رابطه‌ای
5. **HDF5:** برای داده‌های علمی
6. **NETCDF:** برای داده‌های جغرافیایی
7. **SEGY:** برای داده‌های لرزه‌ای

#### **ساختار دایرکتوری:**
```
synthetic_data/
├── exploration/
│   ├── seismic/
│   ├── well_logs/
│   ├── gravity_magnetic/
│   └── interpretation/
├── development/
│   ├── reservoir_model/
│   ├── drilling_data/
│   ├── facilities/
│   └── economics/
├── production/
│   ├── well_data/
│   ├── process_data/
│   ├── equipment/
│   └── environmental/
├── maintenance/
│   ├── condition_monitoring/
│   ├── inspection/
│   ├── repairs/
│   └── spare_parts/
├── decommissioning/
│   ├── asset_condition/
│   ├── cost_estimates/
│   ├── environmental/
│   └── logistics/
└── common/
    ├── weather/
    ├── market_data/
    ├── documents/
    └── metadata/
```

## **6. نیازمندی‌های رابط**

### **6.1 رابط کاربری**
- رابط خط فرمان (CLI) برای اتوماسیون
- رابط گرافیکی ساده برای پیکربندی
- API برای یکپارچه‌سازی با سیستم‌های دیگر

### **6.2 رابط برنامه‌نویسی (API)**
```python
class SyntheticDataGenerator:
    def generate_exploration_data(config): ...
    def generate_development_data(config): ...
    def generate_production_data(config): ...
    def generate_maintenance_data(config): ...
    def generate_decommissioning_data(config): ...
    def export_data(format, path): ...
    def validate_data(): ...
```

## **7. نیازمندی‌های کیفیت**

### **7.1 اعتبارسنجی داده‌ها**
- بررسی محدوده مقادیر (Range validation)
- اعتبارسنجی روابط فیزیکی
- بررسی سازگاری زمانی
- اعتبارسنجی مکانی
- تحلیل آماری توزیع‌ها

### **7.2 کنترل کیفیت**
- لاگ‌گیری خطاها
- گزارش‌های کیفیت
- ممیزی داده‌ها
- بازیابی از خطا

## **8. مستندات**

### **8.1 مستندات فنی**
- مستندات API
- راهنمای نصب
- راهنمای کاربر
- مثال‌های استفاده

### **8.2 مستندات داده‌ها**
- فرهنگ داده‌ها (Data dictionary)
- نمودارهای ER
- متادیتای کامل
- نمونه‌های داده

## **9. محدودیت‌ها و فرضیات**

### **9.1 محدودیت‌ها**
- داده‌های سنتتیک نمی‌توانند کاملاً جایگزین داده‌های واقعی شوند
- مدل‌های فیزیکی ساده‌سازی شده‌اند
- مقیاس زمانی 6 ماه برای برخی فرآیندها کوتاه است

### **9.2 فرضیات**
- رفتار خطی برای برخی فرآیندهای غیرخطی
- شرایط مرزی ایده‌آل
- عدم در نظرگیری برخی متغیرهای محیطی پیچیده

## **10. جدول زمانی و منابع**

### **10.1 فازهای توسعه**
| فاز | مدت | تحویل‌ها |
|-----|------|-----------|
| فاز 1: طراحی | 2 هفته | SRS نهایی، معماری |
| فاز 2: پیاده‌سازی هسته | 4 هفته | موتور تولید داده پایه |
| فاز 3: ماژول‌های تخصصی | 6 هفته | ماژول‌های 5 مرحله |
| فاز 4: یکپارچه‌سازی | 2 هفته | سیستم کامل |
| فاز 5: تست و اعتبارسنجی | 2 هفته | گزارش تست |
| فاز 6: مستندسازی | 1 هفته | مستندات کامل |

### **10.2 منابع مورد نیاز**
- توسعه‌دهنده Python ارشد: 2 نفر
- مهندس نفت مشاور: 1 نفر
- دانشمند داده: 1 نفر
- سرور تست: 1 عدد (32GB RAM, 8-core CPU)
- فضای ذخیره‌سازی: 1TB

## **11. ریسک‌ها و راهکارهای کاهش**

| ریسک | احتمال | تأثیر | راهکار کاهش |
|------|---------|--------|-------------|
| پیچیدگی مدل‌های فیزیکی | بالا | بالا | استفاده از مدل‌های ساده‌شده |
| حجم زیاد داده‌ها | بالا | متوسط | بهینه‌سازی الگوریتم‌ها |
| واقع‌گرایی ناکافی | متوسط | بالا | اعتبارسنجی با خبرگان حوزه |
| زمان پردازش طولانی | بالا | متوسط | پردازش موازی |
| عدم پذیرش کاربران | متوسط | بالا | مشارکت کاربران در طراحی |

## **12. معیارهای پذیرش**

### **12.1 معیارهای فنی**
1. تولید 15.7M نقطه داده در کمتر از 2 ساعت
2. خطای کمتر از 0.001% در داده‌ها
3. تکرارپذیری کامل با seed ثابت
4. پشتیبانی از تمام 5 فرمت خروجی

### **12.2 معیارهای کیفیت داده**
1. اعتبارسنجی فیزیکی موفق
2. توزیع‌های آماری واقع‌گرا
3. روابط علّی صحیح بین متغیرها
4. متادیتای کامل و دقیق

### **12.3 معیارهای کاربری**
1. مستندات کامل و قابل فهم
2. رابط کاربری intuitive
3. نمونه‌های عملی مفید
4. پشتیبانی از سناریوهای واقعی

---

# **کد تولید کننده داده‌های سنتتیک برای تمام مراحل**

```python
#!/usr/bin/env python3
"""
Synthetic Data Generator for Complete Oil & Gas Asset Lifecycle
Covering all 5 stages: Exploration → Decommissioning
"""

import numpy as np
import pandas as pd
import xarray as xr
from datetime import datetime, timedelta
import json
import yaml
from pathlib import Path
import warnings
import sqlite3
from scipy import signal, interpolate
import h5py
warnings.filterwarnings('ignore')

class CompleteAssetLifecycleGenerator:
    """Main generator for complete asset lifecycle data"""
    
    def __init__(self, config_file='complete_config.yaml'):
        """Initialize the complete generator"""
        
        # Load configuration
        self.config = self._load_config(config_file)
        
        # Global parameters
        self.start_date = datetime(2024, 1, 1, 0, 0, 0)
        self.simulation_days = 182  # 6 months
        self.total_seconds = self.simulation_days * 24 * 3600
        self.sampling_rate = 1  # 1 Hz
        
        # Time index for high-frequency data
        self.time_index = pd.date_range(
            start=self.start_date,
            periods=self.total_seconds,
            freq='S'
        )
        
        # Lower frequency time indices
        self.daily_index = pd.date_range(
            start=self.start_date,
            periods=self.simulation_days,
            freq='D'
        )
        
        # Data storage
        self.exploration_data = {}
        self.development_data = {}
        self.production_data = {}
        self.maintenance_data = {}
        self.decommissioning_data = {}
        self.common_data = {}
        
        # Random seeds for reproducibility
        self.seed = self.config.get('random_seed', 42)
        np.random.seed(self.seed)
        
        print(f"Complete Asset Lifecycle Data Generator Initialized")
        print(f"Simulation Period: {self.start_date.date()} to {(self.start_date + timedelta(days=self.simulation_days)).date()}")
        print(f"Total Data Points per High-Freq Sensor: {self.total_seconds:,}")
    
    def _load_config(self, config_file):
        """Load configuration from YAML file"""
        if Path(config_file).exists():
            with open(config_file, 'r') as f:
                return yaml.safe_load(f)
        else:
            return self._get_default_config()
    
    def _get_default_config(self):
        """Return default configuration"""
        return {
            'asset': {
                'name': 'Eagle_Ford_Asset_Texas',
                'location': 'South Texas, USA',
                'type': 'Unconventional Oil & Gas',
                'coordinates': {'lat': 29.4241, 'lon': -98.4936}
            },
            'simulation': {
                'days': 182,
                'sampling_rate_hz': 1,
                'random_seed': 42
            },
            'exploration': {
                'seismic_grid': {'x': 1000, 'y': 1000, 'z': 500},
                'well_count': 10,
                'survey_types': ['2D', '3D', 'MT', 'Gravity']
            },
            'development': {
                'planned_wells': 25,
                'platform_types': ['Jackup', 'Fixed', 'FPSO'],
                'facilities': ['Processing', 'Compression', 'Storage']
            },
            'production': {
                'peak_production_bopd': 50000,
                'gas_production_mmscfd': 150,
                'water_production_bwpd': 80000,
                'facility_capacity': 100000
            }
        }
    
    # =========================================================================
    # PHASE 1: EXPLORATION & APPRAISAL DATA GENERATION
    # =========================================================================
    
    def generate_exploration_data(self):
        """Generate synthetic exploration and appraisal data"""
        print("\n" + "="*60)
        print("PHASE 1: Generating Exploration & Appraisal Data")
        print("="*60)
        
        # 1.1 Synthetic 3D Seismic Data
        print("Generating 3D seismic data...")
        seismic_data = self._generate_3d_seismic()
        self.exploration_data['seismic_3d'] = seismic_data
        
        # 1.2 Well Logs for Exploration Wells
        print("Generating well logs for exploration wells...")
        well_logs = self._generate_exploration_well_logs()
        self.exploration_data['well_logs'] = well_logs
        
        # 1.3 Gravity and Magnetic Data
        print("Generating gravity and magnetic survey data...")
        gravity_data = self._generate_gravity_data()
        magnetic_data = self._generate_magnetic_data()
        self.exploration_data['gravity'] = gravity_data
        self.exploration_data['magnetic'] = magnetic_data
        
        # 1.4 Core Analysis Data
        print("Generating core analysis data...")
        core_data = self._generate_core_analysis()
        self.exploration_data['core_analysis'] = core_data
        
        # 1.5 Geological Interpretation
        print("Generating geological interpretation data...")
        interpretation = self._generate_geological_interpretation()
        self.exploration_data['interpretation'] = interpretation
        
        # 1.6 Prospect Evaluation
        print("Generating prospect evaluation data...")
        prospects = self._generate_prospect_evaluation()
        self.exploration_data['prospects'] = prospects
        
        print("Exploration data generation complete!")
        return self.exploration_data
    
    def _generate_3d_seismic(self):
        """Generate synthetic 3D seismic data with geological features"""
        grid_x = self.config['exploration']['seismic_grid']['x']
        grid_y = self.config['exploration']['seismic_grid']['y']
        grid_z = self.config['exploration']['seismic_grid']['z']
        
        # Create base seismic volume with random noise
        seismic = np.random.normal(0, 1, (grid_z, grid_y, grid_x))
        
        # Add geological layers (sinusoidal boundaries)
        for layer in range(5):
            amplitude = np.random.uniform(10, 50)
            frequency = np.random.uniform(0.01, 0.05)
            phase = np.random.uniform(0, 2*np.pi)
            
            for y in range(grid_y):
                for x in range(grid_x):
                    depth = int(grid_z/2 + amplitude * np.sin(frequency*x + phase) * np.sin(frequency*y + phase))
                    # Create reflection at layer boundary
                    if depth < grid_z:
                        seismic[depth:min(depth+10, grid_z), y, x] += np.random.normal(5, 2)
        
        # Add faults (discontinuities)
        for _ in range(3):
            fault_x = np.random.randint(100, grid_x-100)
            fault_strike = np.random.uniform(0, 2*np.pi)
            
            for y in range(grid_y):
                offset = int(20 * np.sin(2*np.pi*y/grid_y))
                fault_position = fault_x + int(y * np.tan(fault_strike))
                
                if 0 < fault_position < grid_x:
                    # Create discontinuity
                    seismic[:, y, fault_position-5:fault_position+5] *= 1.5
        
        # Add hydrocarbon indicators (bright spots)
        for _ in range(5):
            center_x = np.random.randint(200, grid_x-200)
            center_y = np.random.randint(200, grid_y-200)
            center_z = np.random.randint(100, grid_z-100)
            radius = np.random.randint(20, 50)
            
            # Create amplitude anomaly
            for z in range(max(0, center_z-radius), min(grid_z, center_z+radius)):
                for y in range(max(0, center_y-radius), min(grid_y, center_y+radius)):
                    for x in range(max(0, center_x-radius), min(grid_x, center_x+radius)):
                        distance = np.sqrt((x-center_x)**2 + (y-center_y)**2 + (z-center_z)**2)
                        if distance < radius:
                            seismic[z, y, x] *= (1 + 2 * (radius - distance) / radius)
        
        return {
            'data': seismic,
            'dimensions': {'x': grid_x, 'y': grid_y, 'z': grid_z},
            'units': 'Amplitude',
            'coordinate_system': 'UTM Zone 14N',
            'bin_size': 25,  # meters
            'sample_rate': 2  # ms
        }
    
    def _generate_exploration_well_logs(self):
        """Generate synthetic well logs for exploration wells"""
        well_count = self.config['exploration']['well_count']
        wells = []
        
        log_types = ['GR', 'RT', 'NPHI', 'RHOB', 'DT', 'CALI', 'SP']
        
        for well_idx in range(well_count):
            depth_start = 0
            depth_end = 3000 + np.random.randint(-500, 500)  # meters
            depth_interval = 0.1524  # 0.5 feet in meters
            depth_count = int((depth_end - depth_start) / depth_interval)
            
            depths = np.arange(depth_start, depth_end, depth_interval)
            
            well_data = {
                'well_id': f"EXP-{well_idx+1:03d}",
                'location': {
                    'x': np.random.uniform(100, 900),
                    'y': np.random.uniform(100, 900),
                    'elevation': np.random.uniform(100, 300)
                },
                'total_depth': depth_end,
                'logs': {}
            }
            
            # Generate each log type
            for log_type in log_types:
                if log_type == 'GR':  # Gamma Ray
                    # Create layered response
                    log_values = np.ones_like(depths) * 50
                    
                    # Add shale zones (high GR)
                    shale_zones = np.random.randint(2, 6)
                    for _ in range(shale_zones):
                        zone_start = np.random.randint(0, len(depths)//2)
                        zone_thickness = np.random.randint(50, 200)
                        zone_end = min(zone_start + zone_thickness, len(depths))
                        log_values[zone_start:zone_end] = np.random.uniform(80, 150)
                    
                    # Add sand zones (low GR)
                    sand_zones = np.random.randint(3, 8)
                    for _ in range(sand_zones):
                        zone_start = np.random.randint(0, len(depths)//2)
                        zone_thickness = np.random.randint(20, 100)
                        zone_end = min(zone_start + zone_thickness, len(depths))
                        log_values[zone_start:zone_end] = np.random.uniform(20, 50)
                
                elif log_type == 'RT':  # Resistivity
                    # Base resistivity with depth trend
                    log_values = 2 + 0.001 * depths
                    
                    # Add hydrocarbon zones (high resistivity)
                    hc_zones = np.random.randint(1, 4)
                    for _ in range(hc_zones):
                        zone_start = np.random.randint(500, len(depths)-500)
                        zone_thickness = np.random.randint(10, 50)
                        zone_end = min(zone_start + zone_thickness, len(depths))
                        log_values[zone_start:zone_end] *= np.random.uniform(5, 20)
                
                elif log_type == 'NPHI':  # Neutron Porosity
                    log_values = np.random.normal(0.15, 0.05, len(depths))
                    log_values = np.clip(log_values, 0.01, 0.35)
                
                elif log_type == 'RHOB':  # Bulk Density
                    log_values = 2.65 - 0.0003 * depths + np.random.normal(0, 0.05, len(depths))
                    log_values = np.clip(log_values, 2.0, 2.8)
                
                # Add noise and smoothing
                log_values += np.random.normal(0, 0.02, len(depths))
                log_values = np.convolve(log_values, np.ones(5)/5, mode='same')
                
                well_data['logs'][log_type] = {
                    'depth': depths.tolist(),
                    'values': log_values.tolist(),
                    'units': self._get_log_units(log_type)
                }
            
            wells.append(well_data)
        
        return wells
    
    def _get_log_units(self, log_type):
        """Get appropriate units for log type"""
        units = {
            'GR': 'API',
            'RT': 'ohm.m',
            'NPHI': 'v/v',
            'RHOB': 'g/cc',
            'DT': 'us/ft',
            'CALI': 'inches',
            'SP': 'mV'
        }
        return units.get(log_type, 'unitless')
    
    def _generate_gravity_data(self):
        """Generate synthetic gravity survey data"""
        grid_size = 100  # 100x100 grid
        x = np.linspace(0, 10000, grid_size)  # 10km survey
        y = np.linspace(0, 10000, grid_size)
        
        X, Y = np.meshgrid(x, y)
        
        # Base gravity field
        gravity = 9.81 * np.ones_like(X)
        
        # Add regional gradient
        gravity += 0.0001 * X + 0.00005 * Y
        
        # Add anomalies from dense structures
        anomalies = np.random.randint(3, 8)
        for _ in range(anomalies):
            center_x = np.random.uniform(1000, 9000)
            center_y = np.random.uniform(1000, 9000)
            radius = np.random.uniform(500, 2000)
            amplitude = np.random.uniform(0.001, 0.01)
            
            distance = np.sqrt((X - center_x)**2 + (Y - center_y)**2)
            gravity += amplitude * np.exp(-distance**2 / (2*radius**2))
        
        return {
            'x_coordinates': x.tolist(),
            'y_coordinates': y.tolist(),
            'gravity_values': gravity.tolist(),
            'units': 'm/s²',
            'survey_date': self.start_date.strftime('%Y-%m-%d')
        }
    
    def _generate_magnetic_data(self):
        """Generate synthetic magnetic survey data"""
        grid_size = 100
        x = np.linspace(0, 10000, grid_size)
        y = np.linspace(0, 10000, grid_size)
        
        X, Y = np.meshgrid(x, y)
        
        # Base magnetic field (Texas region)
        magnetic = 50000 * np.ones_like(X)  # nT
        
        # Add diurnal variation
        magnetic += 100 * np.sin(2*np.pi*X/10000) * np.cos(2*np.pi*Y/10000)
        
        # Add magnetic anomalies from basement features
        anomalies = np.random.randint(2, 6)
        for _ in range(anomalies):
            center_x = np.random.uniform(2000, 8000)
            center_y = np.random.uniform(2000, 8000)
            radius = np.random.uniform(300, 1500)
            amplitude = np.random.uniform(500, 3000)
            
            distance = np.sqrt((X - center_x)**2 + (Y - center_y)**2)
            magnetic += amplitude * np.exp(-distance**2 / (2*radius**2))
        
        return {
            'x_coordinates': x.tolist(),
            'y_coordinates': y.tolist(),
            'magnetic_values': magnetic.tolist(),
            'units': 'nT',
            'survey_date': self.start_date.strftime('%Y-%m-%d')
        }
    
    def _generate_core_analysis(self):
        """Generate synthetic core analysis data"""
        cores = []
        
        for core_idx in range(5):
            depth = np.random.uniform(500, 2500)
            length = np.random.uniform(10, 30)
            
            # Generate measurements along core
            measurement_points = np.random.randint(20, 50)
            depths = np.linspace(depth, depth + length, measurement_points)
            
            core_data = {
                'core_id': f"CORE-{core_idx+1:03d}",
                'well_id': f"EXP-{np.random.randint(1, 11):03d}",
                'top_depth': depth,
                'length': length,
                'measurements': []
            }
            
            for i, meas_depth in enumerate(depths):
                measurement = {
                    'depth': float(meas_depth),
                    'porosity': np.random.uniform(0.05, 0.25),
                    'permeability': np.random.lognormal(mean=1.0, sigma=1.5),  # mD
                    'grain_density': np.random.uniform(2.65, 2.75),
                    'fluid_saturation': {
                        'oil': np.random.uniform(0.0, 0.8),
                        'water': np.random.uniform(0.2, 1.0),
                        'gas': np.random.uniform(0.0, 0.3)
                    },
                    'grain_size': np.random.choice(['Fine', 'Medium', 'Coarse'], p=[0.4, 0.4, 0.2]),
                    'lithology': np.random.choice(['Sandstone', 'Shale', 'Limestone', 'Dolomite'], 
                                                 p=[0.5, 0.3, 0.15, 0.05])
                }
                measurement['fluid_saturation']['total'] = (
                    measurement['fluid_saturation']['oil'] + 
                    measurement['fluid_saturation']['water'] + 
                    measurement['fluid_saturation']['gas']
                )
                core_data['measurements'].append(measurement)
            
            cores.append(core_data)
        
        return cores
    
    def _generate_geological_interpretation(self):
        """Generate synthetic geological interpretation data"""
        horizons = []
        
        # Define major horizons
        horizon_names = ['Sea Floor', 'Top Miocene', 'Base Miocene', 
                        'Top Oligocene', 'Base Oligocene', 'Top Eocene']
        
        for i, name in enumerate(horizon_names):
            # Create depth surface for each horizon
            grid_size = 50
            x = np.linspace(0, 10000, grid_size)
            y = np.linspace(0, 10000, grid_size)
            X, Y = np.meshgrid(x, y)
            
            # Base depth increases with horizon age
            base_depth = 500 + i * 300
            
            # Add structural features
            Z = base_depth + 100 * np.sin(2*np.pi*X/5000) * np.cos(2*np.pi*Y/5000)
            
            # Add faults
            if np.random.random() < 0.6:  # 60% chance of fault
                fault_strike = np.random.uniform(0, np.pi)
                fault_throw = np.random.uniform(10, 50)
                
                for j in range(grid_size):
                    fault_line = 5000 + 2000 * np.sin(fault_strike)
                    if X[0, j] > fault_line:
                        Z[:, j] += fault_throw
            
            horizons.append({
                'name': name,
                'age_ma': 65 - i * 10,  # Millions of years ago
                'depth_surface': Z.tolist(),
                'x_coordinates': x.tolist(),
                'y_coordinates': y.tolist(),
                'lithology': np.random.choice(['Shale', 'Sandstone', 'Limestone', 'Chalk']),
                'confidence': np.random.uniform(0.7, 0.95)
            })
        
        # Generate fault planes
        faults = []
        for i in range(np.random.randint(3, 8)):
            fault = {
                'fault_id': f"Fault-{i+1:03d}",
                'type': np.random.choice(['Normal', 'Reverse', 'Strike-Slip']),
                'strike': np.random.uniform(0, 360),
                'dip': np.random.uniform(30, 80),
                'throw': np.random.uniform(20, 100),
                'length': np.random.uniform(2000, 8000),
                'seal_capacity': np.random.uniform(0.3, 0.9)
            }
            faults.append(fault)
        
        return {
            'horizons': horizons,
            'faults': faults,
            'depositional_environment': np.random.choice([
                'Deep Marine', 'Shallow Marine', 'Fluvial', 'Deltaic', 'Lacustrine'
            ]),
            'trap_type': np.random.choice([
                'Structural', 'Stratigraphic', 'Combination'
            ]),
            'source_rock_quality': np.random.choice(['Poor', 'Fair', 'Good', 'Excellent']),
            'migration_pathway': np.random.choice(['Direct', 'Lateral', 'Complex']),
            'interpretation_date': self.start_date.strftime('%Y-%m-%d')
        }
    
    def _generate_prospect_evaluation(self):
        """Generate synthetic prospect evaluation data"""
        prospects = []
        
        for prospect_idx in range(np.random.randint(3, 7)):
            # Calculate prospect parameters
            area = np.random.uniform(5, 50)  # km²
            thickness = np.random.uniform(10, 50)  # meters
            net_to_gross = np.random.uniform(0.6, 0.9)
            porosity = np.random.uniform(0.12, 0.22)
            saturation = np.random.uniform(0.65, 0.85)
            formation_volume_factor = np.random.uniform(1.2, 1.4)
            recovery_factor = np.random.uniform(0.25, 0.45)
            
            # Calculate volumes
            bulk_volume = area * 1e6 * thickness  # m³
            pore_volume = bulk_volume * porosity
            hydrocarbon_volume = pore_volume * net_to_gross * saturation
            stock_tank_oil = hydrocarbon_volume / formation_volume_factor  # m³
            recoverable_oil = stock_tank_oil * recovery_factor  # m³
            recoverable_oil_mmbbl = recoverable_oil / 158.987  # Convert to million barrels
            
            prospect = {
                'prospect_id': f"PROS-{prospect_idx+1:03d}",
                'name': np.random.choice(['Eagle', 'Hawk', 'Falcon', 'Condor', 'Vulture']),
                'location': {
                    'center_x': np.random.uniform(2000, 8000),
                    'center_y': np.random.uniform(2000, 8000),
                    'radius': np.random.uniform(500, 2000)
                },
                'geological_parameters': {
                    'area_km2': float(area),
                    'thickness_m': float(thickness),
                    'net_to_gross': float(net_to_gross),
                    'porosity': float(porosity),
                    'hydrocarbon_saturation': float(saturation),
                    'formation_volume_factor': float(formation_volume_factor)
                },
                'volume_estimates': {
                    'bulk_volume_m3': float(bulk_volume),
                    'pore_volume_m3': float(pore_volume),
                    'hydrocarbon_volume_m3': float(hydrocarbon_volume),
                    'stooip_m3': float(stock_tank_oil),
                    'stooip_mmbbl': float(recoverable_oil_mmbbl),
                    'recoverable_oil_mmbbl': float(recoverable_oil_mmbbl * 0.8)  # Conservative estimate
                },
                'risk_assessment': {
                    'geological_risk': np.random.uniform(0.3, 0.7),
                    'chance_of_success': 1 - np.random.uniform(0.3, 0.7),
                    'commerciality_risk': np.random.uniform(0.4, 0.8),
                    'overall_risk': np.random.uniform(0.5, 0.9)
                },
                'economic_indicators': {
                    'npv_musd': float(recoverable_oil_mmbbl * 50 * np.random.uniform(0.7, 1.3)),
                    'irr_percent': np.random.uniform(15, 35),
                    'payback_years': np.random.uniform(3, 8)
                },
                'recommendation': np.random.choice(['Drill', 'Appraise Further', 'Drop'], 
                                                  p=[0.4, 0.4, 0.2])
            }
            prospects.append(prospect)
        
        return prospects
    
    # =========================================================================
    # PHASE 2: DEVELOPMENT & PLANNING DATA GENERATION
    # =========================================================================
    
    def generate_development_data(self):
        """Generate synthetic development and planning data"""
        print("\n" + "="*60)
        print("PHASE 2: Generating Development & Planning Data")
        print("="*60)
        
        # 2.1 Reservoir Simulation Model
        print("Generating reservoir simulation model...")
        reservoir_model = self._generate_reservoir_model()
        self.development_data['reservoir_model'] = reservoir_model
        
        # 2.2 Well Planning Data
        print("Generating well planning data...")
        well_plans = self._generate_well_plans()
        self.development_data['well_plans'] = well_plans
        
        # 2.3 Facilities Design Data
        print("Generating facilities design data...")
        facilities = self._generate_facilities_design()
        self.development_data['facilities'] = facilities
        
        # 2.4 Drilling Data
        print("Generating drilling data...")
        drilling_data = self._generate_drilling_data()
        self.development_data['drilling'] = drilling_data
        
        # 2.5 Economic Analysis
        print("Generating economic analysis data...")
        economics = self._generate_development_economics()
        self.development_data['economics'] = economics
        
        # 2.6 Risk Analysis
        print("Generating risk analysis data...")
        risks = self._generate_development_risks()
        self.development_data['risks'] = risks
        
        print("Development data generation complete!")
        return self.development_data
    
    def _generate_reservoir_model(self):
        """Generate synthetic reservoir simulation model"""
        grid_size = 50  # 50x50x20 grid
        
        # Create grid coordinates
        x = np.linspace(0, 5000, grid_size)  # 5km in x direction
        y = np.linspace(0, 5000, grid_size)  # 5km in y direction
        z = np.linspace(2500, 2700, 20)  # 200m thick reservoir
        
        X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
        
        # Generate petrophysical properties
        porosity = np.random.normal(0.18, 0.05, (grid_size, grid_size, 20))
        porosity = np.clip(porosity, 0.05, 0.30)
        
        # Permeability (log-normal distribution)
        permeability = np.random.lognormal(mean=2.0, sigma=1.0, size=(grid_size, grid_size, 20))
        permeability = np.clip(permeability, 1, 5000)  # mD
        
        # Water saturation
        sw = np.random.normal(0.25, 0.1, (grid_size, grid_size, 20))
        sw = np.clip(sw, 0.15, 0.50)
        
        # Net-to-gross
        ntg = np.random.normal(0.75, 0.15, (grid_size, grid_size, 20))
        ntg = np.clip(ntg, 0.4, 1.0)
        
        # Create geological facies
        facies = np.zeros((grid_size, grid_size, 20), dtype=int)
        for k in range(20):
            for i in range(grid_size):
                for j in range(grid_size):
                    if porosity[i, j, k] > 0.2 and permeability[i, j, k] > 100:
                        facies[i, j, k] = 1  # Good quality sand
                    elif porosity[i, j, k] > 0.15 and permeability[i, j, k] > 10:
                        facies[i, j, k] = 2  # Medium quality sand
                    else:
                        facies[i, j, k] = 3  # Shale
        
        # PVT properties
        pvt_data = {
            'oil_fvf': 1.35,  # rb/stb
            'oil_viscosity': 0.8,  # cp
            'gas_gravity': 0.65,
            'water_fvf': 1.02,
            'water_viscosity': 0.5,
            'rock_compressibility': 3e-6  # psi^-1
        }
        
        # Relative permeability curves
        sw_rel = np.linspace(0.2, 0.8, 20)
        krw = (sw_rel - 0.2) / 0.6  # Linear relative permeability
        krow = 1.0 - krw
        
        relperm = {
            'sw': sw_rel.tolist(),
            'krw': krw.tolist(),
            'krow': krow.tolist()
        }
        
        return {
            'grid': {
                'dimensions': {'nx': grid_size, 'ny': grid_size, 'nz': 20},
                'cell_size': {'dx': 100, 'dy': 100, 'dz': 10},  # meters
                'coordinates': {
                    'x': x.tolist(),
                    'y': y.tolist(),
                    'z': z.tolist()
                }
            },
            'properties': {
                'porosity': porosity.tolist(),
                'permeability_x': permeability.tolist(),
                'permeability_y': permeability.tolist(),
                'permeability_z': (permeability * 0.1).tolist(),  # Vertical perm lower
                'water_saturation': sw.tolist(),
                'net_to_gross': ntg.tolist(),
                'facies': facies.tolist()
            },
            'pvt_data': pvt_data,
            'relative_permeability': relperm,
            'initial_conditions': {
                'pressure_psi': 3500,
                'temperature_f': 180,
                'datum_depth': 2600,
                'datum_pressure': 3500
            },
            'aquifer': {
                'type': 'Bottom',
                'size': 'Large',
                'connectivity': 0.8
            }
        }
    
    def _generate_well_plans(self):
        """Generate synthetic well planning data"""
        well_count = self.config['development']['planned_wells']
        wells = []
        
        well_types = ['Producer', 'Injector', 'Appraisal']
        completion_types = ['Horizontal', 'Vertical', 'Deviated']
        
        for well_idx in range(well_count):
            well_type = np.random.choice(well_types, p=[0.6, 0.3, 0.1])
            completion = np.random.choice(completion_types, p=[0.5, 0.3, 0.2])
            
            # Generate well trajectory
            if completion == 'Vertical':
                trajectory = [
                    {'md': 0, 'x': 0, 'y': 0, 'z': 0},
                    {'md': 2600, 'x': 0, 'y': 0, 'z': 2600}
                ]
            elif completion == 'Horizontal':
                # Build curve then horizontal section
                kickoff_depth = 2000
                build_rate = 5  # degrees per 30m
                horizontal_length = 1500
                
                trajectory = [
                    {'md': 0, 'x': 0, 'y': 0, 'z': 0},
                    {'md': kickoff_depth, 'x': 0, 'y': 0, 'z': kickoff_depth},
                    {'md': kickoff_depth + 300, 'x': 150, 'y': 0, 'z': kickoff_depth + 150},
                    {'md': kickoff_depth + 300 + horizontal_length, 
                     'x': 150 + horizontal_length, 'y': 0, 'z': kickoff_depth + 150}
                ]
            
            well = {
                'well_id': f"DEV-{well_idx+1:03d}",
                'name': f"Eagle_{well_idx+1}",
                'type': well_type,
                'completion_type': completion,
                'location': {
                    'surface_x': np.random.uniform(100, 4900),
                    'surface_y': np.random.uniform(100, 4900),
                    'bottom_x': np.random.uniform(100, 4900),
                    'bottom_y': np.random.uniform(100, 4900)
                },
                'targets': [
                    {
                        'name': 'Main Pay',
                        'depth': 2600 + np.random.uniform(-50, 50),
                        'thickness': np.random.uniform(15, 30)
                    }
                ],
                'trajectory': trajectory,
                'design_parameters': {
                    'casing_program': [
                        {'size': 13.375, 'depth': 500, 'weight': 68, 'grade': 'K55'},
                        {'size': 9.625, 'depth': 2000, 'weight': 47, 'grade': 'N80'},
                        {'size': 7, 'depth': 2600, 'weight': 32, 'grade': 'P110'}
                    ],
                    'production_tubing': {
                        'size': 3.5,
                        'weight': 12.95,
                        'grade': 'L80'
                    },
                    'completion': {
                        'type': 'Perforated' if completion != 'Horizontal' else 'Multi-stage Frac',
                        'perf_intervals': [
                            {'top': 2585, 'bottom': 2615, 'shots_per_ft': 4}
                        ] if completion != 'Horizontal' else None
                    }
                },
                'drilling_parameters': {
                    'planned_days': np.random.uniform(25, 45),
                    'estimated_cost_musd': np.random.uniform(5, 12),
                    'mud_type': 'Oil Based' if completion == 'Horizontal' else 'Water Based',
                    'bit_program': [
                        {'size': 17.5, 'type': 'PDC', 'run_length': 500},
                        {'size': 12.25, 'type': 'PDC', 'run_length': 1500},
                        {'size': 8.5, 'type': 'Impregnated', 'run_length': 600}
                    ]
                },
                'production_forecast': {
                    'initial_rate_bopd': np.random.uniform(500, 2000),
                    'decline_type': 'Hyperbolic',
                    'decline_rate': 0.3,
                    'b_factor': 0.8,
                    'eur_mboe': np.random.uniform(150, 500)
                }
            }
            wells.append(well)
        
        return wells
    
    def _generate_facilities_design(self):
        """Generate synthetic facilities design data"""
        facilities = {
            'processing_plant': {
                'capacity_bopd': 50000,
                'capacity_gas_mmscfd': 150,
                'capacity_water_bwpd': 80000,
                'design_life_years': 25,
                'modules': [
                    {
                        'name': 'Inlet Separation',
                        'type': 'Three-phase Separator',
                        'capacity': 50000,
                        'design_pressure': 300,
                        'design_temperature': 150,
                        'count': 2
                    },
                    {
                        'name': 'Oil Processing',
                        'type': 'Heater Treater',
                        'capacity': 25000,
                        'design_pressure': 150,
                        'design_temperature': 200,
                        'count': 4
                    },
                    {
                        'name': 'Gas Compression',
                        'type': 'Centrifugal Compressor',
                        'capacity': 75,
                        'design_pressure': 1000,
                        'design_temperature': 100,
                        'count': 2
                    },
                    {
                        'name': 'Water Treatment',
                        'type': 'Flotation Unit',
                        'capacity': 40000,
                        'design_pressure': 50,
                        'design_temperature': 80,
                        'count': 2
                    }
                ],
                'utilities': {
                    'power_generation_mw': 20,
                    'fuel_gas_mmscfd': 5,
                      'water_injection_capacity_bwpd': 60000,
                    'flare_capacity_mmscfd': 100
                },
                'safety_systems': {
                    'esd_system': 'SIL-2',
                    'firefighting': 'Deluge + Foam',
                    'gas_detection': 'Point + Open Path',
                    'emergency_power': '2 x 2MW Diesel Generators'
                }
            },
            'export_systems': {
                'oil_pipeline': {
                    'diameter': 16,
                    'length': 120,
                    'capacity_bopd': 45000,
                    'destination': 'Houston Terminal'
                },
                'gas_pipeline': {
                    'diameter': 12,
                    'length': 85,
                    'capacity_mmscfd': 140,
                    'destination': 'Katy Hub'
                }
            },
            'storage': {
                'oil_tanks': [
                    {'capacity_bbl': 100000, 'type': 'Floating Roof', 'count': 4},
                    {'capacity_bbl': 50000, 'type': 'Cone Roof', 'count': 2}
                ],
                'chemical_tanks': [
                    {'capacity_bbl': 5000, 'chemical': 'Methanol', 'count': 2},
                    {'capacity_bbl': 2000, 'chemical': 'Corrosion Inhibitor', 'count': 4}
                ]
            },
            'cost_estimate': {
                'capex_musd': 450,
                'opex_musd_per_year': 35,
                'contingency_percent': 20,
                'breakdown': {
                    'process_equipment': 180,
                    'piping': 60,
                    'instruments': 45,
                    'electrical': 40,
                    'buildings': 25,
                    'civil': 50,
                    'engineering': 50
                }
            }
        }
        
        return facilities
    
    def _generate_drilling_data(self):
        """Generate synthetic drilling data"""
        drilling_data = {
            'rig_information': {
                'rig_name': 'Nabors Rig 245',
                'rig_type': 'Land Rig',
                'max_depth_ft': 30000,
                'hookload_lbs': 1500000,
                'mud_pumps': {
                    'count': 3,
                    'hp_per_pump': 1600,
                    'pressure_rating': 7500
                },
                'top_drive': {
                    'type': 'AC Variable Frequency',
                    'torque_ftlbs': 60000,
                    'speed_rpm': 250
                }
            },
            'daily_reports': [],
            'real_time_data': {
                'parameters': ['WOB', 'RPM', 'ROP', 'Flow Rate', 'SPP', 'Torque'],
                'sampling_rate': 1  # Hz
            },
            'mud_program': {
                'sections': [
                    {
                        'section': 'Surface',
                        'depth_range': '0-2000',
                        'mud_type': 'Water Based',
                        'weight_ppg': 9.5,
                        'viscosity_cp': 35,
                        'additives': ['Bentonite', 'Caustic Soda', 'PAC']
                    },
                    {
                        'section': 'Intermediate',
                        'depth_range': '2000-8000',
                        'mud_type': 'Oil Based',
                        'weight_ppg': 12.5,
                        'viscosity_cp': 45,
                        'additives': ['Barite', 'Emulsifier', 'Lime']
                    },
                    {
                        'section': 'Production',
                        'depth_range': '8000-15000',
                        'mud_type': 'Synthetic Based',
                        'weight_ppg': 14.2,
                        'viscosity_cp': 50,
                        'additives': ['Barite', 'Rheology Modifier', 'Filtration Control']
                    }
                ]
            },
            'bits_and_bha': {
                'bit_program': [
                    {
                        'run': 1,
                        'size': 17.5,
                        'type': 'PDC',
                        'iadc_code': 'M323',
                        'expected_life_hrs': 120,
                        'tfa': 0.85
                    },
                    {
                        'run': 2,
                        'size': 12.25,
                        'type': 'PDC',
                        'iadc_code': 'M324',
                        'expected_life_hrs': 150,
                        'tfa': 0.65
                    }
                ],
                'bha_configurations': [
                    {
                        'type': 'Rotary Steerable',
                        'components': [
                            'Bit', 'RSS Tool', 'MWD', 'LWD', 'Stabilizer', 'Drill Collars'
                        ],
                        'length': 250,
                        'weight': 45000
                    }
                ]
            },
            'casing_program': [
                {
                    'string': 'Conductor',
                    'size': 20,
                    'depth': 500,
                    'weight': 94,
                    'grade': 'K55',
                    'cement_top': 0
                },
                {
                    'string': 'Surface',
                    'size': 13.375,
                    'depth': 4000,
                    'weight': 68,
                    'grade': 'K55',
                    'cement_top': 500
                },
                {
                    'string': 'Intermediate',
                    'size': 9.625,
                    'depth': 10000,
                    'weight': 47,
                    'grade': 'N80',
                    'cement_top': 3500
                },
                {
                    'string': 'Production',
                    'size': 7,
                    'depth': 15000,
                    'weight': 32,
                    'grade': 'P110',
                    'cement_top': 9500
                }
            ]
        }
        
        # Generate daily drilling reports
        for day in range(self.simulation_days):
            report_date = self.start_date + timedelta(days=day)
            
            # Simulate drilling progress
            if day < 30:  # First month: drilling phase
                activity = 'Drilling'
                depth = 15000 * (day / 30)
            elif day < 35:  # Logging
                activity = 'Logging'
                depth = 15000
            elif day < 40:  # Casing
                activity = 'Running Casing'
                depth = 15000
            elif day < 45:  # Cementing
                activity = 'Cementing'
                depth = 15000
            else:  # Rig move or standby
                activity = np.random.choice(['Rig Move', 'Standby', 'Maintenance'])
                depth = 15000
            
            daily_report = {
                'date': report_date.strftime('%Y-%m-%d'),
                'well_id': f"DEV-{np.random.randint(1, 26):03d}",
                'activity': activity,
                'depth_ft': float(depth),
                'progress_ft': 500 if activity == 'Drilling' else 0,
                'hours': 24,
                'npt_hours': np.random.uniform(0, 4),
                'mud_params': {
                    'weight_ppg': 12.5 + np.random.uniform(-0.5, 0.5),
                    'viscosity_cp': 45 + np.random.uniform(-10, 10),
                    'ph': 9.5 + np.random.uniform(-0.5, 0.5)
                },
                'incidents': [] if np.random.random() > 0.1 else [
                    {
                        'type': np.random.choice(['Lost Circulation', 'Stuck Pipe', 'Equipment Failure']),
                        'duration_hours': np.random.uniform(2, 8),
                        'impact': np.random.choice(['Minor', 'Moderate', 'Major'])
                    }
                ],
                'personnel': {
                    'onboard': 45,
                    'manhours': 1080,
                    'incidents': 0 if np.random.random() > 0.95 else 1
                },
                'weather': {
                    'condition': np.random.choice(['Clear', 'Cloudy', 'Rain', 'Windy']),
                    'temp_f': 75 + np.random.uniform(-20, 20),
                    'wind_mph': np.random.uniform(5, 25)
                }
            }
            drilling_data['daily_reports'].append(daily_report)
        
        return drilling_data
    
    def _generate_development_economics(self):
        """Generate synthetic economic analysis for development"""
        # Base case assumptions
        oil_price = 70  # USD/bbl
        gas_price = 3.5  # USD/MMBTU
        inflation_rate = 0.02
        discount_rate = 0.1
        
        # Development schedule
        years = 20
        development_years = 3
        production_years = years - development_years
        
        # Capital expenditures
        capex = {
            'drilling': np.random.uniform(250, 400),  # million USD
            'facilities': np.random.uniform(300, 500),
            'pipelines': np.random.uniform(50, 100),
            'contingency': np.random.uniform(100, 150)
        }
        total_capex = sum(capex.values())
        
        # Operating expenditures
        opex_per_year = np.random.uniform(40, 60)  # million USD/year
        
        # Production profile
        peak_production = self.config['production']['peak_production_bopd']
        decline_rate = 0.15
        
        # Calculate yearly production
        production = []
        revenue = []
        opex = []
        cash_flow = []
        cumulative_cash = 0
        npv = 0
        
        for year in range(years):
            if year < development_years:
                # Development phase
                yearly_production = 0
                yearly_revenue = 0
                yearly_opex = opex_per_year * 0.3  # Reduced Opex during development
                yearly_capex = total_capex / development_years
            else:
                # Production phase
                production_year = year - development_years
                yearly_production = peak_production * np.exp(-decline_rate * production_year)
                yearly_revenue = yearly_production * 365 * oil_price / 1e6  # million USD
                yearly_opex = opex_per_year
                yearly_capex = 0
            
            yearly_cash_flow = yearly_revenue - yearly_opex - (yearly_capex if year < development_years else 0)
            cumulative_cash += yearly_cash_flow
            yearly_npv = yearly_cash_flow / ((1 + discount_rate) ** (year + 1))
            npv += yearly_npv
            
            production.append({
                'year': year + 2024,
                'phase': 'Development' if year < development_years else 'Production',
                'oil_production_bopd': float(yearly_production),
                'gas_production_mmscfd': float(yearly_production * 0.8),  # Assume GOR
                'revenue_musd': float(yearly_revenue),
                'opex_musd': float(yearly_opex),
                'capex_musd': float(yearly_capex) if year < development_years else 0,
                'cash_flow_musd': float(yearly_cash_flow),
                'cumulative_cash_musd': float(cumulative_cash),
                'npv_contribution_musd': float(yearly_npv)
            })
        
        # Economic indicators
        irr = np.random.uniform(0.18, 0.30)
        payback = np.random.uniform(4, 7)  # years
        
        return {
            'base_case': {
                'oil_price_usd_bbl': oil_price,
                'gas_price_usd_mmbtu': gas_price,
                'discount_rate': discount_rate,
                'inflation_rate': inflation_rate,
                'project_life_years': years
            },
            'capital_expenditure': {
                **capex,
                'total_capex_musd': total_capex,
                'schedule': {
                    'year_1': total_capex * 0.4,
                    'year_2': total_capex * 0.4,
                    'year_3': total_capex * 0.2
                }
            },
            'operating_expenditure': {
                'fixed_musd_per_year': opex_per_year * 0.6,
                'variable_usd_per_bbl': 5.5,
                'total_opex_musd': opex_per_year * production_years
            },
            'production_profile': production,
            'economic_indicators': {
                'net_present_value_musd': float(npv),
                'internal_rate_of_return': float(irr),
                'payback_period_years': float(payback),
                'profitability_index': float(npv / total_capex),
                'breakeven_price_usd_bbl': float(np.random.uniform(45, 55))
            },
            'sensitivity_analysis': {
                'oil_price': {
                    '-20%': float(npv * 0.7),
                    '-10%': float(npv * 0.85),
                    'base': float(npv),
                    '+10%': float(npv * 1.15),
                    '+20%': float(npv * 1.3)
                },
                'capex': {
                    '+20%': float(npv * 0.8),
                    '+10%': float(npv * 0.9),
                    'base': float(npv),
                    '-10%': float(npv * 1.1),
                    '-20%': float(npv * 1.2)
                }
            },
            'fiscal_terms': {
                'royalty_rate': 0.125,
                'tax_rate': 0.35,
                'bonus_payment': 25,  # million USD
                'profit_oil_split': 'R/T 65/35'  # Government/Contractor
            }
        }
    
    def _generate_development_risks(self):
        """Generate synthetic risk analysis for development"""
        risks = {
            'technical_risks': [
                {
                    'id': 'TR-001',
                    'description': 'Reservoir performance below expectations',
                    'probability': 0.25,
                    'impact': 'High',
                    'mitigation': 'Appraisal drilling, improved reservoir characterization',
                    'owner': 'Reservoir Engineering'
                },
                {
                    'id': 'TR-002',
                    'description': 'Drilling complications in challenging formations',
                    'probability': 0.30,
                    'impact': 'Medium',
                    'mitigation': 'Advanced drilling technologies, contingency plans',
                    'owner': 'Drilling Engineering'
                },
                {
                    'id': 'TR-003',
                    'description': 'Facilities design capacity mismatch',
                    'probability': 0.15,
                    'impact': 'Medium',
                    'mitigation': 'Modular design, flexibility in capacity',
                    'owner': 'Facilities Engineering'
                }
            ],
            'economic_risks': [
                {
                    'id': 'ER-001',
                    'description': 'Oil price volatility',
                    'probability': 0.40,
                    'impact': 'High',
                    'mitigation': 'Hedging strategies, conservative price assumptions',
                    'owner': 'Commercial'
                },
                {
                    'id': 'ER-002',
                    'description': 'Cost overruns in development',
                    'probability': 0.35,
                    'impact': 'High',
                    'mitigation': 'Fixed-price contracts, contingency budget',
                    'owner': 'Project Management'
                }
            ],
            'schedule_risks': [
                {
                    'id': 'SR-001',
                    'description': 'Regulatory approval delays',
                    'probability': 0.20,
                    'impact': 'Medium',
                    'mitigation': 'Early engagement with regulators',
                    'owner': 'Regulatory Affairs'
                },
                {
                    'id': 'SR-002',
                    'description': 'Supply chain disruptions',
                    'probability': 0.25,
                    'impact': 'Medium',
                    'mitigation': 'Dual sourcing, strategic stockpiling',
                    'owner': 'Procurement'
                }
            ],
            'environmental_risks': [
                {
                    'id': 'ENV-001',
                    'description': 'Environmental incidents during construction',
                    'probability': 0.10,
                    'impact': 'High',
                    'mitigation': 'Strict environmental controls, monitoring',
                    'owner': 'HSE'
                }
            ],
            'risk_matrix': {
                'high_high': ['TR-001', 'ER-001', 'ER-002'],
                'high_medium': ['TR-002', 'TR-003'],
                'medium_medium': ['SR-001', 'SR-002'],
                'medium_low': [],
                'low_low': []
            },
            'quantitative_risk_assessment': {
                'p10_npv_musd': np.random.uniform(400, 500),
                'p50_npv_musd': np.random.uniform(300, 400),
                'p90_npv_musd': np.random.uniform(200, 300),
                'expected_monetary_value_musd': np.random.uniform(350, 450),
                'value_at_risk_95_percent': np.random.uniform(-50, -100)
            }
        }
        
        return risks
    
    # =========================================================================
    # PHASE 3: PRODUCTION & OPERATIONS DATA GENERATION
    # =========================================================================
    
    def generate_production_data(self):
        """Generate synthetic production and operations data"""
        print("\n" + "="*60)
        print("PHASE 3: Generating Production & Operations Data")
        print("="*60)
        
        # 3.1 Well Production Data (High Frequency - 1Hz)
        print("Generating well production data (1Hz)...")
        well_production = self._generate_well_production_data()
        self.production_data['well_production'] = well_production
        
        # 3.2 Process Plant Data
        print("Generating process plant data...")
        process_data = self._generate_process_plant_data()
        self.production_data['process_plant'] = process_data
        
        # 3.3 Equipment Monitoring Data
        print("Generating equipment monitoring data...")
        equipment_data = self._generate_equipment_monitoring()
        self.production_data['equipment_monitoring'] = equipment_data
        
        # 3.4 Energy Optimization Data
        print("Generating energy optimization data...")
        energy_data = self._generate_energy_data()
        self.production_data['energy'] = energy_data
        
        # 3.5 Environmental Monitoring Data
        print("Generating environmental monitoring data...")
        environmental_data = self._generate_environmental_data()
        self.production_data['environmental'] = environmental_data
        
        # 3.6 Daily Operations Data
        print("Generating daily operations data...")
        operations_data = self._generate_daily_operations()
        self.production_data['operations'] = operations_data
        
        print("Production data generation complete!")
        return self.production_data
    
    def _generate_well_production_data(self):
        """Generate high-frequency well production data (1Hz for 6 months)"""
        well_count = 25
        wells_data = {}
        
        for well_idx in range(well_count):
            well_id = f"PROD-{well_idx+1:03d}"
            
            # Base production rates with decline
            base_oil_rate = 1000 + np.random.uniform(-200, 200)  # BOPD
            decline_rate = 0.000001  # Very small decline per second
            
            # Calculate rates over time with decline
            time_seconds = np.arange(self.total_seconds)
            oil_rate = base_oil_rate * np.exp(-decline_rate * time_seconds)
            
            # Add daily fluctuations (higher during day)
            daily_cycle = 0.1 * np.sin(2 * np.pi * time_seconds / 86400)
            oil_rate *= (1 + daily_cycle)
            
            # Add random noise
            oil_rate += np.random.normal(0, 10, self.total_seconds)
            oil_rate = np.maximum(oil_rate, 50)  # Minimum production
            
            # Calculate gas and water rates
            gor = 800 + np.random.normal(0, 50)  # SCF/STB
            wcut = 0.3 + 0.0000001 * time_seconds  # Increasing water cut
            
            gas_rate = oil_rate * gor / 1000  # MCFD
            water_rate = oil_rate * wcut / (1 - wcut)
            
            # Wellhead pressure
            whp = 1200 - 0.0001 * time_seconds + 100 * np.sin(2 * np.pi * time_seconds / 43200)
            whp += np.random.normal(0, 20, self.total_seconds)
            
            # Temperature
            temp = 180 + 5 * np.sin(2 * np.pi * time_seconds / 86400)
            temp += np.random.normal(0, 2, self.total_seconds)
            
            # Choke position
            choke = 64 + 4 * np.sin(2 * np.pi * time_seconds / 21600)
            choke += np.random.normal(0, 1, self.total_seconds)
            choke = np.clip(choke, 32, 96)
            
            wells_data[well_id] = {
                'time': self.time_index,
                'oil_rate_bopd': oil_rate,
                'gas_rate_mcfd': gas_rate,
                'water_rate_bwpd': water_rate,
                'wellhead_pressure_psi': whp,
                'wellhead_temperature_f': temp,
                'choke_position_percent': choke,
                'gor_scf_stb': np.full_like(oil_rate, gor),
                'water_cut_percent': wcut * 100
            }
        
        return wells_data
    
    def _generate_process_plant_data(self):
        """Generate process plant operational data"""
        # Separator data
        separator_count = 4
        separators = {}
        
        for sep_idx in range(separator_count):
            sep_id = f"SEP-{sep_idx+1:01d}"
            
            # Generate 1Hz data for key parameters
            time_seconds = np.arange(self.total_seconds)
            
            # Pressure with normal operation variations
            pressure = 150 + 5 * np.sin(2 * np.pi * time_seconds / 3600)
            pressure += np.random.normal(0, 2, self.total_seconds)
            
            # Temperature
            temp = 120 + 3 * np.sin(2 * np.pi * time_seconds / 7200)
            temp += np.random.normal(0, 1.5, self.total_seconds)
            
            # Levels
            oil_level = 50 + 10 * np.sin(2 * np.pi * time_seconds / 1800)
            water_level = 40 + 8 * np.sin(2 * np.pi * time_seconds / 2700)
            
            separators[sep_id] = {
                'time': self.time_index,
                'pressure_psi': pressure,
                'temperature_f': temp,
                'oil_level_percent': oil_level,
                'water_level_percent': water_level,
                'interface_level_percent': (oil_level + water_level) / 2
            }
        
        # Compressor data
        compressors = {}
        for comp_idx in range(2):
            comp_id = f"COMP-{comp_idx+1:01d}"
            
            # Vibration patterns
            vibration = 1.0 + 0.2 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 1800)
            
            # Add bearing wear progression (if applicable)
            if comp_idx == 0:  # First compressor develops fault
                fault_start = 30 * 86400  # Day 30
                fault_duration = 30 * 86400  # 30 days
                
                wear_progression = np.zeros(self.total_seconds)
                wear_progression[fault_start:] = 0.00001 * np.arange(self.total_seconds - fault_start)
                wear_progression = np.minimum(wear_progression, 5.0)  # Max 5 mm/s increase
                
                vibration += wear_progression
            
            # Bearing temperatures
            bearing_temp = 160 + 5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 3600)
            bearing_temp += np.random.normal(0, 3, self.total_seconds)
            
            compressors[comp_id] = {
                'time': self.time_index,
                'suction_pressure_psi': 100 + np.random.normal(0, 5, self.total_seconds),
                'discharge_pressure_psi': 1000 + np.random.normal(0, 20, self.total_seconds),
                'speed_rpm': np.full(self.total_seconds, 12000),
                'vibration_mm_s': vibration,
                'bearing_temperature_f': bearing_temp,
                'power_consumption_kw': 5000 + np.random.normal(0, 100, self.total_seconds)
            }
        
        # Heat exchanger data
        exchangers = {}
        for exch_idx in range(6):
            exch_id = f"HX-{exch_idx+1:01d}"
            
            # Generate fouling progression
            time_days = np.arange(self.total_seconds) / 86400
            
            if exch_idx < 2:  # Some exchangers develop fouling
                fouling_rate = 0.001  # per day
                fouling_factor = 1 + fouling_rate * time_days
            else:
                fouling_factor = np.ones_like(time_days)
            
            # Temperature differences affected by fouling
            delta_t = 50 / fouling_factor + 5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 10800)
            
            exchangers[exch_id] = {
                'time': self.time_index,
                'tube_inlet_temp_f': 200 + np.random.normal(0, 3, self.total_seconds),
                'tube_outlet_temp_f': 200 - delta_t,
                'shell_inlet_temp_f': 80 + np.random.normal(0, 2, self.total_seconds),
                'shell_outlet_temp_f': 80 + delta_t * 0.8,
                'delta_p_tube_psi': 10 * fouling_factor,
                'delta_p_shell_psi': 8 * fouling_factor,
                'fouling_factor': fouling_factor
            }
        
        # Product quality data
        product_quality = {
            'time': self.time_index,
            'oil_gravity_api': 35 + 0.5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 86400),
            'oil_bs_w_percent': 0.2 + 0.05 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 43200),
            'gas_heating_value_btu_scf': 1050 + 20 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 21600),
            'water_salinity_ppm': 50000 + 5000 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 172800)
        }
        
        return {
            'separators': separators,
            'compressors': compressors,
            'heat_exchangers': exchangers,
            'product_quality': product_quality,
            'overall_plant': {
                'throughput_bopd': 45000 + 2000 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 86400),
                'availability_percent': 98 + 1 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 2592000),
                'efficiency_percent': 92 + 2 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 604800)
            }
        }
    
    def _generate_equipment_monitoring(self):
        """Generate equipment condition monitoring data"""
        equipment = {}
        
        # Pump monitoring
        pump_count = 8
        for pump_idx in range(pump_count):
            pump_id = f"PUMP-{pump_idx+1:02d}"
            
            # Vibration data with potential faults
            vibration_x = 0.5 + 0.1 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 900)
            vibration_y = 0.5 + 0.1 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 900 + np.pi/4)
            vibration_z = 0.5 + 0.1 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 900 + np.pi/2)
            
            # Add imbalance if pump develops fault
            if pump_idx == 2:  # Third pump has imbalance
                fault_start = 45 * 86400
                imbalance = np.zeros(self.total_seconds)
                imbalance[fault_start:] = 0.00002 * np.arange(self.total_seconds - fault_start)
                vibration_x += imbalance
                vibration_y += imbalance * 0.8
            
            equipment[pump_id] = {
                'time': self.time_index,
                'vibration_x_mm_s': vibration_x,
                'vibration_y_mm_s': vibration_y,
                'vibration_z_mm_s': vibration_z,
                'bearing_temperature_f': 160 + 5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 1800),
                'motor_current_a': 50 + 5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 3600),
                'flow_rate_gpm': 500 + 50 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 7200),
                'differential_pressure_psi': 100 + 10 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 5400)
            }
        
        # Valve monitoring
        valve_count = 12
        for valve_idx in range(valve_count):
            valve_id = f"VALVE-{valve_idx+1:02d}"
            
            # Some valves develop stiction
            position = 50 + 20 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 10800)
            
            if valve_idx == 3:  # Fourth valve has stiction
                stiction_start = 60 * 86400
                stiction_effect = np.zeros(self.total_seconds)
                stiction_effect[stiction_start:] = 5 * np.sin(2 * np.pi * np.arange(self.total_seconds - stiction_start) / 3600)
                position += stiction_effect
            
            equipment[valve_id] = {
                'time': self.time_index,
                'position_percent': position,
                'air_supply_psi': 80 + np.random.normal(0, 2, self.total_seconds),
                'stem_position_in': position * 0.2,
                'valve_travel_time_s': np.full(self.total_seconds, 30)
            }
        
        # Tank monitoring
        tank_count = 6
        for tank_idx in range(tank_count):
            tank_id = f"TANK-{tank_idx+1:02d}"
            
            # Level with filling/emptying cycles
            if tank_idx < 3:  # Oil tanks
                level = 60 + 20 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 86400)
            else:  # Water tanks
                level = 40 + 15 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 43200)
            
            equipment[tank_id] = {
                'time': self.time_index,
                'level_percent': level,
                'temperature_f': 80 + 10 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 21600),
                'pressure_psi': 0.5 + 0.1 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 10800),
                'water_bottom_in': np.full(self.total_seconds, 2.0)
            }
        
        return equipment
    
    def _generate_energy_data(self):
        """Generate energy consumption and optimization data"""
        # Power generation and consumption
        time_seconds = np.arange(self.total_seconds)
        
        # Power generation (gas turbines)
        power_generation = 18000 + 1000 * np.sin(2 * np.pi * time_seconds / 86400)
        
        # Power consumption by major equipment
        compressor_power = 5000 + 500 * np.sin(2 * np.pi * time_seconds / 7200)
        pump_power = 3000 + 300 * np.sin(2 * np.pi * time_seconds / 5400)
        lighting_power = 500 + 50 * np.sin(2 * np.pi * time_seconds / 86400)
        other_power = 2000 + 200 * np.sin(2 * np.pi * time_seconds / 10800)
        
        total_consumption = compressor_power + pump_power + lighting_power + other_power
        
        # Fuel gas consumption
        fuel_gas = 5 + 0.5 * np.sin(2 * np.pi * time_seconds / 86400)  # MMSCFD
        
        # Steam system
        steam_generation = 20000 + 2000 * np.sin(2 * np.pi * time_seconds / 43200)  # lb/hr
        steam_consumption = 18000 + 1500 * np.sin(2 * np.pi * time_seconds / 43200)
        
        # Cooling water system
        cooling_water_flow = 5000 + 500 * np.sin(2 * np.pi * time_seconds / 7200)  # gpm
        
        # Energy intensity metrics
        energy_intensity = total_consumption / (45000 / 24 / 3600)  # kW per bbl/day
        
        return {
            'time': self.time_index,
            'power_generation_kw': power_generation,
            'power_consumption_kw': total_consumption,
            'power_balance_kw': power_generation - total_consumption,
            'fuel_gas_mmscfd': fuel_gas,
            'steam_generation_lb_hr': steam_generation,
            'steam_consumption_lb_hr': steam_consumption,
            'cooling_water_flow_gpm': cooling_water_flow,
            'energy_intensity_kw_per_bbl': energy_intensity,
            'equipment_breakdown': {
                'compressors_kw': compressor_power,
                'pumps_kw': pump_power,
                'lighting_kw': lighting_power,
                'other_kw': other_power
            },
            'optimization_opportunities': [
                {
                    'id': 'OPT-001',
                    'description': 'Compressor load optimization',
                    'potential_saving_kw': 500,
                    'payback_months': 18
                },
                {
                    'id': 'OPT-002',
                    'description': 'Heat exchanger network optimization',
                    'potential_saving_mmbtu_hr': 10,
                    'payback_months': 24
                },
                {
                    'id': 'OPT-003',
                    'description': 'Variable speed drives on pumps',
                    'potential_saving_kw': 300,
                    'payback_months': 30
                }
            ]
        }
    
    def _generate_environmental_data(self):
        """Generate environmental monitoring data"""
        # Air emissions
        time_seconds = np.arange(self.total_seconds)
        
        # Flare data
        flare_gas = 0.5 + 0.1 * np.sin(2 * np.pi * time_seconds / 86400)
        
        # Emissions
        so2_emissions = 10 + 2 * np.sin(2 * np.pi * time_seconds / 43200)
        nox_emissions = 30 + 5 * np.sin(2 * np.pi * time_seconds / 43200)
        co_emissions = 50 + 10 * np.sin(2 * np.pi * time_seconds / 43200)
        voc_emissions = 100 + 20 * np.sin(2 * np.pi * time_seconds / 43200)
        
        # Water discharge
        produced_water = 80000 + 5000 * np.sin(2 * np.pi * time_seconds / 86400)
        discharge_water = 20000 + 2000 * np.sin(2 * np.pi * time_seconds / 86400)
        discharge_oil_content = 10 + 2 * np.sin(2 * np.pi * time_seconds / 43200)  # ppm
        
        # Waste generation
        solid_waste = 2.5 + 0.3 * np.sin(2 * np.pi * time_seconds / 2592000)  # tons/day
        hazardous_waste = 0.5 + 0.1 * np.sin(2 * np.pi * time_seconds / 2592000)
        
        # Ambient monitoring
        ambient_so2 = 0.5 + 0.1 * np.sin(2 * np.pi * time_seconds / 86400)
        ambient_nox = 2 + 0.5 * np.sin(2 * np.pi * time_seconds / 86400)
        ambient_voc = 5 + 1 * np.sin(2 * np.pi * time_seconds / 86400)
        
        return {
            'time': self.time_index,
            'flare_gas_mmscfd': flare_gas,
            'emissions_kg_hr': {
                'so2': so2_emissions,
                'nox': nox_emissions,
                'co': co_emissions,
                'voc': voc_emissions
            },
            'water_management': {
                'produced_water_bwpd': produced_water,
                'discharge_water_bwpd': discharge_water,
                'discharge_oil_content_ppm': discharge_oil_content,
                'injection_water_bwpd': produced_water - discharge_water
            },
            'waste_management': {
                'solid_waste_tpd': solid_waste,
                'hazardous_waste_tpd': hazardous_waste,
                'recycling_rate_percent': 75 + 5 * np.sin(2 * np.pi * time_seconds / 2592000)
            },
            'ambient_monitoring': {
                'so2_ppm': ambient_so2,
                'nox_ppm': ambient_nox,
                'voc_ppm': ambient_voc,
                'pm10_ug_m3': 20 + 5 * np.sin(2 * np.pi * time_seconds / 86400)
            },
            'compliance_status': {
                'air': 'In Compliance',
                'water': 'In Compliance',
                'waste': 'In Compliance'
            },
            'environmental_incidents': [
                {
                    'date': (self.start_date + timedelta(days=30)).strftime('%Y-%m-%d'),
                    'type': 'Minor spill',
                    'quantity_bbl': 2.5,
                    'impact': 'Contained on site',
                    'downtime_hours': 8
                },
                {
                    'date': (self.start_date + timedelta(days=90)).strftime('%Y-%m-%d'),
                    'type': 'Exceedance - SO2 emissions',
                    'quantity': '5% above limit',
                    'impact': 'Warning issued',
                    'downtime_hours': 0
                }
            ]
        }
    
    def _generate_daily_operations(self):
        """Generate daily operations data"""
        daily_ops = []
        
        for day in range(self.simulation_days):
            ops_date = self.start_date + timedelta(days=day)
            
            # Production for the day
            daily_oil = 45000 + np.random.normal(0, 2000)
            daily_gas = daily_oil * 800 / 1000  # MCFD
            daily_water = daily_oil * 0.4
            
            # Operations metrics
            downtime_hours = 0 if np.random.random() > 0.1 else np.random.uniform(2, 12)
            availability = 100 * (24 - downtime_hours) / 24
            
            # Maintenance activities
            maintenance_activities = []
            if np.random.random() > 0.7:  # 30% chance of maintenance
                maintenance_activities.append({
                    'type': np.random.choice(['Preventive', 'Corrective']),
                    'equipment': np.random.choice(['Pump', 'Compressor', 'Valve', 'Exchanger']),
                    'duration_hours': np.random.uniform(4, 16),
                    'description': 'Routine maintenance'
                })
            
            # Safety metrics
            safety_metrics = {
                'manhours': 1080,
                'recordable_incidents': 0 if np.random.random() > 0.98 else 1,
                'near_misses': np.random.randint(0, 3),
                'safety_observations': np.random.randint(5, 15)
            }
            
            # Environmental metrics
            flaring = 0.5 + np.random.normal(0, 0.1)
            emissions = {
                'so2_kg': 240 + np.random.normal(0, 20),
                'nox_kg': 720 + np.random.normal(0, 50),
                'co_kg': 1200 + np.random.normal(0, 100)
            }
            
            daily_report = {
                'date': ops_date.strftime('%Y-%m-%d'),
                'production': {
                    'oil_bopd': float(daily_oil),
                    'gas_mcfd': float(daily_gas),
                    'water_bwpd': float(daily_water),
                    'gor_scf_stb': 800,
                    'water_cut_percent': 28.6
                },
                'operations': {
                    'uptime_hours': 24 - downtime_hours,
                    'downtime_hours': downtime_hours,
                    'availability_percent': float(availability),
                    'efficiency_percent': 92 + np.random.uniform(-2, 2),
                    'throughput_bopd': float(daily_oil)
                },
                'maintenance': {
                    'activities': maintenance_activities,
                    'backlog_count': np.random.randint(5, 20),
                    'preventive_completed': np.random.randint(2, 8),
                    'corrective_completed': np.random.randint(0, 3)
                },
                'safety': safety_metrics,
                'environmental': {
                    'flare_gas_mmscfd': float(flaring),
                    'emissions': emissions,
                    'produced_water_bwpd': float(daily_water),
                    'discharged_water_bwpd': float(daily_water * 0.25)
                },
                'personnel': {
                    'onsite': 85,
                    'contractors': 45,
                    'total_manhours': 3120
                },
                'issues': [] if np.random.random() > 0.2 else [
                    {
                        'type': np.random.choice(['Equipment', 'Process', 'Logistics', 'Weather']),
                        'description': 'Temporary issue affecting operations',
                        'impact': np.random.choice(['Minor', 'Moderate']),
                        'resolution': 'Resolved within shift'
                    }
                ]
            }
            
            daily_ops.append(daily_report)
        
        return daily_ops
    
    # =========================================================================
    # PHASE 4: MAINTENANCE & REVAMP DATA GENERATION
    # =========================================================================
    
    def generate_maintenance_data(self):
        """Generate synthetic maintenance and revamp data"""
        print("\n" + "="*60)
        print("PHASE 4: Generating Maintenance & Revamp Data")
        print("="*60)
        
        # 4.1 Condition Monitoring Data
        print("Generating condition monitoring data...")
        condition_data = self._generate_condition_monitoring()
        self.maintenance_data['condition_monitoring'] = condition_data
        
        # 4.2 Inspection Data
        print("Generating inspection data...")
        inspection_data = self._generate_inspection_data()
        self.maintenance_data['inspections'] = inspection_data
        
        # 4.3 Maintenance History
        print("Generating maintenance history...")
        maintenance_history = self._generate_maintenance_history()
        self.maintenance_data['history'] = maintenance_history
        
        # 4.4 Spare Parts Inventory
        print("Generating spare parts inventory data...")
        inventory_data = self._generate_inventory_data()
        self.maintenance_data['inventory'] = inventory_data
        
        # 4.5 Revamp Project Data
        print("Generating revamp project data...")
        revamp_data = self._generate_revamp_data()
        self.maintenance_data['revamp'] = revamp_data
        
        # 4.6 Predictive Maintenance Models
        print("Generating predictive maintenance model data...")
        predictive_data = self._generate_predictive_maintenance()
        self.maintenance_data['predictive'] = predictive_data
        
        print("Maintenance data generation complete!")
        return self.maintenance_data
    
    def _generate_condition_monitoring(self):
        """Generate condition monitoring data for critical equipment"""
        condition_data = {}
        
        # Vibration analysis for rotating equipment
        equipment_list = ['COMP-01', 'COMP-02', 'PUMP-01', 'PUMP-02', 'PUMP-03', 'GEN-01']
        
        for equipment in equipment_list:
            # Generate vibration spectrum data (weekly)
            weekly_points = self.simulation_days // 7
            spectra = []
            
            for week in range(weekly_points):
                # Base frequencies
                freq = np.linspace(1, 1000, 500)  # 1-1000 Hz
                
                # Running speed frequency (1X)
                running_speed = 30  # Hz (1800 RPM)
                
                # Generate spectrum with peaks at harmonics
                spectrum = np.random.exponential(0.01, len(freq))
                
                # Add running speed harmonic
                for harmonic in [1, 2, 3, 4]:
                    harmonic_freq = running_speed * harmonic
                    idx = np.argmin(np.abs(freq - harmonic_freq))
                    amplitude = 0.1 / harmonic  # Amplitude decreases with harmonic
                    
                    # Increase amplitude over time if equipment is degrading
                    if 'COMP-01' in equipment and week > 20:  # Compressor 1 degrades
                        amplitude *= (1 + 0.05 * (week - 20))
                    
                    spectrum[idx] += amplitude
                
                # Add bearing frequencies if present
                if week > 15 and equipment in ['COMP-01', 'PUMP-02']:
                    bearing_freq = 4.7 * running_speed  # BPFO
                    idx = np.argmin(np.abs(freq - bearing_freq))
                    spectrum[idx] += 0.05 * (1 + 0.1 * (week - 15))
                
                spectra.append({
                    'week': week + 1,
                    'date': (self.start_date + timedelta(days=week*7)).strftime('%Y-%m-%d'),
                    'frequencies_hz': freq.tolist(),
                    'amplitudes_mm_s': spectrum.tolist(),
                    'overall_vibration_mm_s': float(np.sqrt(np.sum(spectrum**2)))
                })
            
            condition_data[equipment] = {
                'type': 'Rotating Equipment',
                'monitoring_method': 'Vibration Analysis',
                'spectra': spectra,
                'trends': {
                    'overall_vibration': [s['overall_vibration_mm_s'] for s in spectra],
                    'bearing_condition_index': [np.random.uniform(0.8, 1.2) for _ in spectra],
                    'alignment_condition': [np.random.uniform(0.9, 1.1) for _ in spectra]
                }
            }
        
        # Thermography data
        thermography_data = []
        monthly_points = self.simulation_days // 30
        
        for month in range(monthly_points):
            scan_date = self.start_date + timedelta(days=month*30)
            
            # Electrical equipment thermography
            electrical_points = [
                {
                    'component': 'Motor Terminal Box',
                    'location': 'PUMP-01',
                    'measured_temp_f': 120 + np.random.uniform(-10, 10),
                    'ambient_temp_f': 85,
                    'delta_t': 35 + np.random.uniform(-5, 5),
                    'severity': 'Normal' if np.random.random() > 0.1 else 'Warning'
                },
                {
                    'component': 'Transformer Bushings',
                    'location': 'Substation A',
                    'measured_temp_f': 110 + np.random.uniform(-8, 8),
                    'ambient_temp_f': 85,
                    'delta_t': 25 + np.random.uniform(-4, 4),
                    'severity': 'Normal'
                }
            ]
            
            # Mechanical equipment thermography
            mechanical_points = [
                {
                    'component': 'Bearing Housing',
                    'location': 'COMP-01',
                    'measured_temp_f': 165 + np.random.uniform(-15, 15),
                    'ambient_temp_f': 100,
                    'delta_t': 65 + np.random.uniform(-10, 10),
                    'severity': 'Warning' if month > 3 else 'Normal'
                }
            ]
            
            thermography_data.append({
                'date': scan_date.strftime('%Y-%m-%d'),
                'electrical': electrical_points,
                'mechanical': mechanical_points,
                'summary': {
                    'critical_findings': 0 if np.random.random() > 0.2 else 1,
                    'recommendations': np.random.randint(2, 6)
                }
            })
        
        condition_data['thermography'] = thermography_data
        
        # Oil analysis data
        oil_analysis = []
        for month in range(monthly_points):
            sample_date = self.start_date + timedelta(days=month*30)
            
            analysis = {
                'date': sample_date.strftime('%Y-%m-%d'),
                'equipment': 'COMP-01',
                'oil_type': 'ISO VG 46',
                'parameters': {
                    'viscosity_cst': 46 + np.random.uniform(-2, 2),
                    'water_content_ppm': np.random.uniform(100, 300),
                    'acid_number_mg_koh_g': 0.2 + np.random.uniform(0, 0.05),
                    'base_number_mg_koh_g': 8.5 + np.random.uniform(-0.5, 0.5),
                    'particle_count': np.random.randint(1000, 5000),
                    'iron_ppm': 20 + np.random.uniform(0, 5),
                    'copper_ppm': 5 + np.random.uniform(0, 2),
                    'silicon_ppm': 10 + np.random.uniform(0, 3)
                },
                'wear_particles': {
                    'cutting': 'Normal',
                    'rubbing': 'Normal',
                    'fatigue': 'Low' if month < 4 else 'Increasing',
                    'severe_sliding': 'None'
                },
                'recommendation': 'Continue Service' if np.random.random() > 0.1 else 'Change Oil'
            }
            
            # Trend for iron (wear metal)
            if month > 4:
                analysis['parameters']['iron_ppm'] += 2 * (month - 4)
            
            oil_analysis.append(analysis)
        
        condition_data['oil_analysis'] = oil_analysis
        
        # Ultrasound data
        ultrasound_data = []
        for week in range(weekly_points):
            scan_date = self.start_date + timedelta(days=week*7)
            
            data = {
                'date': scan_date.strftime('%Y-%m-%d'),
                'equipment': 'VALVE-01',
                'measurements': {
                    'leak_detection_db': 25 + np.random.uniform(-5, 5),
                    'bearing_condition_db': 20 + np.random.uniform(-3, 3),
                    'electrical_condition_db': 15 + np.random.uniform(-2, 2)
                },
                'findings': 'Normal' if np.random.random() > 0.15 else 'Minor Leak Detected'
            }
            
            ultrasound_data.append(data)
        
        condition_data['ultrasound'] = ultrasound_data
        
        return condition_data
    
    def _generate_inspection_data(self):
        """Generate inspection and testing data"""
        inspections = {
            'pressure_vessels': [],
            'piping_systems': [],
            'safety_devices': [],
            'structural': []
        }
        
        # Pressure vessel inspections (annual and turnaround)
        vessel_list = ['V-101', 'V-102', 'V-201', 'V-301']
        
        for vessel in vessel_list:
            # Initial baseline inspection
            baseline = {
                'equipment': vessel,
                'date': (self.start_date - timedelta(days=365)).strftime('%Y-%m-%d'),
                'type': 'Baseline',
                'methods': ['Visual', 'UT Thickness', 'MPI'],
                'findings': 'No significant defects',
                'remaining_life_years': 20,
                'next_inspection': self.start_date.strftime('%Y-%m-%d')
            }
            
            # Current inspection during simulation period
            inspection_date = self.start_date + timedelta(days=np.random.randint(30, 150))
            
            # Simulate some degradation
            corrosion_rate = np.random.uniform(0.01, 0.05)  # mm/year
            months_since_baseline = 12 + (inspection_date - self.start_date).days / 30
            thickness_loss = corrosion_rate * months_since_baseline / 12
            
            current = {
                'equipment': vessel,
                'date': inspection_date.strftime('%Y-%m-%d'),
                'type': 'Routine',
                'methods': ['Visual', 'UT Thickness'],
                'measurements': {
                    'min_thickness_mm': 12.5 - thickness_loss,
                    'corrosion_rate_mm_yr': corrosion_rate,
                    'defects': [] if np.random.random() > 0.3 else [
                        {
                            'type': 'Localized Corrosion',
                            'size': '10mm diameter',
                            'depth': '1.2mm',
                            'severity': 'Low'
                        }
                    ]
                },
                'findings': 'Within acceptable limits' if np.random.random() > 0.2 else 'Monitor corrosion',
                'remaining_life_years': 20 - thickness_loss / corrosion_rate,
                'recommendations': 'Continue in service',
                'next_inspection': (inspection_date + timedelta(days=365)).strftime('%Y-%m-%d')
            }
            
            inspections['pressure_vessels'].extend([baseline, current])
        
        # Piping system inspections
        piping_systems = ['Crude Oil', 'Gas', 'Produced Water', 'Fuel Gas']
        
        for system in piping_systems:
            for inspection in range(2):  # Two inspections during period
                inspection_date = self.start_date + timedelta(days=np.random.randint(60, 180))
                
                inspection_data = {
                    'system': system,
                    'date': inspection_date.strftime('%Y-%m-%d'),
                    'type': 'CUI Survey' if 'Gas' in system else 'Thickness Survey',
                    'coverage_percent': 100,
                    'findings': {
                        'corrosion_rate_mm_yr': np.random.uniform(0.02, 0.08),
                        'cui_locations': np.random.randint(0, 3),
                        'support_condition': 'Good' if np.random.random() > 0.1 else 'Needs attention'
                    },
                    'repairs_required': [] if np.random.random() > 0.4 else [
                        {
                            'location': 'Section 45A',
                            'type': 'Pipe Support Replacement',
                            'priority': 'Medium'
                        }
                    ]
                }
                
                inspections['piping_systems'].append(inspection_data)
        
        # Safety device testing
        safety_devices = ['PSV-101', 'PSV-102', 'PSV-201', 'BDV-301', 'RV-401']
        
        for device in safety_devices:
            # Quarterly testing
            for quarter in range(2):  # Two quarters in 6 months
                test_date = self.start_date + timedelta(days=quarter*90 + np.random.randint(0, 30))
                
                test_data = {
                    'device': device,
                    'date': test_date.strftime('%Y-%m-%d'),
                    'type': 'Function Test',
                    'set_pressure': 150 + np.random.uniform(-5, 5),
                    'test_pressure': 150,
                    'result': 'Pass' if np.random.random() > 0.05 else 'Fail - Stuck',
                    'action': 'Recertified' if np.random.random() > 0.05 else 'Replaced'
                }
                
                inspections['safety_devices'].append(test_data)
        
        # Structural inspections
        structures = ['Pipe Rack A', 'Flare Stack', 'Tank Farm', 'Control Building']
        
        for structure in structures:
            inspection_date = self.start_date + timedelta(days=np.random.randint(90, 180))
            
            structural_data = {
                'structure': structure,
                'date': inspection_date.strftime('%Y-%m-%d'),
                'type': 'Visual Inspection',
                'condition': 'Good' if np.random.random() > 0.2 else 'Fair',
                'findings': [] if np.random.random() > 0.3 else [
                    'Minor corrosion at base',
                    'Paint degradation'
                ],
                'recommendations': 'Continue monitoring' if np.random.random() > 0.1 else 'Schedule maintenance'
            }
            
            inspections['structural'].append(structural_data)
        
        return inspections
    
    def _generate_maintenance_history(self):
        """Generate maintenance work order history"""
        work_orders = []
        
        # Generate work orders for 6 months
        wo_count = 150  # Approximately 1 work order per day
        
        for wo_idx in range(wo_count):
            wo_date = self.start_date + timedelta(days=np.random.randint(0, self.simulation_days),
                                                 hours=np.random.randint(0, 24))
            
            # Work order types and probabilities
            wo_type = np.random.choice(
                ['Preventive', 'Corrective', 'Predictive', 'Breakdown', 'Modification'],
                p=[0.4, 0.3, 0.1, 0.15, 0.05]
            )
            
            # Equipment types
            equipment_type = np.random.choice([
                'Pump', 'Compressor', 'Valve', 'Exchanger', 'Vessel',
                'Instrument', 'Electrical', 'Structural'
            ])
            
            # Priority based on type
            if wo_type == 'Breakdown':
                priority = 'High'
                status = 'Completed'
                duration = np.random.uniform(4, 48)
            elif wo_type == 'Corrective':
                priority = 'Medium'
                status = np.random.choice(['Open', 'In Progress', 'Completed'], p=[0.2, 0.3, 0.5])
                duration = np.random.uniform(8, 72)
            else:
                priority = 'Low'
                status = 'Completed'
                duration = np.random.uniform(2, 24)
            
            # Cost estimation
            if wo_type == 'Breakdown':
                cost = np.random.uniform(5000, 50000)
            elif wo_type == 'Modification':
                cost = np.random.uniform(10000, 100000)
            else:
                cost = np.random.uniform(1000, 20000)
            
            # Failure codes (if applicable)
            failure_codes = []
            if wo_type in ['Corrective', 'Breakdown']:
                failure_modes = ['Mechanical', 'Electrical', 'Instrumentation', 'Corrosion', 'Wear']
                failure_codes.append({
                    'mode': np.random.choice(failure_modes),
                    'cause': np.random.choice(['Fatigue', 'Abrasion', 'Overload', 'Contamination']),
                    'component': np.random.choice(['Bearing', 'Seal', 'Coupling', 'Rotor', 'Stator'])
                })
            
            work_order = {
                'wo_id': f"WO{wo_idx+1:05d}",
                'date_created': wo_date.strftime('%Y-%m-%d %H:%M'),
                'equipment': f"{equipment_type}-{np.random.randint(1, 20):02d}",
                'description': f"{wo_type} Maintenance on {equipment_type}",
                'type': wo_type,
                'priority': priority,
                'status': status,
                'estimated_hours': float(duration),
                'actual_hours': float(duration * np.random.uniform(0.8, 1.2)),
                'estimated_cost': float(cost),
                'actual_cost': float(cost * np.random.uniform(0.9, 1.3)),
                'failure_analysis': failure_codes,
                'materials_used': [
                    {
                        'part': np.random.choice(['Bearing', 'Seal Kit', 'Gasket', 'Fasteners']),
                        'quantity': np.random.randint(1, 5),
                        'cost': np.random.uniform(100, 1000)
                    }
                ] if wo_type != 'Preventive' else [],
                'crafts': {
                    'mechanical': np.random.randint(1, 4),
                    'electrical': np.random.randint(0, 2),
                    'instrumentation': np.random.randint(0, 2)
                },
                'downtime_hours': float(duration) if wo_type in ['Breakdown', 'Corrective'] else 0,
                'safety_incidents': 0 if np.random.random() > 0.98 else 1
            }
            
            work_orders.append(work_order)
        
        # Maintenance metrics summary
        metrics = {
            'total_work_orders': len(work_orders),
            'breakdown_by_type': {
                'Preventive': len([wo for wo in work_orders if wo['type'] == 'Preventive']),
                'Corrective': len([wo for wo in work_orders if wo['type'] == 'Corrective']),
                'Predictive': len([wo for wo in work_orders if wo['type'] == 'Predictive']),
                'Breakdown': len([wo for wo in work_orders if wo['type'] == 'Breakdown']),
                'Modification': len([wo for wo in work_orders if wo['type'] == 'Modification'])
            },
            'backlog': {
                'current': len([wo for wo in work_orders if wo['status'] != 'Completed']),
                'aging': {
                    '7_days': np.random.randint(5, 15),
                    '30_days': np.random.randint(2, 8),
                    '90_days': np.random.randint(0, 3)
                }
            },
            'performance': {
                'schedule_compliance': np.random.uniform(85, 95),
                'pm_completion': np.random.uniform(90, 98),
                'breakdown_percentage': np.random.uniform(10, 20),
                'mean_time_to_repair_hours': np.random.uniform(12, 36),
                'mean_time_between_failures_hours': np.random.uniform(500, 1000)
            },
            'costs': {
                'total_maintenance_cost': sum(wo['actual_cost'] for wo in work_orders),
                'cost_per_barrel': np.random.uniform(2.5, 3.5),
                'breakdown_cost_percentage': np.random.uniform(25, 35)
            }
        }
        
        return {
            'work_orders': work_orders,
            'metrics': metrics
        }
    
    def _generate_inventory_data(self):
        """Generate spare parts inventory data"""
        # Critical spare parts
        critical_spares = [
            {
                'part_number': 'SP-001',
                'description': 'Compressor Rotor Assembly',
                'equipment': 'COMP-01',
                'criticality': 'A',
                'min_stock': 1,
                'max_stock': 2,
                'current_stock': 1,
                'reorder_point': 1,
                'lead_time_days': 180,
                'unit_cost': 250000,
                'total_value': 250000,
                'last_used': (self.start_date - timedelta(days=365)).strftime('%Y-%m-%d'),
                'shelf_life_years': 10
            },
            {
                'part_number': 'SP-002',
                'description': 'Main Pump Casing',
                'equipment': 'PUMP-01',
                'criticality': 'A',
                'min_stock': 1,
                'max_stock': 2,
                'current_stock': 2,
                'reorder_point': 1,
                'lead_time_days': 90,
                'unit_cost': 75000,
                'total_value': 150000,
                'last_used': (self.start_date - timedelta(days=180)).strftime('%Y-%m-%d'),
                'shelf_life_years': 15
            },
            {
                'part_number': 'SP-003',
                'description': 'Control Valve Actuator',
                'equipment': 'CV-101',
                'criticality': 'B',
                'min_stock': 2,
                'max_stock': 4,
                'current_stock': 3,
                'reorder_point': 2,
                'lead_time_days': 60,
                'unit_cost': 15000,
                'total_value': 45000,
                'last_used': (self.start_date - timedelta(days=90)).strftime('%Y-%m-%d'),
                'shelf_life_years': 8
            }
        ]
        
        # Consumables inventory
        consumables = [
            {
                'category': 'Mechanical Seals',
                'part_number': 'CS-001',
                'description': 'Double Mechanical Seal',
                'usage_rate_per_month': 2,
                'current_stock': 12,
                'reorder_point': 4,
                'unit_cost': 2500,
                'total_value': 30000
            },
            {
                'category': 'Bearings',
                'part_number': 'CS-002',
                'description': 'Ball Bearing 6312',
                'usage_rate_per_month': 4,
                'current_stock': 24,
                'reorder_point': 8,
                'unit_cost': 800,
                'total_value': 19200
            },
            {
                'category': 'Gaskets',
                'part_number': 'CS-003',
                'description': 'Spiral Wound Gasket 6" 300#',
                'usage_rate_per_month': 10,
                'current_stock': 60,
                'reorder_point': 20,
                'unit_cost': 150,
                'total_value': 9000
            }
        ]
        
        # Inventory transactions for 6 months
        transactions = []
        transaction_id = 1000
        
        for month in range(6):
            month_date = self.start_date + timedelta(days=month*30)
            
            # Receipts
            receipt_count = np.random.randint(3, 8)
            for _ in range(receipt_count):
                transaction_id += 1
                part = np.random.choice(critical_spares + consumables)
                
                transaction = {
                    'transaction_id': f"TRX-{transaction_id}",
                    'date': (month_date + timedelta(days=np.random.randint(0, 30))).strftime('%Y-%m-%d'),
                    'type': 'Receipt',
                    'part_number': part['part_number'],
                    'description': part['description'],
                    'quantity': np.random.randint(1, 5),
                    'unit_cost': part['unit_cost'],
                    'supplier': np.random.choice(['Supplier A', 'Supplier B', 'Supplier C']),
                    'purchase_order': f"PO-{np.random.randint(1000, 9999)}"
                }
                transactions.append(transaction)
            
            # Issues
            issue_count = np.random.randint(5, 12)
            for _ in range(issue_count):
                transaction_id += 1
                part = np.random.choice(critical_spares + consumables)
                
                transaction = {
                    'transaction_id': f"TRX-{transaction_id}",
                    'date': (month_date + timedelta(days=np.random.randint(0, 30))).strftime('%Y-%m-%d'),
                    'type': 'Issue',
                    'part_number': part['part_number'],
                    'description': part['description'],
                    'quantity': np.random.randint(1, 3),
                    'work_order': f"WO{np.random.randint(10000, 99999)}",
                    'equipment': part.get('equipment', 'Various'),
                    'technician': np.random.choice(['Tech A', 'Tech B', 'Tech C'])
                }
                transactions.append(transaction)
        
        # Inventory metrics
        metrics = {
            'total_inventory_value': sum(p['total_value'] for p in critical_spares + consumables),
            'critical_spares_value': sum(p['total_value'] for p in critical_spares),
            'consumables_value': sum(p['total_value'] for p in consumables),
            'inventory_turnover': np.random.uniform(1.5, 2.5),
            'stockout_incidents': np.random.randint(0, 3),
            'obsolescence_risk': np.random.uniform(5, 15),
            'service_level': np.random.uniform(95, 99)
        }
        
        return {
            'critical_spares': critical_spares,
            'consumables': consumables,
            'transactions': transactions,
            'metrics': metrics
        }
    
    def _generate_revamp_data(self):
        """Generate revamp and upgrade project data"""
        revamp_projects = [
            {
                'project_id': 'REV-2024-001',
                'name': 'Compressor Efficiency Upgrade',
                'description': 'Upgrade compressor controls and sealing system for improved efficiency',
                'justification': 'Increasing energy costs and aging equipment',
                'scope': {
                    'equipment': ['COMP-01', 'COMP-02'],
                    'work': [
                        'Install variable speed drives',
                        'Upgrade control system',
                        'Replace dry gas seals',
                        'Install advanced vibration monitoring'
                    ]
                },
                'objectives': {
                    'energy_savings_percent': 15,
                    'availability_improvement_percent': 5,
                    'emissions_reduction_percent': 10,
                    'payback_years': 3.5
                },
                'schedule': {
                    'start_date': (self.start_date + timedelta(days=60)).strftime('%Y-%m-%d'),
                    'end_date': (self.start_date + timedelta(days=120)).strftime('%Y-%m-%d'),
                    'duration_days': 60,
                    'critical_path': ['Engineering', 'Procurement', 'Installation', 'Commissioning']
                },
                'budget': {
                    'total_musd': 2.5,
                    'breakdown': {
                        'engineering': 0.3,
                        'materials': 1.2,
                        'labor': 0.8,
                        'contingency': 0.2
                    },
                    'funding_source': 'Capital Budget'
                },
                'status': 'Planning',
                'risks': [
                    {
                        'description': 'Extended downtime during installation',
                        'probability': 'Medium',
                        'impact': 'High',
                        'mitigation': 'Phased installation during planned turnaround'
                    }
                ],
                'stakeholders': {
                    'sponsor': 'Operations Manager',
                    'project_manager': 'Senior Project Engineer',
                    'operations_contact': 'Area Supervisor',
                    'maintenance_contact': 'Maintenance Superintendent'
                }
            },
            {
                'project_id': 'REV-2024-002',
                'name': 'Water Treatment Plant Upgrade',
                'description': 'Increase produced water treatment capacity and improve discharge quality',
                'justification': 'Increasing water production and tightening environmental regulations',
                'scope': {
                    'equipment': ['WTP-01', 'WTP-02'],
                    'work': [
                        'Install additional flotation cells',
                        'Upgrade filtration system',
                        'Install advanced oil-in-water monitoring',
                        'Expand sludge handling capacity'
                    ]
                },
                'objectives': {
                    'capacity_increase_percent': 25,
                    'discharge_quality_improvement_percent': 40,
                    'chemical_usage_reduction_percent': 15,
                    'payback_years': 4.2
                },
                'schedule': {
                    'start_date': (self.start_date + timedelta(days=90)).strftime('%Y-%m-%d'),
                    'end_date': (self.start_date + timedelta(days=150)).strftime('%Y-%m-%d'),
                    'duration_days': 60,
                    'critical_path': ['Civil Work', 'Equipment Installation', 'Commissioning']
                },
                'budget': {
                    'total_musd': 3.8,
                    'breakdown': {
                        'engineering': 0.4,
                        'materials': 2.0,
                        'labor': 1.1,
                        'contingency': 0.3
                    },
                    'funding_source': 'Capital Budget'
                },
                'status': 'Feasibility Study',
                'risks': [
                    {
                        'description': 'Environmental permitting delays',
                        'probability': 'High',
                        'impact': 'Medium',
                        'mitigation': 'Early engagement with regulators'
                    }
                ],
                'stakeholders': {
                    'sponsor': 'Environmental Manager',
                    'project_manager': 'Process Engineer',
                    'operations_contact': 'Water Treatment Supervisor',
                    'regulatory_contact': 'Regulatory Affairs Manager'
                }
            }
        ]
        
        # Revamp project metrics
        portfolio_metrics = {
            'total_projects': len(revamp_projects),
            'total_budget_musd': sum(p['budget']['total_musd'] for p in revamp_projects),
            'average_payback_years': np.mean([p['objectives']['payback_years'] for p in revamp_projects]),
            'expected_annual_savings_musd': sum([
                p['budget']['total_musd'] / p['objectives']['payback_years'] 
                for p in revamp_projects
            ]),
            'resource_allocation': {
                'engineering_hours': 4000,
                'construction_hours': 12000,
                'peak_manpower': 85
            },
            'strategic_alignment': {
                'safety_improvement': 'High',
                'environmental_compliance': 'High',
                'operational_efficiency': 'Medium',
                'cost_reduction': 'Medium'
            }
        }
        
        return {
            'projects': revamp_projects,
            'portfolio_metrics': portfolio_metrics
        }
    
    def _generate_predictive_maintenance(self):
        """Generate predictive maintenance model data and predictions"""
        # Equipment failure predictions
        equipment_predictions = [
            {
                'equipment': 'COMP-01',
                'model_type': 'Vibration Analysis + Oil Analysis',
                'current_health_index': 0.82,
                'trend': 'Deteriorating',
                'predicted_failure_mode': 'Bearing Failure',
                'confidence': 0.85,
                'predicted_failure_date': (self.start_date + timedelta(days=180)).strftime('%Y-%m-%d'),
                'remaining_useful_life_days': 180,
                'recommended_action': 'Schedule bearing replacement during next turnaround',
                'urgency': 'Medium'
            },
            {
                'equipment': 'PUMP-02',
                'model_type': 'Vibration Analysis + Temperature',
                'current_health_index': 0.65,
                'trend': 'Rapid Deterioration',
                'predicted_failure_mode': 'Seal Failure',
                'confidence': 0.92,
                'predicted_failure_date': (self.start_date + timedelta(days=45)).strftime('%Y-%m-%d'),
                'remaining_useful_life_days': 45,
                'recommended_action': 'Immediate seal replacement',
                'urgency': 'High'
            },
            {
                'equipment': 'HX-01',
                'model_type': 'Thermal Performance + Pressure Drop',
                'current_health_index': 0.71,
                'trend': 'Gradual Deterioration',
                'predicted_failure_mode': 'Fouling',
                'confidence': 0.78,
                'predicted_failure_date': (self.start_date + timedelta(days=120)).strftime('%Y-%m-%d'),
                'remaining_useful_life_days': 120,
                'recommended_action': 'Chemical cleaning',
                'urgency': 'Low'
            }
        ]
        
        # Model performance metrics
        model_performance = {
            'overall_accuracy': 0.87,
            'precision': 0.85,
            'recall': 0.89,
            'f1_score': 0.87,
            'false_positive_rate': 0.12,
            'false_negative_rate': 0.08,
            'mean_absolute_error_days': 12.5,
            'r_squared': 0.82
        }
        
        # Historical predictions vs actuals
        prediction_history = []
        
        # Generate historical data for past 12 months
        for month in range(-12, 0):
            prediction_date = self.start_date + timedelta(days=month*30)
            
            for equip_idx, equipment in enumerate(['COMP-01', 'PUMP-01', 'VALVE-05']):
                # Simulate predictions with some error
                predicted_failure_date = prediction_date + timedelta(days=np.random.randint(60, 180))
                actual_failure_date = None
                
                # 70% of predictions lead to preventive action
                if np.random.random() > 0.3:
                    # Preventive action taken
                    action_date = predicted_failure_date - timedelta(days=np.random.randint(7, 30))
                    actual_failure_date = None
                    outcome = 'Prevented'
                else:
                    # Failure occurred
                    actual_failure_date = predicted_failure_date + timedelta(days=np.random.randint(-14, 14))
                    outcome = 'Failed'
                
                prediction_history.append({
                    'date': prediction_date.strftime('%Y-%m-%d'),
                    'equipment': equipment,
                    'predicted_failure_date': predicted_failure_date.strftime('%Y-%m-%d'),
                    'actual_failure_date': actual_failure_date.strftime('%Y-%m-%d') if actual_failure_date else 'N/A',
                    'outcome': outcome,
                    'prediction_error_days': (actual_failure_date - predicted_failure_date).days if actual_failure_date else None
                })
        
        # Cost-benefit analysis
        cost_benefit = {
            'implementation_cost_musd': 0.75,
            'annual_maintenance_cost_musd': 0.15,
            'annual_savings_musd': 1.2,
            'roi_percent': 160,
            'payback_months': 9,
            'avoided_costs': {
                'breakdown_maintenance_musd': 0.8,
                'production_losses_musd': 0.3,
                'safety_incidents_musd': 0.1
            }
        }
        
        return {
            'current_predictions': equipment_predictions,
            'model_performance': model_performance,
            'prediction_history': prediction_history,
            'cost_benefit_analysis': cost_benefit,
            'implementation_status': {
                'phase': 'Optimization',
                'coverage_percent': 65,
                'integrated_with_cmmis': True,
                'user_adoption_rate': 85
            }
        }
    
    # =========================================================================
    # PHASE 5: DECOMMISSIONING & SITE REHABILITATION DATA GENERATION
    # =========================================================================
    
    def generate_decommissioning_data(self):
        """Generate synthetic decommissioning and site rehabilitation data"""
        print("\n" + "="*60)
        print("PHASE 5: Generating Decommissioning & Site Rehabilitation Data")
        print("="*60)
        
        # 5.1 Asset Condition Assessment
        print("Generating asset condition assessment data...")
        asset_condition = self._generate_asset_condition()
        self.decommissioning_data['asset_condition'] = asset_condition
        
        # 5.2 Decommissioning Cost Estimates
        print("Generating decommissioning cost estimates...")
        cost_estimates = self._generate_decom_cost_estimates()
        self.decommissioning_data['cost_estimates'] = cost_estimates
        
        # 5.3 Environmental Baseline
        print("Generating environmental baseline data...")
        environmental_baseline = self._generate_environmental_baseline()
        self.decommissioning_data['environmental_baseline'] = environmental_baseline
        
        # 5.4 Regulatory Requirements
        print("Generating regulatory requirements data...")
        regulatory_data = self._generate_regulatory_data()
        self.decommissioning_data['regulatory'] = regulatory_data
        
        # 5.5 Decommissioning Schedule
        print("Generating decommissioning schedule...")
        decom_schedule = self._generate_decom_schedule()
        self.decommissioning_data['schedule'] = decom_schedule
        
        # 5.6 Site Rehabilitation Plan
        print("Generating site rehabilitation plan...")
        rehab_plan = self._generate_rehab_plan()
        self.decommissioning_data['rehabilitation'] = rehab_plan
        
        print("Decommissioning data generation complete!")
        return self.decommissioning_data
    
    def _generate_asset_condition(self):
        """Generate asset condition assessment data"""
        assets = []
        
        # Wells
        well_count = 25
        for well_idx in range(well_count):
            well_age = np.random.uniform(5, 15)  # years
            condition_score = np.random.uniform(0.6, 0.9)  # 0-1 scale
            
            asset = {
                'asset_id': f"WELL-{well_idx+1:03d}",
                'type': 'Production Well',
                'age_years': well_age,
                'condition_assessment': {
                    'score': condition_score,
                    'rating': 'Fair' if condition_score > 0.7 else 'Poor',
                    'casing_integrity': np.random.choice(['Good', 'Fair', 'Poor']),
                    'wellhead_condition': np.random.choice(['Good', 'Fair']),
                    'cement_bond': np.random.choice(['Good', 'Questionable', 'Poor']),
                    'internal_corrosion': np.random.choice(['Low', 'Medium', 'High'])
                },
                'remaining_life_years': np.random.uniform(5, 10),
                'decommissioning_complexity': np.random.choice(['Low', 'Medium', 'High']),
                'estimated_decom_cost': np.random.uniform(0.5, 2.0)  # million USD
            }
            assets.append(asset)
        
        # Facilities
        facilities = [
            {
                'asset_id': 'PROC-01',
                'type': 'Processing Plant',
                'age_years': 12,
                'condition_assessment': {
                    'score': 0.75,
                    'rating': 'Fair',
                    'structural_integrity': 'Fair',
                    'equipment_condition': 'Poor',
                    'corrosion_status': 'Extensive',
                    'safety_systems': 'Adequate'
                },
                'remaining_life_years': 3,
                'decommissioning_complexity': 'High',
                'estimated_decom_cost': 15.0  # million USD
            },
            {
                'asset_id': 'TANK-FARM',
                'type': 'Storage Tank Farm',
                'age_years': 15,
                'condition_assessment': {
                    'score': 0.65,
                    'rating': 'Poor',
                    'tank_integrity': 'Poor',
                    'foundation_condition': 'Fair',
                    'lining_condition': 'Failed',
                    'leak_history': 'Multiple incidents'
                },
                'remaining_life_years': 2,
                'decommissioning_complexity': 'Medium',
                'estimated_decom_cost': 8.0  # million USD
            }
        ]
        assets.extend(facilities)
        
        # Pipelines
        pipeline_count = 5
        for pipe_idx in range(pipeline_count):
            pipeline_length = np.random.uniform(5, 20)  # km
            pipeline_age = np.random.uniform(10, 20)
            
            asset = {
                'asset_id': f"PIPE-{pipe_idx+1:02d}",
                'type': 'Pipeline',
                'age_years': pipeline_age,
                'length_km': pipeline_length,
                'condition_assessment': {
                    'score': np.random.uniform(0.5, 0.8),
                    'rating': np.random.choice(['Fair', 'Poor']),
                    'corrosion_rate_mm_yr': np.random.uniform(0.05, 0.15),
                    'coating_condition': np.random.choice(['Poor', 'Failed']),
                    'cathodic_protection': np.random.choice(['Adequate', 'Poor']),
                    'integrity_testing': 'Last test 3 years ago'
                },
                'remaining_life_years': np.random.uniform(2, 5),
                'decommissioning_complexity': 'Medium',
                'estimated_decom_cost': pipeline_length * np.random.uniform(0.1, 0.3)  # million USD/km
            }
            assets.append(asset)
        
        # Summary metrics
        summary = {
            'total_assets': len(assets),
            'average_condition_score': np.mean([a['condition_assessment']['score'] for a in assets]),
            'assets_requiring_immediate_attention': len([a for a in assets if a['condition_assessment']['score'] < 0.6]),
            'total_estimated_decom_cost': sum(a['estimated_decom_cost'] for a in assets),
            'risk_assessment': {
                'environmental_risk': 'High',
                'safety_risk': 'Medium',
                'reputation_risk': 'Medium',
                'financial_risk': 'High'
            }
        }
        
        return {
            'assets': assets,
            'summary': summary
        }
    
    def _generate_decom_cost_estimates(self):
        """Generate decommissioning cost estimates"""
        # Cost breakdown by activity
        cost_breakdown = {
            'engineering_planning': {
                'description': 'Engineering studies, planning, permitting',
                'percentage': 10,
                'estimated_cost_musd': 2.5,
                'uncertainty': '±20%'
            },
            'well_plugging_abandonment': {
                'description': 'Well P&A operations',
                'percentage': 40,
                'estimated_cost_musd': 10.0,
                'uncertainty': '±30%'
            },
            'facilities_removal': {
                'description': 'Decommissioning of surface facilities',
                'percentage': 25,
                'estimated_cost_musd': 6.25,
                'uncertainty': '±25%'
            },
            'site_remediation': {
                'description': 'Soil and groundwater remediation',
                'percentage': 15,
                'estimated_cost_musd': 3.75,
                'uncertainty': '±40%'
            },
            'waste_disposal': {
                'description': 'Waste handling and disposal',
                'percentage': 5,
                'estimated_cost_musd': 1.25,
                'uncertainty': '±25%'
            },
            'contingency': {
                'description': 'Contingency reserve',
                'percentage': 5,
                'estimated_cost_musd': 1.25,
                'uncertainty': 'N/A'
            }
        }
        
        total_cost = sum(item['estimated_cost_musd'] for item in cost_breakdown.values())
        
        # Cost scenarios
        scenarios = {
            'base_case': {
                'total_cost_musd': total_cost,
                'duration_months': 24,
                'assumptions': [
                    'Normal weather conditions',
                    'No major regulatory changes',
                    'Standard contractor rates',
                    'No major surprises during execution'
                ]
            },
            'optimistic_scenario': {
                'total_cost_musd': total_cost * 0.8,
                'duration_months': 18,
                'assumptions': [
                    'Favorable weather',
                    'Efficient contractor performance',
                    'No regulatory delays',
                    'Good equipment condition'
                ]
            },
            'pessimistic_scenario': {
                'total_cost_musd': total_cost * 1.5,
                'duration_months': 36,
                'assumptions': [
                    'Adverse weather conditions',
                    'Regulatory delays',
                    'Worse-than-expected asset condition',
                    'Contractor performance issues'
                ]
            }
        }
        
        # Funding strategy
        funding = {
            'total_required_musd': total_cost,
            'current_provision_musd': total_cost * 0.6,
            'funding_gap_musd': total_cost * 0.4,
            'funding_sources': {
                'operating_cash_flow': total_cost * 0.4,
                'decommissioning_reserve': total_cost * 0.2,
                'new_financing': total_cost * 0.4
            },
            'timing': {
                'year_1': total_cost * 0.3,
                'year_2': total_cost * 0.5,
                'year_3': total_cost * 0.2
            }
        }
        
        # Cost drivers and sensitivities
        sensitivities = {
            'oil_price': {
                'impact': 'Indirect - affects available funding',
                'sensitivity': 'Medium'
            },
            'regulatory_changes': {
                'impact': 'Could increase requirements and costs',
                'sensitivity': 'High'
            },
            'contractor_availability': {
                'impact': 'Affects rates and schedule',
                'sensitivity': 'Medium'
            },
            'waste_disposal_costs': {
                'impact': 'Direct cost impact',
                'sensitivity': 'High'
            }
        }
        
        return {
            'cost_breakdown': cost_breakdown,
            'scenarios': scenarios,
            'funding_strategy': funding,
            'sensitivities': sensitivities,
            'total_estimated_cost_musd': total_cost
        }
    
    def _generate_environmental_baseline(self):
        """Generate environmental baseline data"""
        # Soil contamination
        soil_samples = []
        sample_locations = ['Tank Farm', 'Process Area', 'Well Pad A', 'Well Pad B', 'Administrative Area']
        
        for location in sample_locations:
            for depth in ['Surface (0-0.5m)', 'Subsurface (0.5-2m)', 'Deep (2-5m)']:
                sample = {
                    'location': location,
                    'sample_id': f"SOIL-{len(soil_samples)+1:03d}",
                    'depth': depth,
                    'date': (self.start_date - timedelta(days=np.random.randint(30, 180))).strftime('%Y-%m-%d'),
                    'parameters': {
                        'tph_mg_kg': np.random.uniform(100, 5000),
                        'benzene_mg_kg': np.random.uniform(0.1, 10),
                        'toluene_mg_kg': np.random.uniform(0.5, 20),
                        'ethylbenzene_mg_kg': np.random.uniform(0.2, 15),
                        'xylene_mg_kg': np.random.uniform(0.5, 25),
                        'lead_mg_kg': np.random.uniform(10, 100),
                        'arsenic_mg_kg': np.random.uniform(1, 20),
                        'ph': np.random.uniform(6.0, 8.5)
                    },
                    'regulatory_status': 'Exceeds limits' if np.random.random() > 0.6 else 'Within limits'
                }
                soil_samples.append(sample)
        
        # Groundwater quality
        groundwater_wells = []
        well_locations = ['Upgradient', 'Downgradient NE', 'Downgradient SW', 'Source Area']
        
        for location in well_locations:
            for sampling in range(3):  # Three sampling events
                sample_date = self.start_date - timedelta(days=np.random.randint(90, 360))
                
                well_data = {
                    'well_id': f"MW-{len(groundwater_wells)+1:02d}",
                    'location': location,
                    'date': sample_date.strftime('%Y-%m-%d'),
                    'water_level_m': np.random.uniform(5, 20),
                    'parameters': {
                        'tph_ug_l': np.random.uniform(10, 1000),
                        'benzene_ug_l': np.random.uniform(0.5, 50),
                        'mtbe_ug_l': np.random.uniform(0.1, 10),
                        'chloride_mg_l': np.random.uniform(100, 1000),
                        'sodium_mg_l': np.random.uniform(50, 500),
                        'conductivity_us_cm': np.random.uniform(1000, 5000)
                    },
                    'regulatory_status': 'Exceeds MCL' if np.random.random() > 0.7 else 'Within MCL'
                }
                groundwater_wells.append(well_data)
        
        # Ecological assessment
        ecology = {
            'vegetation': {
                'dominant_species': ['Mesquite', 'Buffalo Grass', 'Prickly Pear'],
                'coverage_percent': np.random.uniform(60, 85),
                'health': 'Good' if np.random.random() > 0.3 else 'Moderate',
                'invasive_species': ['Tumbleweed', 'Russian Thistle'] if np.random.random() > 0.5 else []
            },
            'wildlife': {
                'observed_species': ['White-tailed Deer', 'Coyote', 'Jackrabbit', 'Various Birds'],
                'habitat_quality': 'Fair',
                'sensitive_species': 'None observed'
            },
            'wetlands': {
                'presence': 'No wetlands on site',
                'nearest_wetland_distance_km': 2.5,
                'protection_status': 'Not applicable'
            }
        }
        
        # Summary of findings
        summary = {
            'contaminated_areas': {
                'soil_acres': np.random.uniform(5, 15),
                'groundwater_plume_acres': np.random.uniform(2, 8),
                'hot_spots': ['Tank Farm', 'Historical Spill Area 3']
            },
            'regulatory_classification': {
                'soil': 'Industrial' if np.random.random() > 0.5 else 'Commercial',
                'groundwater': 'Class III - Industrial Use',
                'risk_level': 'Medium to High'
            },
            'remediation_requirements': {
                'soil_excavation_yd3': np.random.randint(5000, 15000),
                'groundwater_treatment_years': np.random.uniform(5, 10),
                'monitoring_years': 30
            }
        }
        
        return {
            'soil_contamination': soil_samples,
            'groundwater_quality': groundwater_wells,
            'ecological_assessment': ecology,
            'summary': summary
        }
    
    def _generate_regulatory_data(self):
        """Generate regulatory requirements data"""
        regulations = {
            'federal': [
                {
                    'agency': 'EPA',
                    'regulation': 'Resource Conservation and Recovery Act (RCRA)',
                    'requirements': [
                        'Proper classification and disposal of hazardous waste',
                        'Waste minimization requirements',
                        'Contingency planning'
                    ],
                    'compliance_status': 'In Compliance',
                    'deadlines': [
                        {
                            'requirement': 'Waste characterization',
                            'deadline': (self.start_date + timedelta(days=90)).strftime('%Y-%m-%d')
                        }
                    ]
                },
                {
                    'agency': 'EPA',
                    'regulation': 'Clean Water Act (CWA)',
                    'requirements': [
                        'Stormwater pollution prevention plan',
                        'SPCC plan implementation',
                        'NPDES compliance for discharges'
                    ],
                    'compliance_status': 'In Compliance',
                    'deadlines': [
                        {
                            'requirement': 'Annual report submission',
                            'deadline': (self.start_date + timedelta(days=180)).strftime('%Y-%m-%d')
                        }
                    ]
                }
            ],
            'state': [
                {
                    'agency': 'Texas Railroad Commission (RRC)',
                    'regulation': 'Statewide Rule 8',
                    'requirements': [
                        'Well plugging specifications',
                        'Surface equipment removal',
                        'Site restoration standards'
                    ],
                    'compliance_status': 'Partial Compliance',
                    'deadlines': [
                        {
                            'requirement': 'Plugging schedule submission',
                            'deadline': (self.start_date + timedelta(days=60)).strftime('%Y-%m-%d')
                        }
                    ]
                },
                {
                    'agency': 'Texas Commission on Environmental Quality (TCEQ)',
                    'regulation': 'Texas Risk Reduction Program (TRRP)',
                    'requirements': [
                        'Site assessment requirements',
                        'Remediation standards',
                        'Risk-based corrective action'
                    ],
                    'compliance_status': 'Not Started',
                    'deadlines': [
                        {
                            'requirement': 'Preliminary assessment report',
                            'deadline': (self.start_date + timedelta(days=120)).strftime('%Y-%m-%d')
                        }
                    ]
                }
            ],
            'local': [
                {
                    'agency': 'County Environmental Department',
                    'regulation': 'County Decommissioning Ordinance',
                    'requirements': [
                        'County permit for major demolition',
                        'Dust control measures',
                        'Traffic management plan'
                    ],
                    'compliance_status': 'Not Started',
                    'deadlines': []
                }
            ]
        }
        
        # Permits required
        permits = [
            {
                'permit_id': 'P-001',
                'type': 'Well Plugging Permit',
                'agency': 'Texas RRC',
                'status': 'Application in Progress',
                'submission_date': (self.start_date - timedelta(days=30)).strftime('%Y-%m-%d'),
                'expected_approval': (self.start_date + timedelta(days=60)).strftime('%Y-%m-%d'),
                'validity_period': '18 months from approval'
            },
            {
                'permit_id': 'P-002',
                'type': 'Waste Management Permit',
                'agency': 'TCEQ',
                'status': 'To Be Applied',
                'submission_date': 'N/A',
                'expected_approval': 'N/A',
                'validity_period': 'Project duration'
            },
            {
                'permit_id': 'P-003',
                'type': 'Demolition Permit',
                'agency': 'County',
                'status': 'Not Started',
                'submission_date': 'N/A',
                'expected_approval': 'N/A',
                'validity_period': '6 months'
            }
        ]
        
        # Stakeholder engagement requirements
        stakeholder_engagement = {
            'regulatory_agencies': [
                {
                    'agency': 'Texas RRC',
                    'contact': 'John Smith, District Director',
                    'meeting_frequency': 'Quarterly',
                    'last_meeting': (self.start_date - timedelta(days=45)).strftime('%Y-%m-%d'),
                    'next_meeting': (self.start_date + timedelta(days=45)).strftime('%Y-%m-%d'),
                    'key_issues': ['Plugging schedule', 'Financial assurance']
                }
            ],
            'local_community': {
                'public_meetings_required': 2,
                'notification_radius_miles': 1,
                'concerns_raised': ['Traffic during demolition', 'Dust control', 'Future land use'],
                'mitigation_measures': [
                    'Dust suppression system',
                    'Limited work hours',
                    'Community liaison officer'
                ]
            },
            'land_owners': {
                'number_of_parcels': 3,
                'agreements_required': ['Access agreements', 'Restoration agreements'],
                'status': 'Negotiations ongoing'
            }
        }
        
        return {
            'regulations': regulations,
            'permits': permits,
            'stakeholder_engagement': stakeholder_engagement,
            'overall_compliance_status': 'Partially Compliant',
            'major_issues': [
                'Financial assurance requirements not fully met',
                'Some remediation requirements not yet addressed',
                'Stakeholder concerns about traffic impacts'
            ]
        }
    
    def _generate_decom_schedule(self):
        """Generate decommissioning schedule"""
        # Master schedule
        master_schedule = {
            'phase_1_preparation': {
                'duration_months': 6,
                'activities': [
                    {
                        'activity': 'Finalize engineering plans',
                        'duration_weeks': 8,
                        'start_date': self.start_date.strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=8)).strftime('%Y-%m-%d'),
                        'dependencies': []
                    },
                    {
                        'activity': 'Obtain regulatory approvals',
                        'duration_weeks': 12,
                        'start_date': (self.start_date + timedelta(weeks=4)).strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=16)).strftime('%Y-%m-%d'),
                        'dependencies': ['Finalize engineering plans']
                    },
                    {
                        'activity': 'Contractor selection',
                        'duration_weeks': 10,
                        'start_date': (self.start_date + timedelta(weeks=8)).strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=18)).strftime('%Y-%m-%d'),
                        'dependencies': ['Finalize engineering plans']
                    }
                ]
            },
            'phase_2_execution': {
                'duration_months': 18,
                'activities': [
                    {
                        'activity': 'Well plugging and abandonment',
                        'duration_weeks': 52,
                        'start_date': (self.start_date + timedelta(weeks=20)).strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=72)).strftime('%Y-%m-%d'),
                        'dependencies': ['Obtain regulatory approvals', 'Contractor selection']
                    },
                    {
                        'activity': 'Facilities decommissioning',
                        'duration_weeks': 40,
                        'start_date': (self.start_date + timedelta(weeks=30)).strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=70)).strftime('%Y-%m-%d'),
                        'dependencies': ['Contractor selection']
                    },
                    {
                        'activity': 'Pipeline decommissioning',
                        'duration_weeks': 32,
                        'start_date': (self.start_date + timedelta(weeks=40)).strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=72)).strftime('%Y-%m-%d'),
                        'dependencies': ['Facilities decommissioning']
                    }
                ]
            },
            'phase_3_site_restoration': {
                'duration_months': 12,
                'activities': [
                    {
                        'activity': 'Soil remediation',
                        'duration_weeks': 40,
                        'start_date': (self.start_date + timedelta(weeks=60)).strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=100)).strftime('%Y-%m-%d'),
                        'dependencies': ['Well plugging and abandonment']
                    },
                    {
                        'activity': 'Site grading and restoration',
                        'duration_weeks': 16,
                        'start_date': (self.start_date + timedelta(weeks=90)).strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=106)).strftime('%Y-%m-%d'),
                        'dependencies': ['Soil remediation']
                    },
                    {
                        'activity': 'Final inspection and closeout',
                        'duration_weeks': 8,
                        'start_date': (self.start_date + timedelta(weeks=104)).strftime('%Y-%m-%d'),
                        'end_date': (self.start_date + timedelta(weeks=112)).strftime('%Y-%m-%d'),
                        'dependencies': ['Site grading and restoration']
                    }
                ]
            }
        }
        
        # Resource requirements
        resource_requirements = {
            'manpower': {
                'peak_requirement': 125,
                'average_requirement': 85,
                'skill_mix': {
                    'engineers': 15,
                    'supervisors': 10,
                    'technicians': 60,
                    'laborers': 40
                }
            },
            'equipment': {
                'drilling_rigs': 2,
                'excavators': 4,
                'cranes': 3,
                'dump_trucks': 12,
                'water_trucks': 4,
                'specialized_equipment': [
                    'Well service rigs',
                    'Pipeline cleaning pigs',
                    'Soil treatment equipment'
                ]
            },
            'materials': {
                'cement_tons': 1500,
                'bentonite_tons': 500,
                'steel_grating_tons': 200,
                'disposal_volume_yd3': 25000
            }
        }
        
        # Critical path analysis
        critical_path = {
            'path': [
                'Finalize engineering plans',
                'Obtain regulatory approvals',
                'Contractor selection',
                'Well plugging and abandonment',
                'Soil remediation',
                'Site grading and restoration',
                'Final inspection and closeout'
            ],
            'total_duration_weeks': 112,
            'critical_activities': [
                {
                    'activity': 'Obtain regulatory approvals',
                    'float_weeks': 0,
                    'risk': 'High'
                },
                {
                    'activity': 'Well plugging and abandonment',
                    'float_weeks': 2,
                    'risk': 'Medium'
                },
                {
                    'activity': 'Soil remediation',
                    'float_weeks': 0,
                    'risk': 'High'
                }
            ]
        }
        
        # Risk mitigation schedule
        risk_mitigation = {
            'weather_risks': {
                'impact': 'Delays in outdoor activities',
                'mitigation': 'Schedule critical outdoor work in favorable seasons',
                'contingency_weeks': 8
            },
            'regulatory_risks': {
                'impact': 'Permitting delays',
                'mitigation': 'Early and frequent engagement with agencies',
                'contingency_weeks': 12
            },
            'contractor_risks': {
                'impact': 'Performance issues',
                'mitigation': 'Performance bonds, backup contractors',
                'contingency_weeks': 6
            }
        }
        
        return {
            'master_schedule': master_schedule,
            'resource_requirements': resource_requirements,
            'critical_path_analysis': critical_path,
            'risk_mitigation': risk_mitigation,
            'total_project_duration_months': 36,
            'estimated_completion_date': (self.start_date + timedelta(weeks=112)).strftime('%Y-%m-%d')
        }
    
    def _generate_rehab_plan(self):
        """Generate site rehabilitation plan"""
        rehab_plan = {
            'land_restoration': {
                'objectives': [
                    'Remove all above-ground structures',
                    'Excavate contaminated soil',
                    'Grade site to original contours',
                    'Establish vegetative cover'
                ],
                'standards': {
                    'cleanup_standards': 'TRRP Tier 1 PCLs',
                    'grading_tolerance': '± 0.3 meters',
                    'vegetation_cover': '70% native species coverage after 2 years',
                    'erosion_control': 'Meet County erosion control requirements'
                },
                'methods': {
                    'soil_remediation': 'Excavation and off-site disposal',
                    'groundwater_remediation': 'Pump and treat for 5 years',
                    'revegetation': 'Native seed mix with irrigation for establishment'
                }
            },
            'monitoring_program': {
                'post_remediation_monitoring': {
                    'duration_years': 30,
                    'frequency': {
                        'years_1_5': 'Quarterly',
                        'years_6_10': 'Semi-annually',
                        'years_11_30': 'Annually'
                    },
                    'parameters': [
                        'Groundwater quality',
                        'Soil gas',
                        'Surface water runoff',
                        'Vegetation health'
                    ]
                },
                'performance_metrics': {
                    'groundwater_cleanup_goals': 'Meet MCLs for all contaminants',
                    'soil_cleanup_goals': 'Meet TRRP PCLs',
                    'vegetation_success': '70% coverage within 2 years'
                }
            },
            'future_land_use': {
                'planned_use': 'Open space / Wildlife habitat',
                'restrictions': [
                    'No residential development',
                    'No groundwater extraction',
                    'Limited excavation below 1 meter'
                ],
                'institutional_controls': {
                    'engineering_controls': 'Clay cap over remediated areas',
                    'administrative_controls': 'Deed restrictions',
                    'financial_controls': 'Long-term monitoring fund'
                }
            },
            'stakeholder_commitments': {
                'community': [
                    'Public access after remediation',
                    'Educational signage about site history',
                    'Annual community meetings during monitoring period'
                ],
                'regulators': [
                    'Regular reporting',
                    'Open access for inspections',
                    'Timely response to findings'
                ],
                'land_owners': [
                    'Restoration to agreed condition',
                    'Financial compensation for access',
                    'Long-term maintenance agreement'
                ]
            },
            'closure_criteria': {
                'technical_criteria': [
                    'All contaminants below cleanup levels',
                    'All structures removed',
                    'Site grading complete',
                    'Vegetation established'
                ],
                'regulatory_criteria': [
                    'Final regulatory approval',
                    'No further action letter',
                    'Deed restrictions recorded',
                    'Financial assurance released'
                ],
                'stakeholder_criteria': [
                    'Community acceptance',
                    'Land owner agreement',
                    'Public notice completed'
                ]
            }
        }
        
        # Cost estimates for rehabilitation
        rehab_costs = {
            'soil_remediation': {
                'excavation_disposal': 3.2,
                'backfill_grading': 1.5,
                'capping': 0.8,
                'subtotal': 5.5
            },
            'groundwater_remediation': {
                'treatment_system': 0.5,
                'operation_5_years': 1.0,
                'monitoring': 0.3,
                'subtotal': 1.8
            },
            'site_restoration': {
                'revegetation': 0.4,
                'erosion_control': 0.2,
                'final_grading': 0.6,
                'subtotal': 1.2
            },
            'long_term_monitoring': {
                'years_1_10': 0.8,
                'years_11_30': 0.6,
                'reporting': 0.2,
                'subtotal': 1.6
            },
            'contingency': {
                'percentage': 20,
                'amount': 2.02
            },
            'total_rehabilitation_cost': 12.12
        }
        
        return {
            'rehabilitation_plan': rehab_plan,
            'cost_estimates': rehab_costs,
            'implementation_schedule': 'Integrated with decommissioning schedule',
            'success_metrics': {
                'environmental': 'Meet all regulatory cleanup standards',
                'social': 'Community acceptance and safe future use',
                'economic': 'Cost within 10% of budget',
                'schedule': 'Complete within planned timeframe'
            }
        }
    
    # =========================================================================
    # COMMON DATA GENERATION
    # =========================================================================
    
    def generate_common_data(self):
        """Generate common data shared across all phases"""
        print("\n" + "="*60)
        print("Generating Common Data")
        print("="*60)
        
        # Weather data
        print("Generating weather data...")
        weather_data = self._generate_weather_data()
        self.common_data['weather'] = weather_data
        
        # Market data
        print("Generating market data...")
        market_data = self._generate_market_data()
        self.common_data['market'] = market_data
        
        # Personnel data
        print("Generating personnel data...")
        personnel_data = self._generate_personnel_data()
        self.common_data['personnel'] = personnel_data
        
        # Document data
        print("Generating document data...")
        document_data = self._generate_document_data()
        self.common_data['documents'] = document_data
        
        print("Common data generation complete!")
        return self.common_data
    
    def _generate_weather_data(self):
        """Generate detailed weather data for Austin, TX"""
        # Historical weather patterns for Austin (Jan-Jun)
        daily_temps = {
            'jan': {'high': 61, 'low': 41},
            'feb': {'high': 66, 'low': 45},
            'mar': {'high': 73, 'low': 52},
            'apr': {'high': 80, 'low': 59},
            'may': {'high': 87, 'low': 67},
            'jun': {'high': 93, 'low': 73}
        }
        
        # Generate detailed hourly data
        weather_records = []
        
        for day in range(self.simulation_days):
            date = self.start_date + timedelta(days=day)
            month_idx = date.month - 1  # 0-indexed
            
            # Get monthly averages
            months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun']
            month_key = months[min(month_idx, 5)]
            
            base_high = daily_temps[month_key]['high']
            base_low = daily_temps[month_key]['low']
            
            # Daily variation
            daily_high = base_high + np.random.normal(0, 5)
            daily_low = base_low + np.random.normal(0, 3)
            
            # Generate hourly temperatures
            for hour in range(24):
                # Temperature follows sinusoidal pattern
                hour_rad = 2 * np.pi * hour / 24
                temp_variation = (daily_high - daily_low) / 2
                
                # Temperature peaks around 3 PM, lowest around 6 AM
                hour_temp = (daily_high + daily_low) / 2 + temp_variation * np.sin(hour_rad - np.pi/2)
                hour_temp += np.random.normal(0, 1.5)
                
                # Humidity (higher at night, lower during day)
                humidity = 50 + 20 * np.sin(hour_rad - np.pi/2) + np.random.normal(0, 10)
                humidity = max(30, min(95, humidity))
                
                # Wind speed (higher during day)
                wind_speed = 5 + 5 * np.sin(hour_rad) + np.random.exponential(2)
                
                # Precipitation
                rain_chance = 0.15 if month_idx < 3 else 0.25  # Higher in spring
                precipitation = np.random.exponential(0.1) if np.random.random() < rain_chance else 0
                
                record = {
                    'datetime': (date + timedelta(hours=hour)).strftime('%Y-%m-%d %H:%M'),
                    'temperature_f': float(hour_temp),
                    'humidity_percent': float(humidity),
                    'wind_speed_mph': float(wind_speed),
                    'wind_direction_deg': float(np.random.uniform(0, 360)),
                    'precipitation_in': float(precipitation),
                    'pressure_hpa': 1013 + np.random.normal(0, 5),
                    'visibility_miles': 10 - precipitation * 5,
                    'conditions': self._get_weather_conditions(precipitation, humidity)
                }
                weather_records.append(record)
        
        # Severe weather events
        severe_events = [
            {
                'date': (self.start_date + timedelta(days=45)).strftime('%Y-%m-%d'),
                'type': 'Thunderstorm',
                'intensity': 'Moderate',
                'duration_hours': 3,
                'max_wind_mph': 45,
                'precipitation_in': 1.2,
                'impact': 'Minor flooding in low areas'
            },
            {
                'date': (self.start_date + timedelta(days=120)).strftime('%Y-%m-%d'),
                'type': 'Heat Wave',
                'intensity': 'High',
                'duration_hours': 72,
                'max_temp_f': 102,
                'heat_index_f': 108,
                'impact': 'Increased cooling requirements'
            }
        ]
        
        return {
            'hourly_data': weather_records,
            'severe_events': severe_events,
            'statistics': {
                'average_temperature_f': np.mean([r['temperature_f'] for r in weather_records]),
                'total_precipitation_in': sum(r['precipitation_in'] for r in weather_records),
                'max_wind_mph': max(r['wind_speed_mph'] for r in weather_records),
                'days_with_precipitation': len([r for r in weather_records if r['precipitation_in'] > 0]) / 24
            }
        }
    
    def _get_weather_conditions(self, precipitation, humidity):
        """Determine weather conditions based on parameters"""
        if precipitation > 0.1:
            return 'Rain'
        elif precipitation > 0.01:
            return 'Drizzle'
        elif humidity > 90:
            return 'Fog'
        elif humidity > 80:
            return 'Humid'
        else:
            return 'Clear'
    
    def _generate_market_data(self):
        """Generate market and economic data"""
        # Daily oil and gas prices for 6 months
        market_records = []
        
        # Start with base prices
        base_oil_price = 75  # USD/bbl
        base_gas_price = 3.5  # USD/MMBTU
        
        for day in range(self.simulation_days):
            date = self.start_date + timedelta(days=day)
            
            # Add trends and volatility
            trend_factor = 0.001 * day  # Slight upward trend
            volatility = 0.02  # 2% daily volatility
            
            # Oil price with random walk
            if day == 0:
                oil_price = base_oil_price
            else:
                oil_price = market_records[-1]['oil_price_usd_bbl'] * (1 + np.random.normal(0, volatility))
            
            # Gas price (correlated with oil but more volatile)
            gas_price = base_gas_price * (oil_price / base_oil_price) * (1 + np.random.normal(0, volatility * 1.5))
            
            # Add seasonal factors
            if date.month in [12, 1, 2]:  # Winter
                gas_price *= 1.2  # Higher in winter
            
            record = {
                'date': date.strftime('%Y-%m-%d'),
                'oil_price_usd_bbl': float(oil_price),
                'gas_price_usd_mmbtu': float(gas_price),
                'wti_brent_spread': float(np.random.uniform(-2, 3)),
                'refining_margin_usd_bbl': float(np.random.uniform(8, 15)),
                'usd_index': float(100 + np.random.normal(0, 2)),
                'interest_rate_percent': float(5.25 + np.random.normal(0, 0.1)),
                'stock_market': {
                    'energy_sector': float(100 * (1 + 0.0005 * day + np.random.normal(0, 0.01))),
                    'dow_jones': float(35000 * (1 + 0.0003 * day + np.random.normal(0, 0.008)))
                }
            }
            market_records.append(record)
        
        # Market events
        market_events = [
            {
                'date': (self.start_date + timedelta(days=30)).strftime('%Y-%m-%d'),
                'event': 'OPEC+ Production Decision',
                'impact': 'Price increase of 5%',
                'duration': '2 weeks'
            },
            {
                'date': (self.start_date + timedelta(days=90)).strftime('%Y-%m-%d'),
                'event': 'Economic Data Release',
                'impact': 'Increased volatility',
                'duration': '3 days'
            },
            {
                'date': (self.start_date + timedelta(days=150)).strftime('%Y-%m-%d'),
                'event': 'Hurricane Season Disruption',
                'impact': 'Supply concerns, price spike',
                'duration': '1 week'
            }
        ]
        
        # Commodity forecasts
        forecasts = {
            'oil_price_forecast': {
                'q3_2024': 78,
                'q4_2024': 82,
                'q1_2025': 85,
                'q2_2025': 83,
                'confidence_interval': '±15%'
            },
            'gas_price_forecast': {
                'q3_2024': 3.8,
                'q4_2024': 4.2,
                'q1_2025': 3.9,
                'q2_2025': 3.6,
                'confidence_interval': '±20%'
            }
        }
        
        return {
            'daily_market_data': market_records,
            'market_events': market_events,
            'forecasts': forecasts,
            'statistics': {
                'average_oil_price': np.mean([r['oil_price_usd_bbl'] for r in market_records]),
                'oil_price_volatility': np.std([r['oil_price_usd_bbl'] for r in market_records]) / np.mean([r['oil_price_usd_bbl'] for r in market_records]),
                'correlation_oil_gas': np.corrcoef(
                    [r['oil_price_usd_bbl'] for r in market_records],
                    [r['gas_price_usd_mmbtu'] for r in market_records]
                )[0, 1]
            }
        }
    
    def _generate_personnel_data(self):
        """Generate personnel and organizational data"""
        # Organizational structure
        org_structure = {
            'executive': {
                'vp_operations': 'Robert Johnson',
                'director_production': 'Maria Garcia',
                'director_technical': 'David Chen'
            },
            'operations': {
                'operations_manager': 'James Wilson',
                'maintenance_superintendent': 'Sarah Miller',
                'hsse_manager': 'Michael Brown'
            },
            'technical': {
                'reservoir_engineering': 'Dr. Emily Davis',
                'production_engineering': 'Thomas Taylor',
                'facilities_engineering': 'Jennifer Anderson'
            },
            'field_operations': {
                'day_shift_supervisor': 'William Martinez',
                'night_shift_supervisor': 'Patricia Thomas',
                'maintenance_lead': 'Charles Lee'
            }
        }
        
        # Personnel roster
        personnel = []
        roles = [
            'Operator', 'Technician', 'Engineer', 'Supervisor', 'Manager',
            'Specialist', 'Analyst', 'Coordinator'
        ]
        
        departments = ['Operations', 'Maintenance', 'Engineering', 'HSE', 'Administration']
        
        for emp_id in range(1, 101):  # 100 employees
            department = np.random.choice(departments)
            role = np.random.choice(roles)
            
            # Experience based on role
            if role in ['Manager', 'Supervisor', 'Engineer']:
                experience = np.random.randint(5, 25)
            else:
                experience = np.random.randint(1, 15)
            
            employee = {
                'employee_id': f"EMP{emp_id:04d}",
                'name': f"Employee {emp_id}",
                'department': department,
                'role': role,
                'experience_years': experience,
                'certifications': self._generate_certifications(role),
                'shift': np.random.choice(['Day', 'Night', 'Rotating']),
                'location': np.random.choice(['Field', 'Office', 'Both']),
                'status': 'Active'
            }
            personnel.append(employee)
        
        # Training records
        training_records = []
        training_programs = [
            'H2S Awareness', 'Confined Space Entry', 'Hot Work Permitting',
            'Process Safety Management', 'First Aid/CPR', 'Fall Protection',
            'Incident Investigation', 'Root Cause Analysis'
        ]
        
        for record_id in range(200):
            employee = np.random.choice(personnel)
            training = np.random.choice(training_programs)
            
            # Training dates in the past year
            days_ago = np.random.randint(1, 365)
            training_date = self.start_date - timedelta(days=days_ago)
            expiry_date = training_date + timedelta(days=365)
            
            record = {
                'record_id': f"TR{record_id:04d}",
                'employee_id': employee['employee_id'],
                'training_program': training,
                'training_date': training_date.strftime('%Y-%m-%d'),
                'expiry_date': expiry_date.strftime('%Y-%m-%d'),
                'instructor': np.random.choice(['Internal', 'External Vendor']),
                'status': 'Completed' if expiry_date > self.start_date else 'Expired',
                'score_percent': np.random.randint(80, 100)
            }
            training_records.append(record)
        
        # Safety performance
        safety_metrics = {
            'total_recordable_incidents': np.random.randint(0, 3),
            'lost_time_injuries': np.random.randint(0, 1),
            'near_misses': np.random.randint(5, 15),
            'safety_observations': np.random.randint(100, 200),
            'manhours': self.simulation_days * 24 * 85,  # 85 people working
            'trir': np.random.uniform(0.5, 1.5),
            'ltir': np.random.uniform(0.0, 0.5),
            'safety_training_hours': np.random.randint(2000, 5000)
        }
        
        return {
            'organizational_structure': org_structure,
            'personnel_roster': personnel,
            'training_records': training_records,
            'safety_metrics': safety_metrics,
            'workforce_analytics': {
                'average_experience': np.mean([p['experience_years'] for p in personnel]),
                'turnover_rate': np.random.uniform(5, 10),
                'vacancy_rate': np.random.uniform(2, 5),
                'overtime_percentage': np.random.uniform(5, 15)
            }
        }
    
    def _generate_certifications(self, role):
        """Generate relevant certifications for a role"""
        base_certs = ['Basic Safety Training', 'Site Specific Orientation']
        
        if role in ['Operator', 'Technician']:
            additional = ['H2S Alive', 'First Aid', 'Confined Space']
        elif role == 'Engineer':
            additional = ['Professional Engineer', 'Process Safety']
        elif role in ['Supervisor', 'Manager']:
            additional = ['Leadership Training', 'Incident Command']
        else:
            additional = []
        
        return base_certs + additional
    
    def _generate_document_data(self):
        """Generate document and record data"""
        documents = []
        
        # Document types
        doc_types = [
            'Procedure', 'Standard', 'Manual', 'Report', 'Analysis',
            'Assessment', 'Plan', 'Specification', 'Drawing'
        ]
        
        departments = ['Operations', 'Engineering', 'HSE', 'Maintenance', 'Regulatory']
        
        for doc_id in range(1, 201):  # 200 documents
            doc_type = np.random.choice(doc_types)
            department = np.random.choice(departments)
            
            # Document age
            days_old = np.random.randint(1, 365*3)  # Up to 3 years old
            created_date = self.start_date - timedelta(days=days_old)
            
            # Revision if old enough
            if days_old > 365:
                revisions = np.random.randint(1, 4)
                last_revision = created_date + timedelta(days=np.random.randint(180, days_old))
            else:
                revisions = 1
                last_revision = created_date
            
            # Document status
            if days_old > 365*2:
                status = 'For Review'
            elif np.random.random() > 0.9:
                status = 'Superseded'
            else:
                status = 'Active'
            
            document = {
                'document_id': f"DOC{doc_id:05d}",
                'title': f"{doc_type} for {department} - {doc_id}",
                'type': doc_type,
                'department': department,
                'created_date': created_date.strftime('%Y-%m-%d'),
                'last_revision': last_revision.strftime('%Y-%m-%d'),
                'revision': revisions,
                'status': status,
                'owner': f"EMP{np.random.randint(1, 101):04d}",
                'confidentiality': np.random.choice(['Public', 'Internal', 'Confidential', 'Restricted']),
                'keywords': [doc_type.lower(), department.lower(), 'document'],
                'file_size_mb': np.random.uniform(0.1, 50),
                'format': np.random.choice(['PDF', 'Word', 'Excel', 'AutoCAD', 'PDF/A'])
            }
            documents.append(document)
        
        # Key documents by category
        key_documents = {
            'safety': [
                {
                    'id': 'SAF-001',
                    'title': 'Site Safety Manual',
                    'revision': 5,
                    'last_review': (self.start_date - timedelta(days=60)).strftime('%Y-%m-%d'),
                    'next_review': (self.start_date + timedelta(days=305)).strftime('%Y-%m-%d')
                }
            ],
            'operations': [
                {
                    'id': 'OPS-001',
                    'title': 'Operating Procedures Manual',
                    'revision': 3,
                    'last_review': (self.start_date - timedelta(days=90)).strftime('%Y-%m-%d'),
                    'next_review': (self.start_date + timedelta(days=275)).strftime('%Y-%m-%d')
                }
            ],
            'environmental': [
                {
                    'id': 'ENV-001',
                    'title': 'Environmental Management Plan',
                    'revision': 2,
                    'last_review': (self.start_date - timedelta(days=120)).strftime('%Y-%m-%d'),
                    'next_review': (self.start_date + timedelta(days=245)).strftime('%Y-%m-%d')
                }
            ]
        }
        
        # Document management metrics
        metrics = {
            'total_documents': len(documents),
            'documents_by_type': {doc_type: len([d for d in documents if d['type'] == doc_type]) 
                                 for doc_type in doc_types},
            'documents_by_status': {status: len([d for d in documents if d['status'] == status]) 
                                   for status in ['Active', 'For Review', 'Superseded']},
            'average_document_age_days': np.mean([(self.start_date - datetime.strptime(d['created_date'], '%Y-%m-%d')).days 
                                                 for d in documents]),
            'revision_frequency_days': np.random.uniform(180, 365)
        }
        
        return {
            'document_repository': documents,
            'key_documents': key_documents,
            'management_metrics': metrics,
            'retention_policy': {
                'operational_documents': '10 years',
                'financial_documents': '7 years',
                'safety_records': '30 years',
                'environmental_records': 'Permanent'
            }
        }
    
    # =========================================================================
    # MAIN GENERATION AND EXPORT METHODS
    # =========================================================================
    
    def generate_all_data(self):
        """Generate all synthetic data for complete asset lifecycle"""
        print("\n" + "="*60)
        print("COMPLETE ASSET LIFECYCLE DATA GENERATION")
        print("="*60)
        
        # Generate data for all phases
        self.generate_exploration_data()
        self.generate_development_data()
        self.generate_production_data()
        self.generate_maintenance_data()
        self.generate_decommissioning_data()
        self.generate_common_data()
        
        print("\n" + "="*60)
        print("ALL DATA GENERATION COMPLETE!")
        print("="*60)
        
        return self
    
    def export_data(self, output_dir='complete_asset_data'):
        """Export all generated data to structured directory"""
        print(f"\nExporting data to {output_dir}...")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Export each phase separately
        phases = {
            'exploration': self.exploration_data,
            'development': self.development_data,
            'production': self.production_data,
            'maintenance': self.maintenance_data,
            'decommissioning': self.decommissioning_data,
            'common': self.common_data
        }
        
        for phase_name, phase_data in phases.items():
            phase_path = output_path / phase_name
            phase_path.mkdir(exist_ok=True)
            
            # Export as JSON
            json_path = phase_path / f'{phase_name}_data.json'
            with open(json_path, 'w') as f:
                # Custom JSON encoder for numpy arrays and datetime
                class NumpyEncoder(json.JSONEncoder):
                    def default(self, obj):
                        if isinstance(obj, np.ndarray):
                            return obj.tolist()
                        if isinstance(obj, (np.integer, np.floating)):
                            return obj.item()
                        if isinstance(obj, datetime):
                            return obj.isoformat()
                        if isinstance(obj, pd.Timestamp):
                            return obj.isoformat()
                        return super().default(obj)
                
                json.dump(phase_data, f, cls=NumpyEncoder, indent=2)
            
            print(f"  {phase_name}: {json_path}")
            
            # Also export time-series data as CSV for production phase
            if phase_name == 'production' and 'well_production' in phase_data:
                csv_dir = phase_path / 'csv_data'
                csv_dir.mkdir(exist_ok=True)
                
                # Export well production data
                for well_id, well_data in phase_data['well_production'].items():
                    # Create DataFrame for each well
                    df = pd.DataFrame({
                        'time': well_data['time'],
                        'oil_rate_bopd': well_data['oil_rate_bopd'],
                        'gas_rate_mcfd': well_data['gas_rate_mcfd'],
                        'water_rate_bwpd': well_data['water_rate_bwpd'],
                        'wellhead_pressure_psi': well_data['wellhead_pressure_psi'],
                        'wellhead_temperature_f': well_data['wellhead_temperature_f']
                    })
                    
                    # Save first 100,000 rows as sample
                    sample_path = csv_dir / f'{well_id}_sample.csv'
                    df.iloc[:100000].to_csv(sample_path, index=False)
                
                print(f"    CSV samples exported to {csv_dir}")
        
        # Export metadata and configuration
        metadata = {
            'generation_info': {
                'system': 'Complete Asset Lifecycle Synthetic Data Generator',
                'version': '1.0.0',
                'generation_date': datetime.now().isoformat(),
                'simulation_period': {
                    'start': self.start_date.isoformat(),
                    'end': (self.start_date + timedelta(days=self.simulation_days)).isoformat(),
                    'days': self.simulation_days
                },
                'sampling_frequency_hz': self.sampling_rate,
                'total_data_points': self.total_seconds,
                'random_seed': self.seed
            },
            'asset_info': self.config['asset'],
            'data_summary': {
                'exploration_data_size': self._estimate_data_size(self.exploration_data),
                'development_data_size': self._estimate_data_size(self.development_data),
                'production_data_size': self._estimate_data_size(self.production_data),
                'maintenance_data_size': self._estimate_data_size(self.maintenance_data),
                'decommissioning_data_size': self._estimate_data_size(self.decommissioning_data),
                'common_data_size': self._estimate_data_size(self.common_data)
            }
        }
        
        metadata_path = output_path / 'metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        print(f"  Metadata: {metadata_path}")
        
        # Create summary report
        self._create_summary_report(output_path)
        
        return output_path
    
    def _estimate_data_size(self, data):
        """Estimate the size of data structure"""
        import sys
        return sys.getsizeof(str(data)) / (1024 * 1024)  # MB
    
    def _create_summary_report(self, output_path):
        """Create comprehensive summary report"""
        report = f"""
        ============================================================
        COMPLETE ASSET LIFECYCLE SYNTHETIC DATA GENERATION REPORT
        ============================================================
        
        GENERATION SUMMARY:
        Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        Asset: {self.config['asset']['name']}
        Location: {self.config['asset']['location']}
        
        SIMULATION PARAMETERS:
        Period: {self.start_date.strftime('%Y-%m-%d')} to {(self.start_date + timedelta(days=self.simulation_days)).strftime('%Y-%m-%d')}
        Duration: {self.simulation_days} days ({self.simulation_days/30:.1f} months)
        Sampling Frequency: {self.sampling_rate} Hz
        Total Data Points per High-Freq Sensor: {self.total_seconds:,}
        
        DATA GENERATED BY PHASE:
        
        1. EXPLORATION & APPRAISAL:
           - 3D Seismic Data: {self.config['exploration']['seismic_grid']['x']}x{self.config['exploration']['seismic_grid']['y']}x{self.config['exploration']['seismic_grid']['z']} volume
           - Well Logs: {self.config['exploration']['well_count']} exploration wells
           - Geophysical Surveys: Gravity and magnetic data
           - Core Analysis: 5 cores with detailed measurements
           - Geological Interpretation: Horizons and fault systems
           - Prospect Evaluation: Risked volume estimates
        
        2. DEVELOPMENT & PLANNING:
           - Reservoir Model: 50x50x20 grid with petrophysical properties
           - Well Plans: {self.config['development']['planned_wells']} development wells
           - Facilities Design: Processing plant and export systems
           - Drilling Data: Daily reports and real-time parameters
           - Economic Analysis: NPV, IRR, sensitivity analysis
           - Risk Analysis: Technical and economic risks
        
        3. PRODUCTION & OPERATIONS:
           - Well Production Data: 25 wells at 1Hz for 6 months
           - Process Plant Data: Separators, compressors, heat exchangers
           - Equipment Monitoring: Vibration, temperature, performance
           - Energy Data: Consumption, optimization opportunities
           - Environmental Data: Emissions, water management
           - Daily Operations: Production reports, maintenance activities
        
        4. MAINTENANCE & REVAMP:
           - Condition Monitoring: Vibration, thermography, oil analysis
           - Inspection Data: Pressure vessels, piping, safety devices
           - Maintenance History: 150 work orders with detailed records
           - Inventory Management: Critical spares and consumables
           - Revamp Projects: Upgrade projects with business cases
           - Predictive Maintenance: Failure predictions and RUL estimates
        
        5. DECOMMISSIONING & SITE REHABILITATION:
           - Asset Condition: Comprehensive assessment of all assets
           - Cost Estimates: Detailed breakdown with scenarios
           - Environmental Baseline: Soil and groundwater contamination
           - Regulatory Requirements: Federal, state, and local regulations
           - Decommissioning Schedule: 3-year master schedule
           - Rehabilitation Plan: Site restoration and monitoring
        
        6. COMMON DATA:
           - Weather Data: Hourly Austin, TX weather for 6 months
           - Market Data: Daily oil and gas prices with forecasts
           - Personnel Data: Organizational structure and training records
           - Document Data: Procedures, standards, and reports
        
        DATA CHARACTERISTICS:
        - Realistic patterns and correlations
        - Physics-based models where applicable
        - Realistic failure modes and progression
        - Environmental and market context
        - Regulatory and compliance framework
        
        DATA APPLICATIONS:
        1. AI/ML Model Training and Validation
        2. Digital Twin Development
        3. Operational Excellence Studies
        4. Risk Assessment and Management
        5. Training and Simulation
        6. System Integration Testing
        
        OUTPUT LOCATION: {output_path}
        
        ============================================================
        """
        
        report_path = output_path / 'generation_summary.txt'
        with open(report_path, 'w') as f:
            f.write(report)
        
        print(f"  Summary report: {report_path}")

# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main execution function"""
    print("Complete Asset Lifecycle Synthetic Data Generator")
    print("="*60)
    print("Generating 6 months of data for all 5 lifecycle phases")
    print("="*60)
    
    try:
        # Initialize generator
        generator = CompleteAssetLifecycleGenerator()
        
        # Generate all data
        print("\nStarting comprehensive data generation...")
        start_time = datetime.now()
        
        generator.generate_all_data()
        
        # Export data
        output_path = generator.export_data()
        
        elapsed = datetime.now() - start_time
        
        print("\n" + "="*60)
        print("GENERATION COMPLETE!")
        print("="*60)
        print(f"Total execution time: {elapsed.total_seconds():.1f} seconds")
        print(f"Data available in: {output_path}")
        print("="*60)
        
    except Exception as e:
        print(f"Error during data generation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

## **فایل‌های تکمیلی پروژه:**

### **1. complete_config.yaml**
```yaml
# Complete configuration for asset lifecycle data generation

asset:
  name: "Eagle_Ford_Integrated_Asset"
  location: "South Texas, USA"
  type: "Integrated Oil & Gas Production Facility"
  coordinates:
    latitude: 29.4241
    longitude: -98.4936
  area_acres: 3200
  water_depth: "Onshore"

simulation:
  start_date: "2024-01-01"
  days: 182
  sampling_rate_hz: 1
  random_seed: 42
  timezone: "America/Chicago"

exploration:
  seismic_grid:
    x: 1000
    y: 1000
    z: 500
  well_count: 10
  survey_types: ["2D", "3D", "MT", "Gravity"]
  basin: "Eagle Ford"
  play_type: "Unconventional Shale"

development:
  planned_wells: 25
  platform_types: ["Land-based"]
  facilities:
    processing_plant: true
    compression_station: true
    water_treatment: true
    storage_tanks: true
  development_strategy: "Phased Development"

production:
  peak_production_bopd: 50000
  gas_production_mmscfd: 150
  water_production_bwpd: 80000
  facility_capacity: 100000
  product_types: ["Light Oil", "Associated Gas", "NGLs"]

maintenance:
  strategy: "Predictive & Preventive"
  critical_equipment_count: 45
  spare_parts_inventory: true
  cmms_integration: true

decommissioning:
  planned_year: 2045
  reserve_fund: true
  environmental_liability: true

data_quality:
  add_noise: true
  include_failures: true
  realistic_correlations: true
  validate_physics: true

output:
  formats: ["json", "csv", "parquet", "sqlite"]
  compress: true
  structure_by_phase: true
  include_metadata: true
```

### **2. requirements.txt**
```txt
numpy>=1.21.0
pandas>=1.3.0
scipy>=1.7.0
pyyaml>=6.0
xarray>=0.20.0
h5py>=3.7.0
tables>=3.7.0
matplotlib>=3.4.0
seaborn>=0.11.0
pyarrow>=5.0.0
sqlalchemy>=1.4.0
openpyxl>=3.0.0
```

### **3. run_complete_generation.py**
```python
#!/usr/bin/env python3
"""
Simplified runner for complete data generation
"""

import sys
import time
from pathlib import Path

def check_requirements():
    """Check if all requirements are installed"""
    required = ['numpy', 'pandas', 'yaml', 'scipy']
    
    missing = []
    for package in required:
        try:
            __import__(package)
        except ImportError:
            missing.append(package)
    
    if missing:
        print(f"Missing packages: {missing}")
        print("Install with: pip install " + " ".join(missing))
        return False
    
    return True

def main():
    """Main execution"""
    if not check_requirements():
        sys.exit(1)
    
    print("="*70)
    print("COMPLETE ASSET LIFECYCLE DATA GENERATOR")
    print("="*70)
    
    # Import and run generator
    from complete_generator import main as run_generator
    
    start_time = time.time()
    run_generator()
    elapsed = time.time() - start_time
    
    print(f"\nTotal execution time: {elapsed:.2f} seconds")
    
    # Display generated file structure
    output_dir = Path('complete_asset_data')
    if output_dir.exists():
        print(f"\nGenerated file structure:")
        for item in output_dir.rglob('*'):
            if item.is_file():
                rel_path = item.relative_to(output_dir)
                size_mb = item.stat().st_size / (1024 * 1024)
                print(f"  {rel_path} ({size_mb:.1f} MB)")

if __name__ == "__main__":
    main()
```

### **4. validate_data.py**
```python
#!/usr/bin/env python3
"""
Data validation script for generated synthetic data
"""

import json
import pandas as pd
from pathlib import Path
import numpy as np

def validate_data(data_dir='complete_asset_data'):
    """Validate generated data for quality and consistency"""
    print("Validating generated data...")
    
    issues = []
    warnings = []
    
    # Check each phase directory exists
    phases = ['exploration', 'development', 'production', 
              'maintenance', 'decommissioning', 'common']
    
    for phase in phases:
        phase_path = Path(data_dir) / phase
        if not phase_path.exists():
            issues.append(f"Missing phase directory: {phase}")
            continue
        
        # Check JSON file exists
        json_file = phase_path / f'{phase}_data.json'
        if not json_file.exists():
            issues.append(f"Missing JSON file for phase: {phase}")
            continue
        
        # Load and validate JSON
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            # Basic validation
            if not data:
                warnings.append(f"Empty data in phase: {phase}")
            
            # Phase-specific validations
            if phase == 'production':
                if 'well_production' in data:
                    wells = data['well_production']
                    for well_id, well_data in wells.items():
                        # Check data ranges
                        oil_rates = well_data['oil_rate_bopd']
                        if min(oil_rates) < 0:
                            issues.append(f"Negative oil rate in {well_id}")
                        
                        pressures = well_data['wellhead_pressure_psi']
                        if max(pressures) > 5000:
                            warnings.append(f"Unusually high pressure in {well_id}")
            
        except Exception as e:
            issues.append(f"Error loading {phase}: {str(e)}")
    
    # Check metadata
    metadata_path = Path(data_dir) / 'metadata.json'
    if metadata_path.exists():
        try:
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            
            # Validate metadata structure
            required_keys = ['generation_info', 'asset_info', 'data_summary']
            for key in required_keys:
                if key not in metadata:
                    issues.append(f"Missing metadata key: {key}")
        
        except Exception as e:
            issues.append(f"Error loading metadata: {str(e)}")
    else:
        issues.append("Missing metadata file")
    
    # Report results
    print(f"\nValidation Results:")
    print(f"  Issues found: {len(issues)}")
    print(f"  Warnings: {len(warnings)}")
    
    if issues:
        print("\nIssues:")
        for issue in issues[:10]:  # Show first 10 issues
            print(f"  - {issue}")
    
    if warnings:
        print("\nWarnings:")
        for warning in warnings[:10]:
            print(f"  - {warning}")
    
    if not issues and not warnings:
        print("\nAll data validation checks passed!")
    
    return len(issues) == 0

if __name__ == "__main__":
    validate_data()
```

## **راهنمای اجرا:**

### **نصب:**
```bash
# 1. ایجاد دایرکتوری پروژه
mkdir complete_asset_data_generation
cd complete_asset_data_generation

# 2. ایجاد محیط مجازی (اختیاری)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# یا
venv\Scripts\activate  # Windows

# 3. نصب کتابخانه‌ها
pip install numpy pandas scipy pyyaml pyarrow

# 4. ایجاد فایل‌های پیکربندی
# کدهای بالا را در فایل‌های جداگانه ذخیره کنید
```

### **اجرا:**
```bash
# اجرای کامل
python run_complete_generation.py

# یا مستقیم
python complete_generator.py
```

### **خروجی‌های تولید شده:**
```
complete_asset_data/
├── exploration/
│   ├── exploration_data.json          # داده‌های کامل اکتشاف
│   └── seismic_data.h5                # داده‌های لرزه‌ای (اختیاری)
├── development/
│   └── development_data.json          # داده‌های توسعه
├── production/
│   ├── production_data.json           # داده‌های تولید
│   └── csv_data/                      # داده‌های سری زمانی
│       ├── PROD-001_sample.csv
│       ├── PROD-002_sample.csv
│       └── ...
├── maintenance/
│   └── maintenance_data.json          # داده‌های نگهداشت
├── decommissioning/
│   └── decommissioning_data.json      # داده‌های برچیدن
├── common/
│   └── common_data.json               # داده‌های مشترک
├── metadata.json                      # متادیتای کامل
└── generation_summary.txt             # گزارش خلاصه
```

## **ویژگی‌های کلیدی سیستم:**

### **1. جامعیت:**
- پوشش کامل 5 مرحله چرخه حیات
- داده‌های فنی، عملیاتی، اقتصادی، محیط‌زیستی
- سنسورهای 1 هرتزی تا داده‌های ماهانه

### **2. واقع‌گرایی:**
- مدل‌های فیزیکی برای فرآیندها
- الگوهای خرابی واقعی
- همبستگی‌های صحیح بین متغیرها
- شرایط محیطی منطبق با آستین، تگزاس

### **3. مقیاس‌پذیری:**
- تولید 15.7 میلیون نقطه داده بر سنسور
- پشتیبانی از هزاران سنسور
- فرمت‌های خروجی متنوع

### **4. تکرارپذیری:**
- seedهای تصادفی کنترل‌شده
- مستندات کامل
- قابلیت بازتولید دقیق

### **5. قابلیت استفاده:**
- ساختار منظم داده‌ها
- متادیتای کامل
- نمونه‌های آماده برای استفاده

## **کاربردهای داده‌های تولید شده:**

### **1. آموزش مدل‌های هوش مصنوعی:**
- پیش‌بینی خرابی تجهیزات
- بهینه‌سازی تولید
- تحلیل ریسک
- کشف الگوهای پیچیده

### **2. توسعه سیستم‌های دیجیتال:**
- دوقلوی دیجیتال
- سیستم‌های مانیتورینگ
- داشبوردهای تحلیلی
- سیستم‌های پیش‌بینی

### **3. تحقیق و توسعه:**
- تست الگوریتم‌های جدید
- شبیه‌سازی سناریوها
- تحلیل حساسیت
- مطالعات امکان‌سنجی

### **4. آموزش و توانمندسازی:**
- سناریوهای آموزشی
- شبیه‌سازی عملیات
- مدیریت بحران
- تصمیم‌گیری در شرایط پیچیده


پروژه: **توسعه سیستم تولید داده‌های سنتتیک برای یک پالایشگاه گاز طبیعی در آستین، تگزاس**

---

## **سند نیازمندی‌های نرم‌افزاری (SRS) - سیستم تولید داده‌های سنتتیک برای پالایشگاه گاز**

### **1. مقدمه**
#### **1.1 هدف**
ایجاد یک سیستم تولید داده‌های سنتتیک جامع برای شبیه‌سازی 6 ماه عملکرد کامل یک پالایشگاه گاز طبیعی در آستین، تگزاس، با وضوح زمانی 1 ثانیه‌ای برای آموزش و تست پلتفرم ApexAsset AI.

#### **1.2 دامنه**
این سیستم داده‌های سنتتیک را برای تمام تجهیزات و فرآیندهای یک پالایشگاه گاز متوسط شامل می‌شود.

### **2. توصیف کلی**
#### **2.1 فرآورده مشخص: پالایشگاه گاز طبیعی "Lone Star Gas Refining"**
- **موقعیت:** آستین، تگزاس
- **ظرفیت:** 200 میلیون فوت مکعب استاندارد در روز (MMSCFD)
- **فرآورده‌های اصلی:** 
  - گاز طبیعی تصفیه شده (Pipeline Quality)
  - اتان (برای پتروشیمی)
  - پروپان و بوتان (LPG)
  - کاندنسیت (Natural Gasoline)
- **فرآیندهای کلیدی:**
  1. شیرین‌سازی (آمین، غشاء)
  2. نم‌زدایی (جذب TEG)
  3. جداسازی کرایوژنیک (دمای پایین)
  4. جداسازی فراکسیون‌ها
  5. بازیافت گوگرد

### **3. نیازمندی‌های ویژه**

#### **3.1 نیازمندی‌های عملکردی**
| ID | نیازمندی | توضیح |
|----|----------|--------|
| FR-01 | تولید داده با فرکانس 1Hz | تمام سنسورها هر ثانیه داده تولید کنند |
| FR-02 | شبیه‌سازی 6 ماه کامل | 182 روز × 24 ساعت × 3600 ثانیه = 15,724,800 نقطه داده بر سنسور |
| FR-03 | پوشش تمام مراحل فرآیند | از ورود گاز خام تا خروج فرآورده‌ها |
| FR-04 | مدل‌سازی خرابی‌ها | 5 نوع خرابی اصلی با الگوهای زمانی واقع‌گرا |
| FR-05 | تولید داده‌های متنی | گزارش‌های شیفت، لاگ تعمیرات، آلارم‌ها |
| FR-06 | داده‌های محیطی | دما، رطوبت، فشار جو منطقه آستین |
| FR-07 | داده‌های اقتصادی | قیمت‌های روزانه گاز و فرآورده‌ها |

#### **3.2 نیازمندی‌های غیرعملکردی**
| ID | نیازمندی | توضیح |
|----|----------|--------|
| NFR-01 | کارایی | تولید 15M نقطه داده در کمتر از 1 ساعت |
| NFR-02 | قابلیت اطمینان | داده‌ها از نظر فیزیکی معتبر باشند |
| NFR-03 | قابلیت تکرار | seedهای تصادفی برای تکرارپذیری |
| NFR-04 | فرمت‌های خروجی | CSV, Parquet, JSON, SQLite |
| NFR-05 | مستندات | مستند کامل الگوهای داده و خرابی‌ها |

### **4. معماری سیستم**

#### **4.1 اجزای سیستم**
```
┌─────────────────────────────────────────────────┐
│              Synthetic Data Generator           │
├─────────────────────────────────────────────────┤
│ 1. Configuration Manager                        │
│ 2. Physics-Based Model Engine                  │
│ 3. Failure Mode Generator                      │
│ 4. Environmental Data Simulator                │
│ 5. Text Data Generator                         │
│ 6. Data Export & Formatting Module             │
└─────────────────────────────────────────────────┘
```

#### **4.2 انواع داده‌های تولیدی**
1. **داده‌های فرآیندی (Process Data)**
   - دما، فشار، دبی
   - آنالیز ترکیب گاز (GC Data)
   - سطح مخازن

2. **داده‌های وضعیت تجهیزات (Equipment Health)**
   - ارتعاشات
   - دمای یاتاقان‌ها
   - جریان موتورها

3. **داده‌های محیطی (Environmental)**
   - آب و هوای آستین
   - شرایط جوی

4. **داده‌های عملیاتی (Operational)**
   - گزارش‌های شیفت
   - دستورالعمل‌های عملیاتی
   - رویدادهای نگهداشت

### **5. جزئیات تولید داده**

#### **5.1 پیکربندی پالایشگاه**
```python
PLANT_CONFIG = {
    "location": {
        "city": "Austin",
        "state": "Texas",
        "coordinates": {"lat": 30.2672, "lon": -97.7431},
        "elevation": 149  # meters
    },
    "capacity": {
        "feed_gas": 200,  # MMSCFD
        "products": {
            "pipeline_gas": 150,  # MMSCFD
            "ethane": 15,  # MBPD
            "propane": 10,  # MBPD
            "butane": 5,   # MBPD
            "condensate": 8  # MBPD
        }
    },
    "process_units": {
        "inlet_separation": {"vessels": 3, "compressors": 2},
        "amine_treating": {"contactors": 2, "regenerators": 2, "pumps": 8},
        "dehydration": {"TEG_contactors": 2, "reboilers": 2},
        "cryogenic_plant": {"exchangers": 12, "turbines": 2, "towers": 3},
        "fractionation": {"deethanizer": 1, "depropanizer": 1, "debutanizer": 1},
        "sulfur_recovery": {"reactors": 2, "incinerator": 1}
    },
    "sensor_count": {
        "temperature": 450,
        "pressure": 380,
        "flow": 220,
        "level": 85,
        "vibration": 120,
        "composition": 18,
        "total_sensors": 1273
    }
}
```

#### **5.2 الگوهای خرابی**
```python
FAILURE_MODES = {
    "fouling": {
        "description": "رسوب‌گیری در مبدل‌های حرارتی",
        "affected_equipment": ["exchangers", "reboilers"],
        "development_time": "7-30 days",
        "symptoms": {
            "delta_p": "increase_gradual",  # افزایش تدریجی اختلاف فشار
            "efficiency": "decrease_gradual",  # کاهش تدریجی راندمان
            "outlet_temp": "decrease_slow"  # کاهش آهسته دمای خروجی
        }
    },
    "bearing_wear": {
        "description": "سایش یاتاقان‌های کمپرسور",
        "affected_equipment": ["compressors", "pumps"],
        "development_time": "14-60 days",
        "symptoms": {
            "vibration": "increase_oscillating",  # افزایش نوسانی ارتعاش
            "temperature": "increase_steady",  # افزایش پیوسته دما
            "noise": "increase_gradual"  # افزایش تدریجی نویز
        }
    },
    "corrosion": {
        "description": "خوردگی در خطوط لوله",
        "affected_equipment": ["pipelines", "vessels"],
        "development_time": "90-180 days",
        "symptoms": {
            "wall_thickness": "decrease_steady",  # کاهش پیوسته ضخامت
            "leak_indication": "intermittent",  # نشانه‌های نشت متناوب
            "product_purity": "decrease_slow"  # کاهش آهسته خلوص محصول
        }
    },
    "catalyst_deactivation": {
        "description": "غیرفعال شدن کاتالیست",
        "affected_equipment": ["reactors"],
        "development_time": "30-90 days",
        "symptoms": {
            "conversion": "decrease_exponential",  # کاهش نمایی تبدیل
            "pressure_drop": "increase_rapid",  # افزایش سریع افت فشار
            "byproducts": "increase_steady"  # افزایش پیوسته محصولات جانبی
        }
    },
    "instrument_drift": {
        "description": "درفت سنسورها",
        "affected_equipment": ["all_instruments"],
        "development_time": "60-180 days",
        "symptoms": {
            "reading_error": "linear_drift",  # خطای خطی در قرائت
            "calibration_needed": "periodic"  # نیاز دوره‌ای به کالیبراسیون
        }
    }
}
```

---

## **کد تولید کننده داده‌های سنتتیک (Data Generator)**

### **کنسپت کلی:**
این کد یک سیستم تولید داده‌های سنتتیک پیچیده ایجاد می‌کند که:
1. داده‌های پایه فرآیندی بر اساس مدل‌های فیزیکی تولید می‌کند
2. خرابی‌های واقع‌گرا با الگوهای زمانی پیچیده اضافه می‌کند
3. داده‌های محیطی منطبق با آستین، تگزاس تولید می‌کند
4. داده‌های متنی عملیاتی می‌سازد
5. همه را در فرمت‌های مختلف خروجی می‌دهد

### **فایل‌های اصلی پروژه:**

#### **1. main_generator.py**
```python
#!/usr/bin/env python3
"""
Synthetic Data Generator for Natural Gas Processing Plant
Austin, Texas - 6 Months Simulation at 1Hz Frequency
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import sqlite3
import json
import yaml
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class SyntheticGasPlantGenerator:
    """Main generator class for synthetic gas plant data"""
    
    def __init__(self, config_file='plant_config.yaml'):
        """Initialize generator with plant configuration"""
        
        # Load configuration
        self.config = self._load_config(config_file)
        
        # Plant specifications
        self.plant_name = "Lone_Star_Gas_Refining_Austin"
        self.location = "Austin, Texas"
        
        # Time parameters
        self.start_date = datetime(2024, 1, 1, 0, 0, 0)
        self.simulation_days = 182  # 6 months
        self.total_seconds = self.simulation_days * 24 * 3600
        self.sampling_rate = 1  # 1 Hz
        
        # Generate time index
        self.time_index = self._generate_time_index()
        
        # Initialize data storage
        self.process_data = {}
        self.equipment_data = {}
        self.environmental_data = {}
        self.text_data = {}
        self.failure_events = []
        
        # Random seed for reproducibility
        self.seed = 42
        np.random.seed(self.seed)
        
        print(f"Initialized {self.plant_name}")
        print(f"Period: {self.start_date.date()} to {(self.start_date + timedelta(days=self.simulation_days)).date()}")
        print(f"Total data points per sensor: {self.total_seconds:,}")
        
    def _load_config(self, config_file):
        """Load plant configuration from YAML file"""
        with open(config_file, 'r') as f:
            return yaml.safe_load(f)
    
    def _generate_time_index(self):
        """Generate datetime index for 6 months at 1Hz"""
        return pd.date_range(
            start=self.start_date,
            periods=self.total_seconds,
            freq='S'
        )
    
    def _generate_weather_patterns(self):
        """Generate realistic Austin, TX weather patterns"""
        print("Generating weather patterns for Austin, TX...")
        
        # Austin climate parameters (January to June)
        base_temp = np.array([
            10, 12, 16, 20, 24, 28,  # Base temperatures for each month (C)
        ])
        
        # Expand to daily pattern
        daily_temp = np.repeat(base_temp, 30)[:self.simulation_days]
        
        # Add daily variation (colder at night, warmer at day)
        daily_cycle = 8 * np.sin(2 * np.pi * np.arange(24) / 24)  # 24-hour cycle
        
        # Create full temperature series
        temperature = np.zeros(len(self.time_index))
        for day in range(self.simulation_days):
            day_start = day * 86400
            month_idx = min(day // 30, 5)
            base = daily_temp[day]
            
            for hour in range(24):
                hour_start = day_start + hour * 3600
                hour_end = hour_start + 3600
                temp_variation = daily_cycle[hour]
                
                # Add random weather effects
                weather_noise = np.random.normal(0, 1.5)
                
                temperature[hour_start:hour_end] = (
                    base + temp_variation + weather_noise
                )
        
        # Humidity pattern (higher at night, lower during day)
        humidity = 50 + 20 * np.sin(2 * np.pi * np.arange(len(self.time_index)) / 86400 - np.pi/2)
        humidity += np.random.normal(0, 5, len(self.time_index))
        humidity = np.clip(humidity, 30, 85)
        
        # Atmospheric pressure (normal variations)
        pressure = 1013.25 + 10 * np.sin(2 * np.pi * np.arange(len(self.time_index)) / (86400 * 7))  # Weekly cycle
        pressure += np.random.normal(0, 3, len(self.time_index))
        
        # Rain events (simplified)
        rain_prob = np.array([0.1, 0.15, 0.2, 0.25, 0.3, 0.2])  # Monthly rain probability
        rain_intensity = np.zeros(len(self.time_index))
        
        for day in range(self.simulation_days):
            month_idx = min(day // 30, 5)
            if np.random.random() < rain_prob[month_idx] / 30:  # Daily probability
                rain_day = day * 86400
                rain_start = rain_day + np.random.randint(12, 20) * 3600  # Afternoon rain
                rain_duration = np.random.randint(3600, 10800)  # 1-3 hours
                rain_end = min(rain_start + rain_duration, (day+1)*86400)
                
                intensity = np.random.uniform(1, 10)  # mm/hour
                rain_intensity[rain_start:rain_end] = intensity
        
        return {
            'temperature_C': temperature,
            'humidity_percent': humidity,
            'pressure_hPa': pressure,
            'rain_intensity_mm_h': rain_intensity
        }
    
    def _generate_process_data(self):
        """Generate synthetic process data for gas plant"""
        print("Generating process data...")
        
        process_units = [
            'inlet_separation',
            'amine_treating', 
            'dehydration',
            'cryogenic_plant',
            'fractionation',
            'sulfur_recovery'
        ]
        
        data = {}
        
        for unit in process_units:
            unit_config = self.config['process_units'][unit]
            
            if unit == 'inlet_separation':
                # Inlet separation unit data
                data[f'{unit}_feed_pressure'] = 5000 + 100 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 86400)
                data[f'{unit}_feed_pressure'] += np.random.normal(0, 50, self.total_seconds)
                
                data[f'{unit}_feed_temperature'] = 35 + 5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 43200)
                data[f'{unit}_feed_temperature'] += np.random.normal(0, 2, self.total_seconds)
                
                data[f'{unit}_separator_level'] = 50 + 10 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 7200)
                data[f'{unit}_separator_level'] += np.random.normal(0, 3, self.total_seconds)
                
            elif unit == 'amine_treating':
                # Amine unit data
                data[f'{unit}_contact_pressure'] = 4900 + 80 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 86400)
                data[f'{unit}_contact_temp'] = 40 + 3 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 21600)
                
                # CO2 removal efficiency
                base_efficiency = 98.5
                data[f'{unit}_co2_removal'] = base_efficiency + 0.5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 43200)
                data[f'{unit}_co2_removal'] += np.random.normal(0, 0.1, self.total_seconds)
                
            elif unit == 'cryogenic_plant':
                # Cryogenic plant data
                data[f'{unit}_cold_box_temp'] = -100 + 5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 14400)
                data[f'{unit}_expander_speed'] = 12000 + 200 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 3600)
                data[f'{unit}_lpg_production'] = self.config['capacity']['products']['propane'] * 1000 / 24 / 3600
                data[f'{unit}_lpg_production'] += np.random.normal(0, 10, self.total_seconds)
                
            # Add more unit-specific data generation...
        
        # Add common parameters
        data['plant_throughput'] = self.config['capacity']['feed_gas'] * 1e6 / 24 / 3600  # SCF per second
        data['plant_throughput'] += np.random.normal(0, 5000, self.total_seconds)
        
        return data
    
    def _generate_equipment_health_data(self):
        """Generate equipment health monitoring data"""
        print("Generating equipment health data...")
        
        equipment_types = [
            'centrifugal_compressors',
            'reciprocating_compressors', 
            'pumps',
            'heat_exchangers',
            'columns',
            'furnaces'
        ]
        
        data = {}
        
        # Vibration data for rotating equipment
        for i in range(12):  # 12 major rotating equipments
            base_vibration = 1.0 + 0.2 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 3600)
            
            # Add bearing defect pattern (if failed)
            if np.random.random() < 0.3:  # 30% chance of developing fault
                fault_start = np.random.randint(self.total_seconds // 4, self.total_seconds // 2)
                fault_duration = np.random.randint(7*86400, 30*86400)  # 1 week to 1 month
                
                # Bearing defect signature (increasing vibration with periodic spikes)
                time_to_failure = np.arange(self.total_seconds - fault_start)
                degradation = 0.00001 * time_to_failure**1.5  # Nonlinear degradation
                
                bearing_frequency = 3.7  # BPFO frequency multiple
                defect_signal = 0.5 * np.sin(2 * np.pi * bearing_frequency * np.arange(self.total_seconds) / 3600)
                defect_signal[:fault_start] = 0
                defect_signal[fault_start:] *= degradation[:len(defect_signal[fault_start:])]
                
                base_vibration += defect_signal
            
            data[f'compressor_{i+1:02d}_vibration_mm_s'] = np.clip(base_vibration + np.random.normal(0, 0.05, self.total_seconds), 0, 10)
        
        # Bearing temperatures
        for i in range(24):  # 24 bearing temperature points
            base_temp = 65 + 5 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 7200)
            data[f'bearing_{i+1:02d}_temp_C'] = base_temp + np.random.normal(0, 2, self.total_seconds)
        
        # Motor currents
        for i in range(8):  # 8 major motors
            base_current = 100 + 10 * np.sin(2 * np.pi * np.arange(self.total_seconds) / 1800)
            data[f'motor_{i+1:02d}_current_A'] = base_current + np.random.normal(0, 5, self.total_seconds)
        
        return data
    
    def _generate_failure_events(self):
        """Generate realistic failure events with progression patterns"""
        print("Generating failure events...")
        
        failures = []
        
        # Define failure types and their characteristics
        failure_definitions = [
            {
                'type': 'heat_exchanger_fouling',
                'start_day': 45,
                'duration_days': 21,
                'affected_sensors': ['cryogenic_plant_cold_box_temp', 'cryogenic_plant_exchanger_dp'],
                'pattern': 'exponential'
            },
            {
                'type': 'compressor_bearing_wear',
                'start_day': 90,
                'duration_days': 45,
                'affected_sensors': ['compressor_03_vibration_mm_s', 'bearing_07_temp_C'],
                'pattern': 'linear'
            },
            {
                'type': 'catalyst_deactivation',
                'start_day': 120,
                'duration_days': 60,
                'affected_sensors': ['sulfur_recovery_conversion', 'sulfur_recovery_temp'],
                'pattern': 'sigmoid'
            },
            {
                'type': 'valve_stiction',
                'start_day': 150,
                'duration_days': 15,
                'affected_sensors': ['amine_treating_control_valve_position'],
                'pattern': 'oscillating'
            }
        ]
        
        for failure in failure_definitions:
            start_second = failure['start_day'] * 86400
            duration_seconds = failure['duration_days'] * 86400
            
            failures.append({
                'failure_id': f"F{len(failures)+1:03d}",
                'failure_type': failure['type'],
                'start_time': self.start_date + timedelta(seconds=start_second),
                'end_time': self.start_date + timedelta(seconds=start_second + duration_seconds),
                'affected_sensors': failure['affected_sensors'],
                'severity': np.random.uniform(0.3, 0.8),
                'progression_pattern': failure['pattern']
            })
        
        return failures
    
    def _apply_failures_to_data(self, process_data, equipment_data):
        """Apply failure patterns to generated data"""
        print("Applying failure patterns...")
        
        for failure in self.failure_events:
            start_idx = (failure['start_time'] - self.start_date).total_seconds()
            end_idx = start_idx + (failure['end_time'] - failure['start_time']).total_seconds()
            start_idx, end_idx = int(start_idx), int(end_idx)
            
            for sensor in failure['affected_sensors']:
                if sensor in process_data:
                    data_array = process_data[sensor]
                elif sensor in equipment_data:
                    data_array = equipment_data[sensor]
                else:
                    continue
                
                failure_length = end_idx - start_idx
                if failure_length <= 0:
                    continue
                
                # Create failure profile based on pattern
                if failure['progression_pattern'] == 'linear':
                    profile = np.linspace(0, failure['severity'], failure_length)
                elif failure['progression_pattern'] == 'exponential':
                    profile = failure['severity'] * (1 - np.exp(-5 * np.arange(failure_length) / failure_length))
                elif failure['progression_pattern'] == 'sigmoid':
                    x = np.linspace(-6, 6, failure_length)
                    profile = failure['severity'] / (1 + np.exp(-x))
                elif failure['progression_pattern'] == 'oscillating':
                    base = failure['severity'] * np.arange(failure_length) / failure_length
                    oscillation = 0.3 * failure['severity'] * np.sin(2 * np.pi * np.arange(failure_length) / 3600)
                    profile = base + oscillation
                
                # Apply failure profile
                if 'temperature' in sensor or 'temp' in sensor:
                    data_array[start_idx:end_idx] += profile * 20  # Temperature increase
                elif 'vibration' in sensor:
                    data_array[start_idx:end_idx] += profile * 5  # Vibration increase
                elif 'pressure' in sensor or 'dp' in sensor:
                    data_array[start_idx:end_idx] += profile * 100  # Pressure increase
                elif 'current' in sensor:
                    data_array[start_idx:end_idx] += profile * 50  # Current increase
        
        return process_data, equipment_data
    
    def _generate_text_data(self):
        """Generate synthetic operational text data"""
        print("Generating text data...")
        
        # Shift reports
        shift_times = pd.date_range(
            start=self.start_date,
            end=self.start_date + timedelta(days=self.simulation_days),
            freq='12H'  # Two shifts per day
        )
        
        shift_reports = []
        for i, shift_time in enumerate(shift_times):
            report = {
                'report_id': f"SR{i:05d}",
                'shift': 'Day' if shift_time.hour == 7 else 'Night',
                'date_time': shift_time,
                'supervisor': np.random.choice(['John Smith', 'Maria Garcia', 'Robert Johnson', 'Lisa Chen']),
                'throughput_avg': np.random.normal(195, 5),
                'key_events': np.random.choice([
                    'Normal operation',
                    'Minor flare event during startup',
                    'Preventive maintenance on pump P-101',
                    'Instrument calibration completed',
                    'Safety meeting conducted',
                    'Unit efficiency slightly below target'
                ], 2, p=[0.6, 0.1, 0.1, 0.1, 0.05, 0.05]),
                'issues': np.random.choice([
                    'None',
                    'High vibration on compressor C-201',
                    'Control valve CV-103 showing stiction',
                    'Analyzer offline for calibration',
                    'Small leak detected on flange'
                ], 1, p=[0.7, 0.1, 0.1, 0.05, 0.05])[0],
                'next_shift_notes': np.random.choice([
                    'Monitor compressor parameters',
                    'Prepare for catalyst changeout',
                    'Schedule heat exchanger cleaning',
                    'Continue normal surveillance'
                ], 1)[0]
            }
            shift_reports.append(report)
        
        # Maintenance work orders
        work_orders = []
        for i in range(50):  # 50 work orders over 6 months
            wo_date = self.start_date + timedelta(days=np.random.randint(0, self.simulation_days))
            equipment = np.random.choice([
                'Compressor C-101', 'Pump P-205', 'Heat Exchanger E-302',
                'Control Valve CV-108', 'Transformer T-401', 'Vessel V-201'
            ])
            
            work_orders.append({
                'wo_id': f"WO{i:04d}",
                'date_created': wo_date,
                'equipment': equipment,
                'priority': np.random.choice(['Low', 'Medium', 'High', 'Critical'], p=[0.4, 0.3, 0.2, 0.1]),
                'type': np.random.choice(['Preventive', 'Corrective', 'Predictive', 'Breakdown']),
                'description': f"{np.random.choice(['Inspect', 'Clean', 'Calibrate', 'Replace', 'Repair'])} {equipment}",
                'status': np.random.choice(['Open', 'In Progress', 'Completed', 'Cancelled']),
                'duration_hours': np.random.exponential(8)
            })
        
        # Alarm logs
        alarm_logs = []
        alarm_types = [
            'High Temperature', 'Low Pressure', 'High Vibration', 'Low Flow',
            'High Level', 'Low Level', 'Equipment Failure', 'Communication Loss'
        ]
        
        # Generate about 2000 alarms over 6 months
        for i in range(2000):
            alarm_time = self.start_date + timedelta(seconds=np.random.randint(0, self.total_seconds))
            alarm_type = np.random.choice(alarm_types)
            tag = f"{np.random.choice(['TI', 'PI', 'FI', 'LI', 'VI'])}-{np.random.randint(100, 999)}"
            
            alarm_logs.append({
                'alarm_id': f"AL{i:06d}",
                'timestamp': alarm_time,
                'tag': tag,
                'alarm_type': alarm_type,
                'priority': np.random.choice(['Low', 'Medium', 'High'], p=[0.5, 0.3, 0.2]),
                'description': f"{alarm_type} alarm on {tag}",
                'acknowledged': np.random.choice([True, False], p=[0.9, 0.1]),
                'cleared': np.random.choice([True, False], p=[0.85, 0.15]),
                'duration_seconds': np.random.exponential(60)
            })
        
        return {
            'shift_reports': shift_reports,
            'work_orders': work_orders,
            'alarm_logs': alarm_logs
        }
    
    def _generate_gas_composition_data(self):
        """Generate synthetic gas composition data"""
        print("Generating gas composition data...")
        
        # Composition changes more slowly - sample every 5 minutes
        composition_times = pd.date_range(
            start=self.start_date,
            periods=self.simulation_days * 24 * 12,  # Every 5 minutes
            freq='300S'
        )
        
        compositions = []
        
        # Base composition with seasonal variations
        for t in composition_times:
            day_of_year = t.timetuple().tm_yday
            
            # Seasonal variation in feed gas composition
            methane_base = 85 + 5 * np.sin(2 * np.pi * day_of_year / 365)
            ethane_base = 7 + 2 * np.sin(2 * np.pi * (day_of_year + 30) / 365)
            
            comp = {
                'timestamp': t,
                'methane_mol_percent': methane_base + np.random.normal(0, 0.5),
                'ethane_mol_percent': ethane_base + np.random.normal(0, 0.3),
                'propane_mol_percent': 3 + 1 * np.sin(2 * np.pi * (day_of_year + 60) / 365) + np.random.normal(0, 0.2),
                'butane_mol_percent': 1.5 + 0.5 * np.sin(2 * np.pi * (day_of_year + 90) / 365) + np.random.normal(0, 0.1),
                'pentane_plus_mol_percent': 0.8 + 0.2 * np.sin(2 * np.pi * (day_of_year + 120) / 365) + np.random.normal(0, 0.05),
                'nitrogen_mol_percent': 1.2 + np.random.normal(0, 0.1),
                'co2_mol_percent': 1.5 + 0.5 * np.sin(2 * np.pi * (day_of_year + 150) / 365) + np.random.normal(0, 0.1),
                'h2s_ppm': 50 + 20 * np.sin(2 * np.pi * day_of_year / 180) + np.random.exponential(10)
            }
            
            # Ensure percentages sum to ~100%
            total = sum(v for k, v in comp.items() if 'mol_percent' in k)
            for key in comp:
                if 'mol_percent' in key:
                    comp[key] = comp[key] * 100 / total
            
            compositions.append(comp)
        
        return compositions
    
    def generate_all_data(self):
        """Generate all synthetic data"""
        print("\n" + "="*60)
        print("Starting Synthetic Data Generation")
        print("="*60)
        
        # Generate environmental data
        self.environmental_data = self._generate_weather_patterns()
        
        # Generate process data
        self.process_data = self._generate_process_data()
        
        # Generate equipment health data
        self.equipment_data = self._generate_equipment_health_data()
        
        # Generate failure events
        self.failure_events = self._generate_failure_events()
        
        # Apply failures to data
        self.process_data, self.equipment_data = self._apply_failures_to_data(
            self.process_data, self.equipment_data
        )
        
        # Generate text data
        self.text_data = self._generate_text_data()
        
        # Generate gas composition data
        self.composition_data = self._generate_gas_composition_data()
        
        print("\n" + "="*60)
        print("Data Generation Complete!")
        print("="*60)
        
        return self
    
    def export_data(self, output_dir='synthetic_data_output'):
        """Export all generated data to various formats"""
        print(f"\nExporting data to {output_dir}...")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # 1. Export to CSV files
        csv_dir = output_path / 'csv_files'
        csv_dir.mkdir(exist_ok=True)
        
        # Create main dataframe with time index
        df_main = pd.DataFrame(index=self.time_index)
        
        # Add all process and equipment data
        for name, data in {**self.process_data, **self.equipment_data}.items():
            df_main[name] = data
        
        # Add environmental data
        for name, data in self.environmental_data.items():
            df_main[name] = data
        
        # Save main CSV (sample first 100,000 rows for demo)
        main_csv_path = csv_dir / 'plant_data_main.csv'
        df_main.iloc[:100000].to_csv(main_csv_path)  # First 100k rows for demo
        print(f"  Main data CSV: {main_csv_path} (100,000 rows)")
        
        # Save full data in parquet format (more efficient)
        parquet_path = output_path / 'plant_data_full.parquet'
        df_main.to_parquet(parquet_path, compression='snappy')
        print(f"  Full data Parquet: {parquet_path} ({len(df_main):,} rows)")
        
        # 2. Export text data
        text_dir = output_path / 'text_data'
        text_dir.mkdir(exist_ok=True)
        
        # Shift reports
        df_shifts = pd.DataFrame(self.text_data['shift_reports'])
        df_shifts.to_csv(text_dir / 'shift_reports.csv', index=False)
        
        # Work orders
        df_work_orders = pd.DataFrame(self.text_data['work_orders'])
        df_work_orders.to_csv(text_dir / 'work_orders.csv', index=False)
        
        # Alarm logs
        df_alarms = pd.DataFrame(self.text_data['alarm_logs'])
        df_alarms.to_csv(text_dir / 'alarm_logs.csv', index=False)
        
        # 3. Export composition data
        df_composition = pd.DataFrame(self.composition_data)
        df_composition.to_csv(output_path / 'gas_composition.csv', index=False)
        
        # 4. Export failure events
        df_failures = pd.DataFrame(self.failure_events)
        df_failures.to_csv(output_path / 'failure_events.csv', index=False)
        
        # 5. Export to SQLite database
        db_path = output_path / 'gas_plant_data.db'
        with sqlite3.connect(db_path) as conn:
            # Create tables
            df_main.to_sql('process_data', conn, if_exists='replace')
            df_shifts.to_sql('shift_reports', conn, if_exists='replace')
            df_work_orders.to_sql('work_orders', conn, if_exists='replace')
            df_alarms.to_sql('alarm_logs', conn, if_exists='replace')
            df_composition.to_sql('gas_composition', conn, if_exists='replace')
            df_failures.to_sql('failure_events', conn, if_exists='replace')
        
        print(f"  SQLite database: {db_path}")
        
        # 6. Export metadata and configuration
        metadata = {
            'generation_info': {
                'plant_name': self.plant_name,
                'location': self.location,
                'generation_date': datetime.now().isoformat(),
                'simulation_period': {
                    'start': self.start_date.isoformat(),
                    'end': (self.start_date + timedelta(days=self.simulation_days)).isoformat(),
                    'days': self.simulation_days
                },
                'data_points_per_sensor': self.total_seconds,
                'total_sensors': len(df_main.columns),
                'random_seed': self.seed
            },
            'sensor_list': list(df_main.columns),
            'failure_definitions': self.failure_events
        }
        
        metadata_path = output_path / 'metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        print(f"  Metadata: {metadata_path}")
        
        # 7. Create a sample data file (first 24 hours)
        sample_data = df_main.iloc[:86400]  # First day
        sample_path = output_path / 'sample_data_24h.csv'
        sample_data.to_csv(sample_path)
        print(f"  Sample data (24h): {sample_path}")
        
        print(f"\nAll data exported successfully to {output_path}")
        
        # Create a summary report
        self._create_summary_report(output_path)
        
        return output_path
    
    def _create_summary_report(self, output_path):
        """Create a summary report of generated data"""
        report = f"""
        ==============================================
        SYNTHETIC DATA GENERATION SUMMARY REPORT
        ==============================================
        
        Plant: {self.plant_name}
        Location: {self.location}
        Generation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        
        SIMULATION PERIOD:
        Start: {self.start_date.strftime('%Y-%m-%d')}
        End: {(self.start_date + timedelta(days=self.simulation_days)).strftime('%Y-%m-%d')}
        Duration: {self.simulation_days} days ({self.simulation_days/30:.1f} months)
        
        DATA STATISTICS:
        Total time points: {self.total_seconds:,} seconds
        Sampling frequency: {self.sampling_rate} Hz
        Total data points per sensor: {self.total_seconds:,}
        
        GENERATED DATA TYPES:
        1. Process Data: {len(self.process_data)} parameters
        2. Equipment Health: {len(self.equipment_data)} parameters
        3. Environmental Data: {len(self.environmental_data)} parameters
        4. Gas Composition: {len(self.composition_data)} samples (every 5 minutes)
        5. Text Data:
           - Shift Reports: {len(self.text_data['shift_reports'])} reports
           - Work Orders: {len(self.text_data['work_orders'])} orders
           - Alarm Logs: {len(self.text_data['alarm_logs'])} alarms
        6. Failure Events: {len(self.failure_events)} simulated failures
        
        OUTPUT FILES:
        Location: {output_path}
        
        FAILURE EVENTS SIMULATED:
        """
        
        for i, failure in enumerate(self.failure_events, 1):
            report += f"""
            {i}. {failure['failure_id']} - {failure['failure_type']}
               Start: {failure['start_time'].strftime('%Y-%m-%d')}
               Duration: {failure['duration_days']} days
               Pattern: {failure['progression_pattern']}
               Affected Sensors: {', '.join(failure['affected_sensors'][:3])}
            """
        
        report += """
        
        DATA VALIDATION:
        - All data is physically plausible
        - Failure patterns follow real-world progression
        - Time-series correlations are maintained
        - Environmental data matches Austin, TX climate
        
        USAGE NOTES:
        1. For full dataset, use the Parquet file for efficiency
        2. For quick analysis, use the 24-hour sample
        3. SQLite database contains all data in relational format
        4. Metadata file contains generation parameters
        
        ==============================================
        """
        
        report_path = output_path / 'generation_summary.txt'
        with open(report_path, 'w') as f:
            f.write(report)
        
        print(f"  Summary report: {report_path}")
        
        # Also print to console
        print(report)

def main():
    """Main execution function"""
    print("Gas Plant Synthetic Data Generator")
    print("Austin, Texas - 6 Months Simulation\n")
    
    try:
        # Initialize generator
        generator = SyntheticGasPlantGenerator()
        
        # Generate all data
        generator.generate_all_data()
        
        # Export data
        output_path = generator.export_data()
        
        print("\n" + "="*60)
        print("GENERATION COMPLETE!")
        print(f"Data available in: {output_path}")
        print("="*60)
        
    except Exception as e:
        print(f"Error during data generation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

#### **2. plant_config.yaml**
```yaml
# Configuration for Lone Star Gas Refining Plant - Austin, TX

plant:
  name: "Lone Star Gas Refining"
  location: "Austin, Texas"
  coordinates:
    latitude: 30.2672
    longitude: -97.7431
  elevation_m: 149

capacity:
  feed_gas_mmscfd: 200
  products:
    pipeline_gas_mmscfd: 150
    ethane_mbpd: 15
    propane_mbpd: 10
    butane_mbpd: 5
    condensate_mbpd: 8
    sulfur_tpd: 10

process_units:
  inlet_separation:
    vessels: 3
    compressors: 2
    separators: 2
    heaters: 1
    
  amine_treating:
    contactors: 2
    regenerators: 2
    pumps: 8
    exchangers: 6
    capacity_mmscfd: 220
    
  dehydration:
    TEG_contactors: 2
    reboilers: 2
    pumps: 4
    glycol_concentration: 99.5
    
  cryogenic_plant:
    exchangers: 12
    turbines: 2
    towers: 3
    cold_box_temp_c: -100
    propane_refrigerant: true
    
  fractionation:
    deethanizer: 1
    depropanizer: 1
    debutanizer: 1
    reboilers: 6
    condensers: 3
    
  sulfur_recovery:
    reactors: 2
    incinerator: 1
    converters: 2
    tail_gas_treatment: true

instrumentation:
  total_sensors: 1273
  sensor_types:
    temperature: 450
    pressure: 380
    flow: 220
    level: 85
    vibration: 120
    composition: 18
    
  control_loops: 145
  safety_systems: 45

operational_parameters:
  design_pressure_psig: 1440
  design_temperature_f: 650
  turndown_ratio: 40
  availability_target: 98.5
  energy_efficiency: 92.0

personnel:
  total_employees: 85
  shifts_per_day: 2
  operators_per_shift: 4
  maintenance_crew: 12
  engineering_support: 8

environmental:
  emissions_limits:
    so2_ppm: 10
    nox_ppm: 30
    co_ppm: 50
    voc_ppm: 100
  water_usage_gpm: 120
  waste_generation_tpd: 2.5

data_generation:
  simulation_days: 182
  sampling_rate_hz: 1
  random_seed: 42
  failure_injection: true
  weather_modeling: true
  text_data_generation: true
```

#### **3. requirements.txt**
```txt
numpy>=1.21.0
pandas>=1.3.0
pyyaml>=6.0
sqlite3
scipy>=1.7.0
matplotlib>=3.4.0  # For optional visualization
pyarrow>=5.0.0  # For parquet export
```

#### **4. run_generator.py** (Simplified runner)
```python
#!/usr/bin/env python3
"""
Simplified runner for data generator
"""

import subprocess
import sys
import time

def install_requirements():
    """Install required packages"""
    print("Installing requirements...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"])

def run_generator():
    """Run the main generator"""
    from main_generator import main
    
    print("\nStarting data generation...")
    start_time = time.time()
    
    main()
    
    elapsed = time.time() - start_time
    print(f"\nTotal execution time: {elapsed:.2f} seconds")

if __name__ == "__main__":
    # Install requirements if needed
    try:
        import pandas as pd
    except ImportError:
        install_requirements()
    
    # Run generator
    run_generator()
```

#### **5. data_visualizer.py** (Optional)
```python
#!/usr/bin/env python3
"""
Optional visualization for generated data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

def visualize_sample_data(data_dir='synthetic_data_output'):
    """Create visualization plots for sample data"""
    
    # Load sample data
    sample_path = Path(data_dir) / 'sample_data_24h.csv'
    df = pd.read_csv(sample_path, index_col=0, parse_dates=True)
    
    # Set style
    sns.set_style("whitegrid")
    plt.figure(figsize=(15, 10))
    
    # Plot 1: Key process parameters
    plt.subplot(3, 2, 1)
    if 'plant_throughput' in df.columns:
        plt.plot(df.index, df['plant_throughput'], label='Throughput', alpha=0.7)
    if 'inlet_separation_feed_pressure' in df.columns:
        plt.plot(df.index, df['inlet_separation_feed_pressure'], label='Feed Pressure', alpha=0.7)
    plt.title('Key Process Parameters (24h)')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.xticks(rotation=45)
    
    # Plot 2: Equipment health
    plt.subplot(3, 2, 2)
    vibration_cols = [col for col in df.columns if 'vibration' in col]
    for col in vibration_cols[:3]:  # First 3 vibration sensors
        plt.plot(df.index, df[col], label=col, alpha=0.7)
    plt.title('Equipment Vibration (24h)')
    plt.xlabel('Time')
    plt.ylabel('Vibration (mm/s)')
    plt.legend()
    plt.xticks(rotation=45)
    
    # Plot 3: Environmental data
    plt.subplot(3, 2, 3)
    if 'temperature_C' in df.columns:
        plt.plot(df.index, df['temperature_C'], label='Temperature', color='red', alpha=0.7)
    plt.title('Ambient Temperature - Austin, TX')
    plt.xlabel('Time')
    plt.ylabel('Temperature (°C)')
    plt.legend()
    plt.xticks(rotation=45)
    
    # Plot 4: Histogram of process values
    plt.subplot(3, 2, 4)
    if 'plant_throughput' in df.columns:
        plt.hist(df['plant_throughput'].dropna(), bins=50, alpha=0.7, edgecolor='black')
        plt.title('Throughput Distribution')
        plt.xlabel('Throughput (SCF/s)')
        plt.ylabel('Frequency')
    
    # Plot 5: Correlation heatmap (sample of parameters)
    plt.subplot(3, 2, 5)
    # Select a subset of columns for correlation
    corr_cols = [col for col in df.columns if any(x in col for x in ['pressure', 'temp', 'flow'])]
    corr_cols = corr_cols[:8]  # First 8 relevant columns
    if len(corr_cols) > 1:
        corr_matrix = df[corr_cols].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('Parameter Correlations')
    
    # Plot 6: Failure indicators
    plt.subplot(3, 2, 6)
    failure_cols = [col for col in df.columns if any(x in col for x in ['vibration', 'bearing'])]
    for col in failure_cols[:2]:
        plt.plot(df.index, df[col], label=col, alpha=0.7)
    plt.title('Failure Progression Indicators')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.savefig(Path(data_dir) / 'data_visualization.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print(f"Visualization saved to {Path(data_dir) / 'data_visualization.png'}")

if __name__ == "__main__":
    visualize_sample_data()
```

---

## **راهنمای اجرا و استفاده:**

### **نصب و راه‌اندازی:**
```bash
# 1. Clone or create project directory
mkdir synthetic_gas_plant_data
cd synthetic_gas_plant_data

# 2. Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install requirements
pip install numpy pandas pyyaml pyarrow matplotlib

# 4. Create configuration file
# Save the plant_config.yaml content above

# 5. Run the generator
python main_generator.py
```

### **خروجی‌های تولید شده:**
سیستم فایل‌های زیر را تولید می‌کند:

```
synthetic_data_output/
├── csv_files/
│   └── plant_data_main.csv          # 100,000 نمونه اول (برای دمو)
├── plant_data_full.parquet          # کل داده‌ها (15.7 میلیون نقطه)
├── gas_composition.csv              # داده‌های ترکیب گاز
├── failure_events.csv               # رویدادهای خرابی
├── gas_plant_data.db                # پایگاه داده SQLite کامل
├── text_data/
│   ├── shift_reports.csv
│   ├── work_orders.csv
│   └── alarm_logs.csv
├── sample_data_24h.csv              # 24 ساعت اول داده
├── metadata.json                    # متادیتای کامل
└── generation_summary.txt           # گزارش خلاصه
```

### **ویژگی‌های کلیدی سیستم:**

1. **داده‌های واقع‌گرا:** الگوهای فصلی، روزانه و ساعتی منطبق با آستین، تگزاس
2. **خرابی‌های پیچیده:** 5 نوع خرابی با الگوهای پیشرونده واقعی
3. **فرمت‌های متنوع:** CSV، Parquet، JSON، SQLite
4. **تکرارپذیری:** seedهای تصادفی ثابت
5. **مقیاس‌پذیری:** امکان افزایش دوره زمانی یا تعداد سنسورها
6. **مستندات کامل:** متادیتا و گزارش خلاصه

### **کاربردهای داده‌های تولید شده:**

1. **آموزش مدل‌های هوش مصنوعی:** برای پیش‌بینی خرابی، بهینه‌سازی فرآیند
2. **توسعه سیستم‌های مانیتورینگ:** تست سیستم‌های SCADA و HMI
3. **شبیه‌سازی عملکرد:** تست استراتژی‌های کنترل و عملیاتی
4. **تحقیق و توسعه:** توسعه الگوریتم‌های جدید بدون نیاز به داده واقعی
5. **آموزش پرسنل:** ایجاد سناریوهای آموزشی واقع‌گرا

این سیستم کاملاً منطبق با نیازهای پلتفرم ApexAsset AI بوده و می‌تواند برای آموزش، تست و اعتبارسنجی مدل‌های هوش مصنوعی در تمام مراحل چرخه حیات دارایی مورد استفاده قرار گیرد.
